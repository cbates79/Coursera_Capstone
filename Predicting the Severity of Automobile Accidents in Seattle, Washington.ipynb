{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting the Severity of Automobile Accidents in Seattle, Washington ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this first week, you will discover your\n",
    "project objectives, find your dataset that you will use for this capstone project, and publish your\n",
    "dataset on GitHub.\n",
    "\n",
    "In the second week, you will build your machine\n",
    "learning solution.\n",
    "\n",
    "In the third week,\n",
    "you will finalize your model and be ready\n",
    "to submit your work.\n",
    "\n",
    "To complete capstone,\n",
    "you will be working on a case study which is to predict the severity\n",
    "of an accident.\n",
    "Now, wouldn't it be great if there were something in place that could warn you, \n",
    "given the weather and the road conditions,\n",
    "about the possibility of you getting into a car accident and how severe it would be,\n",
    "so that you would drive more carefully or even change your travel plans?\n",
    "Let's use our shared data for Seattle, Washington as an example of how to deal with the accidents data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import common packages for Data Science applications.\n",
    "import io\n",
    "import itertools\n",
    "import matplotlib as mpl\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import pylab as pl\n",
    "import scipy\n",
    "import scipy.optimize as opt\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "import sklearn.linear_model\n",
    "import sys\n",
    "from matplotlib.ticker import NullFormatter\n",
    "from scipy import optimize\n",
    "from scipy.optimize import curve_fit\n",
    "from sklearn import linear_model\n",
    "from sklearn import metrics\n",
    "from sklearn import pipeline\n",
    "from sklearn import preprocessing\n",
    "from sklearn import svm\n",
    "from sklearn import tree\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import jaccard_score\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "notebook_start_time = os.times()[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting display options...\n",
      "max_info_columns: 1000\n",
      "colheader_justify: right\n",
      "max_info_rows: 1000000\n",
      "column_space: 1000\n",
      "max_rows: 1000000\n",
      "precision: 9\n",
      "max_seq_items: 1000000000000\n",
      "show_dimensions: True\n",
      "max_categories: 1000\n",
      "memory_usage: True\n",
      "max_columns: 1000\n",
      "max_colwidth: 1000\n",
      "float_format: <function <lambda> at 0x7efcabcff670>\n"
     ]
    }
   ],
   "source": [
    "# Create a list of display options.\n",
    "list_of_display_options_fully_qualified_names = str(\\\n",
    "\"pd.options.display.chop_threshold, pd.options.display.float_format, pd.options.display.max_info_columns, pd.options.display.notebook_repr_html, \\\n",
    "pd.options.display.colheader_justify, pd.options.display.html, pd.options.display.max_info_rows, pd.options.display.pprint_nest_depth, \\\n",
    "pd.options.display.column_space, pd.options.display.large_repr, pd.options.display.max_rows, pd.options.display.precision, \\\n",
    "pd.options.display.date_dayfirst, pd.options.display.latex, pd.options.display.max_seq_items, pd.options.display.show_dimensions, \\\n",
    "pd.options.display.date_yearfirst, pd.options.display.max_categories, pd.options.display.memory_usage, pd.options.display.unicode, \\\n",
    "pd.options.display.encoding, pd.options.display.max_columns, pd.options.display.min_rows, pd.options.display.width, \\\n",
    "pd.options.display.expand_frame_repr, pd.options.display.max_colwidth, pd.options.display.multi_sparse\").split(sep=', ')\n",
    "\n",
    "# Initialize an empty list to store all the short names for display options.\n",
    "list_of_display_options_short_names = list()\n",
    "# For each fully qualified option name,\n",
    "# get the option's short name and add it to the list of short names.\n",
    "for fully_qualified_option_name in list_of_display_options_fully_qualified_names:\n",
    "    # Get short option name.\n",
    "    short_option_name = fully_qualified_option_name.split(sep='.')[-1]\n",
    "    \n",
    "    # Add short option name to list of display option short names.\n",
    "    list_of_display_options_short_names.append(short_option_name)\n",
    "\n",
    "# Define dictionary of display option settings.\n",
    "dict_of_display_option_settings_short_names=\\\n",
    "{\"max_info_columns\": 1000,\\\n",
    "\"colheader_justify\": \"right\",\\\n",
    "\"max_info_rows\": 1000000,\\\n",
    "\"column_space\": 1000,\\\n",
    "\"max_rows\": 1000000,\\\n",
    "\"precision\": 9,\\\n",
    "\"max_seq_items\": 1000000000000,\\\n",
    "\"show_dimensions\": True,\\\n",
    "\"max_categories\": 1000,\\\n",
    "\"memory_usage\": True,\\\n",
    "\"max_columns\": 1000,\\\n",
    "\"max_colwidth\": 1000,\\\n",
    "\"float_format\": lambda x: '%.9f' % x}\n",
    "\n",
    "# Set pandas display options using dictionary of short names,\n",
    "# and display the options/value pairs.\n",
    "print(\"Setting display options...\")\n",
    "for key in list(dict_of_display_option_settings_short_names.keys()):\n",
    "    # Set display option.\n",
    "    pd.set_option(key, dict_of_display_option_settings_short_names[key])\n",
    "    # Print display option name and value.\n",
    "    print(key, \": \", pd.get_option(key), sep='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attribute Information URL: https://www.seattle.gov/Documents/Departments/SDOT/GIS/Collisions_OD.pdf\n",
    "# Read the Collisions Data CSV file and store it as a DataFrame.\n",
    "# url=\"https://opendata.arcgis.com/datasets/5b5c745e0f1f48e7a53acec63a0022ab_0.csv\" # HTTPError at 202009151050, using local copy of .csv instead.\n",
    "# print(os.listdir(\"..\")) # Print list of contents of current working directory.\n",
    "local_path_to_csv = \"../Collisions.csv\"\n",
    "df=pd.read_csv(local_path_to_csv, low_memory=False)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# View the first few rows of the collisions DataFrame.\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# For each column of the collisions DataFrame,\n",
    "# print the data type and relative frequencies of the values.\n",
    "for column in list(df.columns):\n",
    "    print(column,\":\", df[column].dtype)\n",
    "    print(df[column].value_counts(normalize=True, dropna=False))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"data_wrangling\">Data Wrangling</h2>\n",
    "\n",
    "Steps for working with missing data:\n",
    "<ol>\n",
    "    <li>Identify missing data.</li>\n",
    "    <li>Deal with missing data.</li>\n",
    "    <li>Correct data format.</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 id=\"identifying_missing_data\">Identifying Missing Data</h3>\n",
    "\n",
    "The metadata document that accompanied the data set indicates that certain columns have \"sentinel\" values\n",
    "that indicate an unknown or missing value. Each of these missing values will first be converted into NaN.\n",
    "Subsequently, the NaN values will be dropped from the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If any row of the collisions DataFrame contains a sentinel value representing \"unknown\",\n",
    "# then replace it with NaN. \n",
    "# Sentinels for \"unknown\" are listed in the metadata form that accompanied the dataset.\n",
    "df_unknowns_converted_to_nan = df.replace(to_replace=\\\n",
    "{\"EXCEPTRSNCODE\": \" \",\\\n",
    " \"EXCEPTRSNDESC\": \"Not Enough Information, or Insufficient Location Information\",\\\n",
    " \"SEVERITYCODE\": \"0\",\\\n",
    " \"SEVERITYDESC\": \"Unknown\",\\\n",
    " \"JUNCTIONTYPE\": \"Unknown\",\\\n",
    " \"WEATHER\": \"Unknown\",\\\n",
    " \"ROADCOND\": \"Unknown\",\\\n",
    " \"LIGHTCOND\": \"Unknown\",\\\n",
    " \"SDOT_COLCODE\": float(0),\\\n",
    " \"SDOT_COLDESC\": \"NOT ENOUGH INFORMATION / NOT APPLICABLE\",\\\n",
    " \"ST_COLCODE\": \" \",\\\n",
    " \"ST_COLDESC\": \"Not stated\"},\\\n",
    "value=np.nan, inplace=False, limit=None, regex=False, method='pad')\n",
    "\n",
    "df_unknowns_converted_to_nan.replace(to_replace={\"ST_COLCODE\": \"0\", }, value=np.nan, inplace=True, limit=None, regex=False, method='pad')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 id=\"deal_with_missing_data\">Deal with Missing Data</h3>\n",
    "\n",
    "<ol>\n",
    "    <li>Drop the Data\n",
    "        <ol>\n",
    "            <li>Drop entire row.</li>\n",
    "            <li>Drop entire column.</li>\n",
    "        </ol>\n",
    "    </li>\n",
    "    <li>Replace the Data\n",
    "        <ol>\n",
    "            <li>Replace data by mean.</li>\n",
    "            <li>Replace data by frequency.</li>\n",
    "            <li>Replace data based on other functions.</li>\n",
    "        </ol>\n",
    "    </li>\n",
    "        \n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whole columns should be dropped only if most entries in the column are empty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a list to store the labels for the columns with missing data.\n",
    "list_of_columns_with_missing_data = list()\n",
    "\n",
    "# For each column in the collisions DataFrame,\n",
    "# if the column contains at least one NaN, \n",
    "# then add the column's label to the list.\n",
    "for column in list(df_unknowns_converted_to_nan.columns):\n",
    "    if df_unknowns_converted_to_nan[column].hasnans:\n",
    "        list_of_columns_with_missing_data.append(column)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Print the number of columns and their labels,\n",
    "# as well as the number of columns missing data and their labels.\n",
    "print(\"Number of columns: %d\" % len(df_unknowns_converted_to_nan.columns))\n",
    "print(\"List of labels for columns:\")\n",
    "print(list(df_unknowns_converted_to_nan.columns))\n",
    "print()\n",
    "print(\"Number of columns that are missing data: %d\" % len(list_of_columns_with_missing_data))\n",
    "print(\"List of labels for columns that are missing data:\")\n",
    "print(list_of_columns_with_missing_data)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# For each column in the DataFrame after unknowns have been converted to Nan,\n",
    "# print the relative frequencies of the column's values.\n",
    "for column in list(df_unknowns_converted_to_nan.columns):\n",
    "    print(column, df_unknowns_converted_to_nan[column].dtype, \"Relative Frequencies:\")\n",
    "    print(df_unknowns_converted_to_nan[column].value_counts(normalize=True, dropna=False))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['X', 'Y', 'OBJECTID', 'INCKEY', 'COLDETKEY', 'REPORTNO', 'STATUS', 'ADDRTYPE', 'INTKEY', 'LOCATION', 'EXCEPTRSNCODE', 'EXCEPTRSNDESC', 'SEVERITYCODE', 'SEVERITYDESC', 'COLLISIONTYPE', 'PERSONCOUNT', 'PEDCOUNT', 'PEDCYLCOUNT', 'VEHCOUNT', 'INJURIES', 'SERIOUSINJURIES', 'FATALITIES', 'INCDATE', 'INCDTTM', 'JUNCTIONTYPE', 'SDOT_COLCODE', 'SDOT_COLDESC', 'INATTENTIONIND', 'UNDERINFL', 'WEATHER', 'ROADCOND', 'LIGHTCOND', 'PEDROWNOTGRNT', 'SDOTCOLNUM', 'SPEEDING', 'ST_COLCODE', 'ST_COLDESC', 'SEGLANEKEY', 'CROSSWALKKEY', 'HITPARKEDCAR']\n"
     ]
    }
   ],
   "source": [
    "print(list(df.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop any column from the collisions DataFrame if it satisfies at least one of the following conditions:\n",
    "# 1) more than 15% of the column's data is NaN;\n",
    "# 2) the column only contains unique identification keys;\n",
    "# 3) the column's data is naturally categorical but does not fit into a small (< 50) number of categories;\n",
    "# 4) information in one column is redundant because it is already represented by another column;\n",
    "# 5) it is not clear how to interpret the column's data.\n",
    "\n",
    "list_of_columns_to_drop = [\\\n",
    "                           \"ADDRTYPE\",\\\n",
    "                           \"STATUS\",\\\n",
    "                           \"OBJECTID\",\\\n",
    "                           \"INCKEY\",\\\n",
    "                           \"COLDETKEY\",\\\n",
    "                           \"REPORTNO\",\\\n",
    "                           \"INTKEY\",\\\n",
    "                           \"LOCATION\",\\\n",
    "                           \"EXCEPTRSNCODE\",\\\n",
    "                           \"EXCEPTRSNDESC\",\\\n",
    "                           \"SEVERITYDESC\",\\\n",
    "                           \"INCDATE\",\\\n",
    "                           \"SDOT_COLCODE\",\\\n",
    "                           \"SDOT_COLDESC\",\\\n",
    "                           \"INATTENTIONIND\",\\\n",
    "                           \"UNDERINFL\",\\\n",
    "                           \"PEDROWNOTGRNT\",\\\n",
    "                           \"SDOTCOLNUM\",\\\n",
    "                           \"SPEEDING\",\\\n",
    "                           \"ST_COLCODE\",\\\n",
    "                           \"ST_COLDESC\",\\\n",
    "                           \"SEGLANEKEY\",\\\n",
    "                           \"CROSSWALKKEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the selected columns from the DataFrame after converting unknowns to NaN.\n",
    "# and store the result in a new DataFrame.\n",
    "df_drop_columns = df_unknowns_converted_to_nan.drop(columns=list_of_columns_to_drop, inplace=False)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Test if DataFrame has NaN after dropping columns.\n",
    "if df_drop_columns.isna().any(axis=None):\n",
    "    print(\"DataFrame has NaN.\")\n",
    "else:\n",
    "    print(\"DataFrame has no NaN.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop any row that contains at least one NaN.\n",
    "df_drop_columns_and_rows = df_drop_columns.dropna(axis=\"index\", how=\"any\", thresh=None, subset=None, inplace=False)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Test if DataFrame has NaN values after dropping columns and rows.\n",
    "if df_drop_columns_and_rows.isna().any(axis=None):\n",
    "    print(\"DataFrame has NaN.\")\n",
    "else:\n",
    "    print(\"DataFrame has no NaN.\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# For each column in DataFrame after dropping columns and rows,\n",
    "# print the relative frequencies of the column's values.\n",
    "for column in list(df_drop_columns_and_rows.columns):\n",
    "    print(column, \"Relative Frequencies:\")\n",
    "    print(df_drop_columns_and_rows[column].value_counts(normalize=True, dropna=False))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df_drop_columns_and_rows.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 id=\"correct_data_format\">Correct Data Format</h3>\n",
    "\n",
    "Ensure that each data type is appropriate for the corresponding feature.\n",
    "Convert integer data to \"ordered\" categorical types, e.g. SEVERITYCODE,\n",
    "especially if the \"integer ordering\" of the original data is inappropriate.\n",
    "\n",
    "If data represents date, time, or date/time information, then convert the data to the appropriate datetime representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new DataFrame to store converted data types.\n",
    "df_converted = pd.DataFrame()\n",
    "\n",
    "for column in list(df_drop_columns_and_rows.columns):\n",
    "    # Cast columns \"INCDTTM\" to type datetime.\n",
    "    if column in [\"INCDTTM\"]:\n",
    "        df_converted[column] = pd.to_datetime(df_drop_columns_and_rows[column], infer_datetime_format=True)\n",
    "    # Cast columns of type object to type category.\n",
    "    elif (df_drop_columns_and_rows[column].dtype in [np.dtype('object')]):\n",
    "        df_converted[column] = df_drop_columns_and_rows[column].astype('category')\n",
    "    # Copy all other columns to new DataFrame without changing their types.\n",
    "    else:\n",
    "        df_converted[column] = df_drop_columns_and_rows[column]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Verify that DataFrame has no NaN.\n",
    "if df_converted.isna().any(axis=None):\n",
    "    print(\"DataFrame has NaN.\")\n",
    "else:\n",
    "    print(\"DataFrame has no NaN.\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Display info about new DataFrame after casting objects to category or date\n",
    "df_converted.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame of categorical columns.\n",
    "df_categorical = df_converted.select_dtypes(include=\"category\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df_categorical.info()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# For each column in the categorical DataFrame,\n",
    "# print the relative frequency of the values.\n",
    "for column in list(df_categorical.columns):\n",
    "    print(column, \":\", df_categorical[column].dtype)\n",
    "    print(df_categorical[column].value_counts(normalize=True, dropna=False))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Features before One Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SEVERITYCODE',\n",
       " 'COLLISIONTYPE',\n",
       " 'JUNCTIONTYPE',\n",
       " 'WEATHER',\n",
       " 'ROADCOND',\n",
       " 'LIGHTCOND',\n",
       " 'HITPARKEDCAR']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(df_categorical.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SEVERITYCODE</th>\n",
       "      <th>COLLISIONTYPE</th>\n",
       "      <th>JUNCTIONTYPE</th>\n",
       "      <th>WEATHER</th>\n",
       "      <th>ROADCOND</th>\n",
       "      <th>LIGHTCOND</th>\n",
       "      <th>HITPARKEDCAR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Sideswipe</td>\n",
       "      <td>Mid-Block (not related to intersection)</td>\n",
       "      <td>Raining</td>\n",
       "      <td>Wet</td>\n",
       "      <td>Dark - Street Lights On</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Parked Car</td>\n",
       "      <td>Mid-Block (not related to intersection)</td>\n",
       "      <td>Clear</td>\n",
       "      <td>Dry</td>\n",
       "      <td>Daylight</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>Rear Ended</td>\n",
       "      <td>Mid-Block (not related to intersection)</td>\n",
       "      <td>Clear</td>\n",
       "      <td>Dry</td>\n",
       "      <td>Daylight</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>Other</td>\n",
       "      <td>Mid-Block (but intersection related)</td>\n",
       "      <td>Clear</td>\n",
       "      <td>Wet</td>\n",
       "      <td>Daylight</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>Sideswipe</td>\n",
       "      <td>At Intersection (intersection related)</td>\n",
       "      <td>Overcast</td>\n",
       "      <td>Dry</td>\n",
       "      <td>Daylight</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>Sideswipe</td>\n",
       "      <td>Mid-Block (not related to intersection)</td>\n",
       "      <td>Clear</td>\n",
       "      <td>Dry</td>\n",
       "      <td>Daylight</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>Rear Ended</td>\n",
       "      <td>Mid-Block (not related to intersection)</td>\n",
       "      <td>Overcast</td>\n",
       "      <td>Dry</td>\n",
       "      <td>Daylight</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>Angles</td>\n",
       "      <td>Mid-Block (but intersection related)</td>\n",
       "      <td>Overcast</td>\n",
       "      <td>Dry</td>\n",
       "      <td>Daylight</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "      <td>Parked Car</td>\n",
       "      <td>Mid-Block (not related to intersection)</td>\n",
       "      <td>Clear</td>\n",
       "      <td>Wet</td>\n",
       "      <td>Dark - Street Lights On</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2</td>\n",
       "      <td>Parked Car</td>\n",
       "      <td>Mid-Block (not related to intersection)</td>\n",
       "      <td>Overcast</td>\n",
       "      <td>Dry</td>\n",
       "      <td>Dark - Street Lights On</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   SEVERITYCODE COLLISIONTYPE                             JUNCTIONTYPE  \\\n",
       "0             1     Sideswipe  Mid-Block (not related to intersection)   \n",
       "1             1    Parked Car  Mid-Block (not related to intersection)   \n",
       "5             1    Rear Ended  Mid-Block (not related to intersection)   \n",
       "6             1         Other     Mid-Block (but intersection related)   \n",
       "8             1     Sideswipe   At Intersection (intersection related)   \n",
       "9             1     Sideswipe  Mid-Block (not related to intersection)   \n",
       "10            1    Rear Ended  Mid-Block (not related to intersection)   \n",
       "11            1        Angles     Mid-Block (but intersection related)   \n",
       "12            1    Parked Car  Mid-Block (not related to intersection)   \n",
       "13            2    Parked Car  Mid-Block (not related to intersection)   \n",
       "\n",
       "     WEATHER ROADCOND                LIGHTCOND HITPARKEDCAR  \n",
       "0    Raining      Wet  Dark - Street Lights On            N  \n",
       "1      Clear      Dry                 Daylight            Y  \n",
       "5      Clear      Dry                 Daylight            N  \n",
       "6      Clear      Wet                 Daylight            N  \n",
       "8   Overcast      Dry                 Daylight            N  \n",
       "9      Clear      Dry                 Daylight            N  \n",
       "10  Overcast      Dry                 Daylight            N  \n",
       "11  Overcast      Dry                 Daylight            N  \n",
       "12     Clear      Wet  Dark - Street Lights On            N  \n",
       "13  Overcast      Dry  Dark - Street Lights On            N  \n",
       "\n",
       "[10 rows x 7 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_categorical.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#features = df_categorical[[\"COLLISIONTYPE\", \"WEATHER\", \"ROADCOND\", \"LIGHTCOND\"]]\n",
    "#features = df_categorical[[\"WEATHER\", \"ROADCOND\", \"LIGHTCOND\"]]\n",
    "features = df_categorical[[\"WEATHER\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_features = list(features.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SEVERITYCODE relative frequencies:\n",
      "1    0.655966184\n",
      "2    0.324930284\n",
      "2b   0.017183785\n",
      "3    0.001919746\n",
      "Name: SEVERITYCODE, Length: 4, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(\"SEVERITYCODE relative frequencies:\")\n",
    "print(df_categorical[\"SEVERITYCODE\"].value_counts(normalize=True, dropna=False))\n",
    "#print(\"SEVERITYCODE value counts:\")\n",
    "#print(df_categorical[\"SEVERITYCODE\"].value_counts(normalize=False, dropna=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WEATHER                   SEVERITYCODE\n",
      "Blowing Sand/Dirt         1              0.697674419\n",
      "                          2              0.302325581\n",
      "Clear                     1              0.655914077\n",
      "                          2              0.324196986\n",
      "                          2b             0.017903702\n",
      "                          3              0.001985234\n",
      "Fog/Smog/Smoke            1              0.655616943\n",
      "                          2              0.333333333\n",
      "                          2b             0.005524862\n",
      "                          3              0.005524862\n",
      "Other                     1              0.661354582\n",
      "                          2              0.306772908\n",
      "                          2b             0.019920319\n",
      "                          3              0.011952191\n",
      "Overcast                  1              0.664109691\n",
      "                          2              0.317843246\n",
      "                          2b             0.016120067\n",
      "                          3              0.001926996\n",
      "Partly Cloudy             1              0.500000000\n",
      "                          2              0.400000000\n",
      "                          3              0.100000000\n",
      "Raining                   1              0.645571584\n",
      "                          2              0.336900884\n",
      "                          2b             0.015976423\n",
      "                          3              0.001551109\n",
      "Severe Crosswind          1              0.680000000\n",
      "                          2              0.280000000\n",
      "                          3              0.040000000\n",
      "Sleet/Hail/Freezing Rain  1              0.734513274\n",
      "                          2              0.247787611\n",
      "                          2b             0.017699115\n",
      "Snowing                   1              0.789793439\n",
      "                          2              0.198055893\n",
      "                          2b             0.012150668\n",
      "Name: SEVERITYCODE, Length: 34, dtype: float64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for feature in list_of_features:\n",
    "    print(df_categorical.groupby(feature)[\"SEVERITYCODE\"].value_counts(normalize=True, dropna=False))\n",
    "    #print(df_categorical.groupby(\"SEVERITYCODE\")[feature].value_counts(normalize=True, dropna=False))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SEVERITYCODE  WEATHER                 \n",
      "1             Clear                      0.641666816\n",
      "              Raining                    0.186245905\n",
      "              Overcast                   0.160389854\n",
      "              Snowing                    0.005817388\n",
      "              Fog/Smog/Smoke             0.003186139\n",
      "              Other                      0.001485671\n",
      "              Sleet/Hail/Freezing Rain   0.000742836\n",
      "              Blowing Sand/Dirt          0.000268495\n",
      "              Severe Crosswind           0.000152147\n",
      "              Partly Cloudy              0.000044749\n",
      "2             Clear                      0.640269572\n",
      "              Raining                    0.196216597\n",
      "              Overcast                   0.154967749\n",
      "              Fog/Smog/Smoke             0.003270277\n",
      "              Snowing                    0.002945056\n",
      "              Other                      0.001391223\n",
      "              Sleet/Hail/Freezing Rain   0.000505899\n",
      "              Blowing Sand/Dirt          0.000234882\n",
      "              Severe Crosswind           0.000126475\n",
      "              Partly Cloudy              0.000072271\n",
      "2b            Clear                      0.668602665\n",
      "              Raining                    0.175948070\n",
      "              Overcast                   0.148616331\n",
      "              Snowing                    0.003416467\n",
      "              Other                      0.001708234\n",
      "              Fog/Smog/Smoke             0.001024940\n",
      "              Sleet/Hail/Freezing Rain   0.000683293\n",
      "3             Clear                      0.663608563\n",
      "              Overcast                   0.159021407\n",
      "              Raining                    0.152905199\n",
      "              Fog/Smog/Smoke             0.009174312\n",
      "              Other                      0.009174312\n",
      "              Partly Cloudy              0.003058104\n",
      "              Severe Crosswind           0.003058104\n",
      "Name: WEATHER, Length: 34, dtype: float64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for feature in list_of_features:\n",
    "    print(df_categorical.groupby(\"SEVERITYCODE\")[feature].value_counts(normalize=True, dropna=False))\n",
    "    #print(df_categorical.groupby(\"SEVERITYCODE\")[feature].value_counts(normalize=False, dropna=False))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>WEATHER</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Raining</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Clear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Clear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Clear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Overcast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Clear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Overcast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Overcast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Clear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Overcast</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     WEATHER\n",
       "0    Raining\n",
       "1      Clear\n",
       "5      Clear\n",
       "6      Clear\n",
       "8   Overcast\n",
       "9      Clear\n",
       "10  Overcast\n",
       "11  Overcast\n",
       "12     Clear\n",
       "13  Overcast\n",
       "\n",
       "[10 rows x 1 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use one hot encoding technique to convert categorical varables to binary variables and append them to the features DataFrame "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each feature of the features DataFrame,\n",
    "# get dummy encoding for the feature,\n",
    "# prefix the category column labels with the feature label and a '_' separator,\n",
    "# and concatenate the one-hot encoded columns to the features DataFrame.\n",
    "for feature in list(features.columns):\n",
    "    features = pd.concat([features, pd.get_dummies(features[feature], prefix=feature, prefix_sep='_', dummy_na=False, columns=feature, sparse=False, drop_first=False)], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define a features set represented by the numerical DataFrame X_not_normalized:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_not_normalized = features.select_dtypes(include=\"number\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 170335 entries, 0 to 221388\n",
      "Data columns (total 10 columns):\n",
      " #   Column                            Non-Null Count   Dtype\n",
      "---  ------                            --------------   -----\n",
      " 0   WEATHER_Blowing Sand/Dirt         170335 non-null  uint8\n",
      " 1   WEATHER_Clear                     170335 non-null  uint8\n",
      " 2   WEATHER_Fog/Smog/Smoke            170335 non-null  uint8\n",
      " 3   WEATHER_Other                     170335 non-null  uint8\n",
      " 4   WEATHER_Overcast                  170335 non-null  uint8\n",
      " 5   WEATHER_Partly Cloudy             170335 non-null  uint8\n",
      " 6   WEATHER_Raining                   170335 non-null  uint8\n",
      " 7   WEATHER_Severe Crosswind          170335 non-null  uint8\n",
      " 8   WEATHER_Sleet/Hail/Freezing Rain  170335 non-null  uint8\n",
      " 9   WEATHER_Snowing                   170335 non-null  uint8\n",
      "dtypes: uint8(10)\n",
      "memory usage: 2.9 MB\n"
     ]
    }
   ],
   "source": [
    "X_not_normalized.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also define the labels for the target variable, SEVERITYCODE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df_categorical[\"SEVERITYCODE\"].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(170335,)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize Data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We normalize the data, transforming to have zero mean and unit variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X is a normalized numpy ndarray.\n",
    "X = preprocessing.StandardScaler().fit(X_not_normalized).transform(X_not_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(170335, 10)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We split the normalized data and target labels into a training test and a test set.\n",
    "We use the training set to build an accurate model.\n",
    "Afterwards, we use the test set to report the accuracy of the model.\n",
    "\n",
    "We apply the following algorithms to produce various kinds of models.\n",
    "- K Nearest Neighbor(KNN)\n",
    "- Decision Tree\n",
    "- Support Vector Machine\n",
    "- Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K Nearest Neighbor(KNN)\n",
    "First, we find the best value of k with which to build a model with the greatest accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "136268"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.01589048,  0.7472064 , -0.05655113, -0.03841538, -0.43387274,\n",
       "        -0.00766233, -0.48313333, -0.01211574, -0.02576507, -0.06967864],\n",
       "       [-0.01589048, -1.33831831, -0.05655113, -0.03841538,  2.30482331,\n",
       "        -0.00766233, -0.48313333, -0.01211574, -0.02576507, -0.06967864],\n",
       "       [-0.01589048,  0.7472064 , -0.05655113, -0.03841538, -0.43387274,\n",
       "        -0.00766233, -0.48313333, -0.01211574, -0.02576507, -0.06967864],\n",
       "       [-0.01589048,  0.7472064 , -0.05655113, -0.03841538, -0.43387274,\n",
       "        -0.00766233, -0.48313333, -0.01211574, -0.02576507, -0.06967864],\n",
       "       [-0.01589048,  0.7472064 , -0.05655113, -0.03841538, -0.43387274,\n",
       "        -0.00766233, -0.48313333, -0.01211574, -0.02576507, -0.06967864],\n",
       "       [-0.01589048, -1.33831831, -0.05655113, -0.03841538,  2.30482331,\n",
       "        -0.00766233, -0.48313333, -0.01211574, -0.02576507, -0.06967864],\n",
       "       [-0.01589048,  0.7472064 , -0.05655113, -0.03841538, -0.43387274,\n",
       "        -0.00766233, -0.48313333, -0.01211574, -0.02576507, -0.06967864],\n",
       "       [-0.01589048, -1.33831831, -0.05655113, -0.03841538, -0.43387274,\n",
       "        -0.00766233,  2.06982202, -0.01211574, -0.02576507, -0.06967864],\n",
       "       [-0.01589048,  0.7472064 , -0.05655113, -0.03841538, -0.43387274,\n",
       "        -0.00766233, -0.48313333, -0.01211574, -0.02576507, -0.06967864],\n",
       "       [-0.01589048,  0.7472064 , -0.05655113, -0.03841538, -0.43387274,\n",
       "        -0.00766233, -0.48313333, -0.01211574, -0.02576507, -0.06967864]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0:10,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building KNeighborsClassifier for number of neighbors k = 10 ...\n",
      "Completed in 290.589999999851 seconds.\n"
     ]
    }
   ],
   "source": [
    "# Define the best KNN model.\n",
    "print(\"Building KNeighborsClassifier for number of neighbors k = 10 ...\")\n",
    "start_time = os.times()[4]\n",
    "neigh_best = KNeighborsClassifier(n_neighbors = 10).fit(X_train, y_train)\n",
    "end_time = os.times()[4]\n",
    "total_elapsed_time = end_time - start_time\n",
    "print(\"Completed in\", total_elapsed_time, \"seconds.\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Calculate the accuracy of KNN for different Ks\n",
    "Ks = min(y_train.shape[0], 50)\n",
    "mean_acc = np.zeros(Ks - 1)\n",
    "std_acc = np.zeros(Ks - 1)\n",
    "time_on_enter_for_loop = os.times()[4]\n",
    "for n in range(1, Ks):\n",
    "    single_pass_start_time = os.times()[4]\n",
    "    print(\"For number of neighbors k = \", n, \", \", sep='', end='')\n",
    "    # Train Model and Predict \n",
    "    neigh = KNeighborsClassifier(n_neighbors = n, weights='uniform', algorithm='auto', leaf_size=30, p=2, metric='minkowski', metric_params=None, n_jobs=-1).fit(X_train, y_train)\n",
    "    yhat = neigh.predict(X_test)\n",
    "    mean_acc[n - 1] = metrics.accuracy_score(y_test, yhat)\n",
    "    std_acc[n - 1] = np.std(yhat == y_test) / np.sqrt(Ks)\n",
    "    print(\"accuracy = \", mean_acc[n - 1], sep='', end='')\n",
    "    single_pass_end_time = os.times()[4]\n",
    "    single_pass_elapsed_time = time = single_pass_end_time - single_pass_start_time\n",
    "    print(\" in\", single_pass_elapsed_time, \"seconds.\")\n",
    "\n",
    "print()\n",
    "time_on_exit_for_loop = os.times()[4]\n",
    "total_elapsed_time = time_on_exit_for_loop - time_on_enter_for_loop\n",
    "print(\"Total elapsed time to find optimum number of neighbors k =\", total_elapsed_time, \"seconds.\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Define the best KNN model.\n",
    "neigh_best = KNeighborsClassifier(n_neighbors = mean_acc.argmax() + 1).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Plot model accuracy for different number of neighbors k\n",
    "plt.plot(range(1, Ks), mean_acc, 'g')\n",
    "plt.fill_between(range(1, Ks), mean_acc - 1 * std_acc, mean_acc + 1 * std_acc, alpha=0.10)\n",
    "plt.legend(('Accuracy ', '+/- 3 std. dev.'))\n",
    "plt.ylabel('Accuracy ')\n",
    "plt.xlabel('Number of Neighbors (k)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Plot model accuracy for different values of k \n",
    "print( \"The best accuracy was\", mean_acc.max(), \"with a number of neighbors k =\", mean_acc.argmax()+1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a Decision Tree Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built Decision Tree Model in 0.7899999991059303 seconds.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Build a decision tree model from the training data previously generated.\n",
    "start_time = os.times()[4]\n",
    "decision_tree = DecisionTreeClassifier(criterion=\"entropy\")\n",
    "decision_tree.fit(X_train,y_train)\n",
    "end_time = os.times()[4]\n",
    "elapsed_time = end_time - start_time\n",
    "print(\"Built Decision Tree Model in\", elapsed_time, \"seconds.\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a Support Vector Machine Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built Support Vector Machine Model in 1562.800000000745 seconds.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Build a support vector machine model from the training data previously generated.\n",
    "start_time = os.times()[4]\n",
    "clf = svm.SVC(kernel='rbf', gamma='auto')\n",
    "clf.fit(X_train, y_train)\n",
    "end_time = os.times()[4]\n",
    "elapsed_time = end_time - start_time\n",
    "print(\"Built Support Vector Machine Model in\", elapsed_time, \"seconds.\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built Logistic Regression Model in 3.3900000005960464 seconds.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Build a logistic regression model from the training data previously generated.\n",
    "start_time = os.times()[4]\n",
    "lr = LogisticRegression(C=0.01, solver='liblinear').fit(X_train,y_train)\n",
    "end_time = os.times()[4]\n",
    "elapsed_time = end_time - start_time\n",
    "print(\"Built Logistic Regression Model in\", elapsed_time, \"seconds.\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate the Various Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    0.655502392\n",
       "2    0.326063346\n",
       "2b   0.016291426\n",
       "3    0.002142836\n",
       "Length: 4, dtype: float64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(y_test).value_counts(normalize=True, dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running command: y_knn_predictions=neigh_best.predict(X_test)\n",
      "Completed in 441.3499999977648 seconds.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "start_time = os.times()[4]\n",
    "# Apply KNN to the test set, generate predictions for KNN.\n",
    "print(\"Running command: y_knn_predictions=neigh_best.predict(X_test)\")\n",
    "y_knn_predictions=neigh_best.predict(X_test)\n",
    "end_time = os.times()[4]\n",
    "elapsed_time = end_time - start_time\n",
    "print(\"Completed in\", elapsed_time, \"seconds.\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(34067,)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_knn_predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1   1.000000000\n",
       "Length: 1, dtype: float64"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(y_knn_predictions).value_counts(normalize=True, dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running command: y_tree_predictions = decision_tree.predict(X_test)\n",
      "Completed in 0.019999999552965164 seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Apply Decision Tree to the test set, generate predictions for Decision Tree.\n",
    "print(\"Running command: y_tree_predictions = decision_tree.predict(X_test)\")\n",
    "start_time = os.times()[4]\n",
    "y_tree_predictions = decision_tree.predict(X_test)\n",
    "end_time = os.times()[4]\n",
    "elapsed_time = end_time - start_time\n",
    "print(\"Completed in\", elapsed_time, \"seconds\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1   1.000000000\n",
       "Length: 1, dtype: float64"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(y_tree_predictions).value_counts(normalize=True, dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running command: y_svm_predictions = clf.predict(X_test)\n",
      "Completed in 323.16999999806285 seconds.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Apply SVM to the test set, generate predictions for SVM.\n",
    "print(\"Running command: y_svm_predictions = clf.predict(X_test)\")\n",
    "start_time = os.times()[4]\n",
    "y_svm_predictions = clf.predict(X_test)\n",
    "end_time = os.times()[4]\n",
    "elapsed_time = end_time - start_time\n",
    "print(\"Completed in\", elapsed_time, \"seconds.\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1   1.000000000\n",
       "Length: 1, dtype: float64"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(y_svm_predictions).value_counts(normalize=True, dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running command: y_lr_predictions = lr.predict(X_test)\n",
      "Completed in 0.05000000074505806 seconds.\n",
      "\n",
      "Running command: y_lr_probabilities = lr.predict_proba(X_test\\)\n",
      "Completed in 0.019999999552965164 seconds.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Apply Logistic Regression to the test set, generate predictions and probabilities for Logistic Regression.\n",
    "print(\"Running command: y_lr_predictions = lr.predict(X_test)\")\n",
    "start_time = os.times()[4]\n",
    "y_lr_predictions = lr.predict(X_test)\n",
    "end_time = os.times()[4]\n",
    "elapsed_time = end_time - start_time\n",
    "print(\"Completed in\", elapsed_time, \"seconds.\")\n",
    "print()\n",
    "\n",
    "print(\"Running command: y_lr_probabilities = lr.predict_proba(X_test\\)\")\n",
    "start_time = os.times()[4]\n",
    "y_lr_probabilities = lr.predict_proba(X_test)\n",
    "end_time = os.times()[4]\n",
    "elapsed_time = end_time - start_time\n",
    "print(\"Completed in\", elapsed_time, \"seconds.\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1   1.000000000\n",
       "Length: 1, dtype: float64"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(y_lr_predictions).value_counts(normalize=True, dropna=False)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Create and fit a label encoder for the target labels.\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(y_test)\n",
    "# Transform the labels of the target test set into dummy labels.\n",
    "y_test_lr_dummies = le.transform(y_test)\n",
    "y_lr_predictions_dummies = le.transform(y_lr_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define numpy arrays to store the results of tests of the various algorithms.\n",
    "# index = 0 => KNN score\n",
    "# index = 1 => Decision Tree\n",
    "# index = 2 => SVM\n",
    "# index = 3 => Logistic Regression\n",
    "jaccard = np.zeros((4,4))\n",
    "f1 = np.zeros((4,4))\n",
    "logloss = np.zeros((4,4))\n",
    "logloss[0] = np.nan\n",
    "logloss[1] = np.nan\n",
    "logloss[2] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN Jaccard score is [0.65550239 0.         0.         0.        ]\n",
      "KNN F1-score is [0.79190751 0.         0.         0.        ]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# For KNN model, compute Jaccard score.\n",
    "jaccard[0] = jaccard_score(y_test, y_knn_predictions, labels=[\"1\", \"2\", \"2b\", \"3\"], average=None)\n",
    "print(\"KNN Jaccard score is\", jaccard[0,:])\n",
    "# For KNN model, compute F1-score.\n",
    "f1[0] = f1_score(y_test, y_knn_predictions, average=None)\n",
    "print(\"KNN F1-score is\", f1[0])\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Jaccard score is:  [0.65550239 0.         0.         0.        ]\n",
      "Decision Tree F1-score is:  [0.79190751 0.         0.         0.        ]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# For Decision Tree model, compute Jaccard score.\n",
    "jaccard[1] = jaccard_score(y_test, y_tree_predictions, labels=[\"1\", \"2\", \"2b\", \"3\"], average=None)\n",
    "print(\"Decision Tree Jaccard score is: \", jaccard[1])\n",
    "# For Decision Tree model, compute F1-score.\n",
    "f1[1] = f1_score(y_test, y_tree_predictions, average=None)\n",
    "print(\"Decision Tree F1-score is: \", f1[1])\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Jaccard score is [0.65550239 0.         0.         0.        ]\n",
      "SVM F1-score is [0.79190751 0.         0.         0.        ]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# For SVM algorithm, compute Jaccard score.\n",
    "jaccard[2] = jaccard_score(y_test, y_svm_predictions, labels=[\"1\", \"2\", \"2b\", \"3\"], average=None)\n",
    "print(\"SVM Jaccard score is\", jaccard[2])\n",
    "# For SVM algorithm, compute F1-score.\n",
    "f1[2] = f1_score(y_test, y_svm_predictions, average=None)\n",
    "print(\"SVM F1-score is\", f1[2])\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Jaccard similarity score is [0.65550239 0.         0.         0.        ]\n",
      "Logistic Regression F1-score is [0.79190751 0.         0.         0.        ]\n",
      "Logistic Regression log loss is [0.72416609 0.72416609 0.72416609 0.72416609]\n"
     ]
    }
   ],
   "source": [
    "# For logistic regression algorithm, compute Jaccard score.\n",
    "jaccard[3] = jaccard_score(y_test, y_lr_predictions, labels=[\"1\", \"2\", \"2b\", \"3\"], average=None)\n",
    "print(\"Logistic Regression Jaccard similarity score is\", jaccard[3])\n",
    "# For logistic regression algorithm, compute F1-score.\n",
    "f1[3] = f1_score(y_test, y_lr_predictions, average=None)\n",
    "print(\"Logistic Regression F1-score is\", f1[3])\n",
    "# For logistic regression algorithm, compute log loss.\n",
    "logloss[3] = log_loss(y_test, y_lr_probabilities)\n",
    "print(\"Logistic Regression log loss is\", logloss[3])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "report_df = pd.DataFrame(data={'Algorithm':['KNN', 'Decision Tree', 'SVM', 'LogisticRegression'],\n",
    "                               'Jaccard':jaccard,\n",
    "                               'F1-score':f1,\n",
    "                               'LogLoss':logloss})\n",
    "\n",
    "report_df[['Algorithm', 'Jaccard', 'F1-score', 'LogLoss']].style.hide_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notebook total elapsed time: 2728.390000000596 seconds.\n"
     ]
    }
   ],
   "source": [
    "notebook_end_time = os.times()[4]\n",
    "notebook_total_elapsed_time = notebook_end_time - notebook_start_time\n",
    "print(\"Notebook total elapsed time:\", notebook_total_elapsed_time, \"seconds.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
