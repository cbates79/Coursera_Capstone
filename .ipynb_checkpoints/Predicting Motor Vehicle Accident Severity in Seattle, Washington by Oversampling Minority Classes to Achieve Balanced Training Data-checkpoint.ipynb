{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 id=\"title\">Predicting Motor Vehicle Accident Severity in Seattle, Washington by Oversampling Minority Classes to Achieve Balanced Training Data<\\h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages and modules.\n",
    "import io\n",
    "import itertools\n",
    "import matplotlib as mpl\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import pylab as pl\n",
    "import scipy\n",
    "import scipy.optimize as opt\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "import sys\n",
    "import timeit\n",
    "import warnings\n",
    "from matplotlib.ticker import NullFormatter\n",
    "from scipy import optimize\n",
    "from scipy.optimize import curve_fit\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn import linear_model\n",
    "from sklearn import metrics\n",
    "from sklearn import pipeline\n",
    "from sklearn import preprocessing\n",
    "from sklearn import svm\n",
    "from sklearn import tree\n",
    "from sklearn import utils\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import jaccard_score\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.utils import resample\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "from timeit import default_timer\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning, module=\"sklearn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the starting time for this notebook. \n",
    "notebook_start_time = default_timer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function computes the time elapsed in seconds from the \n",
    "# time represented by the first parameter (start_time)\n",
    "# to the time represented by the second parameter (end_time)\n",
    "# This function requires the os package to be imported.\n",
    "def elapsed_time(start_time = notebook_start_time):\n",
    "    return default_timer() - start_time   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function prints the time elapsed in seconds from the \n",
    "# time represented by the first parameter (start_time)\n",
    "# to the time represented by the second parameter (end_time)\n",
    "# This function requires the os package to be imported.\n",
    "def print_elapsed_time(start_time = notebook_start_time):\n",
    "    print(\"Elapsed time is\", elapsed_time(start_time), \"seconds.\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting display options...\n",
      "max_info_columns: 1000\n",
      "colheader_justify: right\n",
      "max_info_rows: 1000000\n",
      "column_space: 1000\n",
      "max_rows: 1000000\n",
      "precision: 9\n",
      "max_seq_items: 1000000000000\n",
      "show_dimensions: True\n",
      "max_categories: 100\n",
      "memory_usage: True\n",
      "max_columns: 1000\n",
      "max_colwidth: 1000\n",
      "float_format: <function <lambda> at 0x7f84b83c4310>\n"
     ]
    }
   ],
   "source": [
    "# Create a list of display options.\n",
    "list_of_display_options_fully_qualified_names = str(\\\n",
    "\"pd.options.display.chop_threshold, pd.options.display.float_format, pd.options.display.max_info_columns, pd.options.display.notebook_repr_html, \\\n",
    "pd.options.display.colheader_justify, pd.options.display.html, pd.options.display.max_info_rows, pd.options.display.pprint_nest_depth, \\\n",
    "pd.options.display.column_space, pd.options.display.large_repr, pd.options.display.max_rows, pd.options.display.precision, \\\n",
    "pd.options.display.date_dayfirst, pd.options.display.latex, pd.options.display.max_seq_items, pd.options.display.show_dimensions, \\\n",
    "pd.options.display.date_yearfirst, pd.options.display.max_categories, pd.options.display.memory_usage, pd.options.display.unicode, \\\n",
    "pd.options.display.encoding, pd.options.display.max_columns, pd.options.display.min_rows, pd.options.display.width, \\\n",
    "pd.options.display.expand_frame_repr, pd.options.display.max_colwidth, pd.options.display.multi_sparse\").split(sep=', ')\n",
    "\n",
    "# Initialize an empty list to store all the short names for display options.\n",
    "list_of_display_options_short_names = list()\n",
    "# For each fully qualified option name,\n",
    "# get the option's short name and add it to the list of short names.\n",
    "for fully_qualified_option_name in list_of_display_options_fully_qualified_names:\n",
    "    # Get short option name.\n",
    "    short_option_name = fully_qualified_option_name.split(sep='.')[-1]\n",
    "    \n",
    "    # Add short option name to list of display option short names.\n",
    "    list_of_display_options_short_names.append(short_option_name)\n",
    "\n",
    "# Define dictionary of display option settings.\n",
    "dict_of_display_option_settings_short_names=\\\n",
    "{\"max_info_columns\": 1000,\\\n",
    "\"colheader_justify\": \"right\",\\\n",
    "\"max_info_rows\": 1000000,\\\n",
    "\"column_space\": 1000,\\\n",
    "\"max_rows\": 1000000,\\\n",
    "\"precision\": 9,\\\n",
    "\"max_seq_items\": 1000000000000,\\\n",
    "\"show_dimensions\": True,\\\n",
    "\"max_categories\": 100,\\\n",
    "\"memory_usage\": True,\\\n",
    "\"max_columns\": 1000,\\\n",
    "\"max_colwidth\": 1000,\\\n",
    "\"float_format\": lambda x: '%.9f' % x}\n",
    "\n",
    "# Set pandas display options using dictionary of short names,\n",
    "# and display the options/value pairs.\n",
    "print(\"Setting display options...\")\n",
    "for key in list(dict_of_display_option_settings_short_names.keys()):\n",
    "    # Set display option.\n",
    "    pd.set_option(key, dict_of_display_option_settings_short_names[key])\n",
    "    # Print display option name and value.\n",
    "    print(key, \": \", pd.get_option(key), sep='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42\n"
     ]
    }
   ],
   "source": [
    "# Set seed for random number generator.\n",
    "# seed = np.int(os.times()[4]) # Use this line for better pseudo-random behavior.\n",
    "seed = 42\n",
    "print(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attribute Information URL: https://www.seattle.gov/Documents/Departments/SDOT/GIS/Collisions_OD.pdf\n",
    "# Read the Collisions Data CSV file and store it as a DataFrame.\n",
    "# url=\"https://opendata.arcgis.com/datasets/5b5c745e0f1f48e7a53acec63a0022ab_0.csv\" # HTTPError at 202009151050, using local copy of .csv instead.\n",
    "# print(os.listdir(\"..\")) # Print list of contents of current working directory.\n",
    "local_path_to_csv = '~/IBM Data Science Professional Certificate Course/Course 9 - Applied Data Science Capstone/projects/Collisions.csv'\n",
    "df=pd.read_csv(local_path_to_csv, low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X</th>\n",
       "      <th>Y</th>\n",
       "      <th>OBJECTID</th>\n",
       "      <th>INCKEY</th>\n",
       "      <th>COLDETKEY</th>\n",
       "      <th>REPORTNO</th>\n",
       "      <th>STATUS</th>\n",
       "      <th>ADDRTYPE</th>\n",
       "      <th>INTKEY</th>\n",
       "      <th>LOCATION</th>\n",
       "      <th>EXCEPTRSNCODE</th>\n",
       "      <th>EXCEPTRSNDESC</th>\n",
       "      <th>SEVERITYCODE</th>\n",
       "      <th>SEVERITYDESC</th>\n",
       "      <th>COLLISIONTYPE</th>\n",
       "      <th>PERSONCOUNT</th>\n",
       "      <th>PEDCOUNT</th>\n",
       "      <th>PEDCYLCOUNT</th>\n",
       "      <th>VEHCOUNT</th>\n",
       "      <th>INJURIES</th>\n",
       "      <th>SERIOUSINJURIES</th>\n",
       "      <th>FATALITIES</th>\n",
       "      <th>INCDATE</th>\n",
       "      <th>INCDTTM</th>\n",
       "      <th>JUNCTIONTYPE</th>\n",
       "      <th>SDOT_COLCODE</th>\n",
       "      <th>SDOT_COLDESC</th>\n",
       "      <th>INATTENTIONIND</th>\n",
       "      <th>UNDERINFL</th>\n",
       "      <th>WEATHER</th>\n",
       "      <th>ROADCOND</th>\n",
       "      <th>LIGHTCOND</th>\n",
       "      <th>PEDROWNOTGRNT</th>\n",
       "      <th>SDOTCOLNUM</th>\n",
       "      <th>SPEEDING</th>\n",
       "      <th>ST_COLCODE</th>\n",
       "      <th>ST_COLDESC</th>\n",
       "      <th>SEGLANEKEY</th>\n",
       "      <th>CROSSWALKKEY</th>\n",
       "      <th>HITPARKEDCAR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-122.320757054</td>\n",
       "      <td>47.609407946</td>\n",
       "      <td>1</td>\n",
       "      <td>328476</td>\n",
       "      <td>329976</td>\n",
       "      <td>EA08706</td>\n",
       "      <td>Matched</td>\n",
       "      <td>Block</td>\n",
       "      <td>nan</td>\n",
       "      <td>BROADWAY BETWEEN E COLUMBIA ST AND BOYLSTON AVE</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>Property Damage Only Collision</td>\n",
       "      <td>Sideswipe</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2020/01/22 00:00:00+00</td>\n",
       "      <td>1/22/2020 3:21:00 PM</td>\n",
       "      <td>Mid-Block (not related to intersection)</td>\n",
       "      <td>11.000000000</td>\n",
       "      <td>MOTOR VEHICLE STRUCK MOTOR VEHICLE, FRONT END AT ANGLE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>N</td>\n",
       "      <td>Raining</td>\n",
       "      <td>Wet</td>\n",
       "      <td>Dark - Street Lights On</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11</td>\n",
       "      <td>From same direction - both going straight - both moving - sideswipe</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-122.319560827</td>\n",
       "      <td>47.662220664</td>\n",
       "      <td>2</td>\n",
       "      <td>328142</td>\n",
       "      <td>329642</td>\n",
       "      <td>EA06882</td>\n",
       "      <td>Matched</td>\n",
       "      <td>Block</td>\n",
       "      <td>nan</td>\n",
       "      <td>8TH AVE NE BETWEEN NE 45TH E ST AND NE 47TH ST</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>Property Damage Only Collision</td>\n",
       "      <td>Parked Car</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2020/01/07 00:00:00+00</td>\n",
       "      <td>1/7/2020 8:00:00 AM</td>\n",
       "      <td>Mid-Block (not related to intersection)</td>\n",
       "      <td>15.000000000</td>\n",
       "      <td>MOTOR VEHICLE STRUCK MOTOR VEHICLE, RIGHT SIDE SIDESWIPE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>N</td>\n",
       "      <td>Clear</td>\n",
       "      <td>Dry</td>\n",
       "      <td>Daylight</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>32</td>\n",
       "      <td>One parked--one moving</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-122.327524508</td>\n",
       "      <td>47.604393273</td>\n",
       "      <td>3</td>\n",
       "      <td>20700</td>\n",
       "      <td>20700</td>\n",
       "      <td>1181833</td>\n",
       "      <td>Unmatched</td>\n",
       "      <td>Block</td>\n",
       "      <td>nan</td>\n",
       "      <td>JAMES ST BETWEEN 6TH AVE AND 7TH AVE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2004/01/30 00:00:00+00</td>\n",
       "      <td>1/30/2004</td>\n",
       "      <td>Mid-Block (but intersection related)</td>\n",
       "      <td>11.000000000</td>\n",
       "      <td>MOTOR VEHICLE STRUCK MOTOR VEHICLE, FRONT END AT ANGLE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4030032.000000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-122.327524934</td>\n",
       "      <td>47.708621579</td>\n",
       "      <td>4</td>\n",
       "      <td>332126</td>\n",
       "      <td>333626</td>\n",
       "      <td>M16001640</td>\n",
       "      <td>Unmatched</td>\n",
       "      <td>Block</td>\n",
       "      <td>nan</td>\n",
       "      <td>NE NORTHGATE WAY BETWEEN 1ST AVE NE AND NE NORTHGATE DR</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2016/01/23 00:00:00+00</td>\n",
       "      <td>1/23/2016</td>\n",
       "      <td>Mid-Block (not related to intersection)</td>\n",
       "      <td>11.000000000</td>\n",
       "      <td>MOTOR VEHICLE STRUCK MOTOR VEHICLE, FRONT END AT ANGLE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-122.292120049</td>\n",
       "      <td>47.559009080</td>\n",
       "      <td>5</td>\n",
       "      <td>328238</td>\n",
       "      <td>329738</td>\n",
       "      <td>3857118</td>\n",
       "      <td>Unmatched</td>\n",
       "      <td>Block</td>\n",
       "      <td>nan</td>\n",
       "      <td>M L KING JR ER WAY S BETWEEN S ANGELINE ST AND S EDMUNDS ST</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2020/01/26 00:00:00+00</td>\n",
       "      <td>1/26/2020</td>\n",
       "      <td>Mid-Block (not related to intersection)</td>\n",
       "      <td>28.000000000</td>\n",
       "      <td>MOTOR VEHICLE RAN OFF ROAD - HIT FIXED OBJECT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               X            Y  OBJECTID  INCKEY  COLDETKEY   REPORTNO  \\\n",
       "0 -122.320757054 47.609407946         1  328476     329976    EA08706   \n",
       "1 -122.319560827 47.662220664         2  328142     329642    EA06882   \n",
       "2 -122.327524508 47.604393273         3   20700      20700    1181833   \n",
       "3 -122.327524934 47.708621579         4  332126     333626  M16001640   \n",
       "4 -122.292120049 47.559009080         5  328238     329738    3857118   \n",
       "\n",
       "      STATUS ADDRTYPE  INTKEY  \\\n",
       "0    Matched    Block     nan   \n",
       "1    Matched    Block     nan   \n",
       "2  Unmatched    Block     nan   \n",
       "3  Unmatched    Block     nan   \n",
       "4  Unmatched    Block     nan   \n",
       "\n",
       "                                                      LOCATION EXCEPTRSNCODE  \\\n",
       "0              BROADWAY BETWEEN E COLUMBIA ST AND BOYLSTON AVE                 \n",
       "1               8TH AVE NE BETWEEN NE 45TH E ST AND NE 47TH ST                 \n",
       "2                         JAMES ST BETWEEN 6TH AVE AND 7TH AVE           NaN   \n",
       "3      NE NORTHGATE WAY BETWEEN 1ST AVE NE AND NE NORTHGATE DR                 \n",
       "4  M L KING JR ER WAY S BETWEEN S ANGELINE ST AND S EDMUNDS ST                 \n",
       "\n",
       "  EXCEPTRSNDESC SEVERITYCODE                    SEVERITYDESC COLLISIONTYPE  \\\n",
       "0           NaN            1  Property Damage Only Collision     Sideswipe   \n",
       "1           NaN            1  Property Damage Only Collision    Parked Car   \n",
       "2           NaN            0                         Unknown           NaN   \n",
       "3           NaN            0                         Unknown           NaN   \n",
       "4           NaN            0                         Unknown           NaN   \n",
       "\n",
       "   PERSONCOUNT  PEDCOUNT  PEDCYLCOUNT  VEHCOUNT  INJURIES  SERIOUSINJURIES  \\\n",
       "0            2         0            0         2         0                0   \n",
       "1            2         0            0         2         0                0   \n",
       "2            0         0            0         0         0                0   \n",
       "3            0         0            0         0         0                0   \n",
       "4            0         0            0         0         0                0   \n",
       "\n",
       "   FATALITIES                 INCDATE               INCDTTM  \\\n",
       "0           0  2020/01/22 00:00:00+00  1/22/2020 3:21:00 PM   \n",
       "1           0  2020/01/07 00:00:00+00   1/7/2020 8:00:00 AM   \n",
       "2           0  2004/01/30 00:00:00+00             1/30/2004   \n",
       "3           0  2016/01/23 00:00:00+00             1/23/2016   \n",
       "4           0  2020/01/26 00:00:00+00             1/26/2020   \n",
       "\n",
       "                              JUNCTIONTYPE  SDOT_COLCODE  \\\n",
       "0  Mid-Block (not related to intersection)  11.000000000   \n",
       "1  Mid-Block (not related to intersection)  15.000000000   \n",
       "2     Mid-Block (but intersection related)  11.000000000   \n",
       "3  Mid-Block (not related to intersection)  11.000000000   \n",
       "4  Mid-Block (not related to intersection)  28.000000000   \n",
       "\n",
       "                                               SDOT_COLDESC INATTENTIONIND  \\\n",
       "0    MOTOR VEHICLE STRUCK MOTOR VEHICLE, FRONT END AT ANGLE            NaN   \n",
       "1  MOTOR VEHICLE STRUCK MOTOR VEHICLE, RIGHT SIDE SIDESWIPE            NaN   \n",
       "2    MOTOR VEHICLE STRUCK MOTOR VEHICLE, FRONT END AT ANGLE            NaN   \n",
       "3    MOTOR VEHICLE STRUCK MOTOR VEHICLE, FRONT END AT ANGLE            NaN   \n",
       "4             MOTOR VEHICLE RAN OFF ROAD - HIT FIXED OBJECT            NaN   \n",
       "\n",
       "  UNDERINFL  WEATHER ROADCOND                LIGHTCOND PEDROWNOTGRNT  \\\n",
       "0         N  Raining      Wet  Dark - Street Lights On           NaN   \n",
       "1         N    Clear      Dry                 Daylight           NaN   \n",
       "2       NaN      NaN      NaN                      NaN           NaN   \n",
       "3       NaN      NaN      NaN                      NaN           NaN   \n",
       "4       NaN      NaN      NaN                      NaN           NaN   \n",
       "\n",
       "         SDOTCOLNUM SPEEDING ST_COLCODE  \\\n",
       "0               nan      NaN         11   \n",
       "1               nan      NaN         32   \n",
       "2 4030032.000000000      NaN        NaN   \n",
       "3               nan      NaN              \n",
       "4               nan      NaN              \n",
       "\n",
       "                                                            ST_COLDESC  \\\n",
       "0  From same direction - both going straight - both moving - sideswipe   \n",
       "1                                               One parked--one moving   \n",
       "2                                                                  NaN   \n",
       "3                                                                  NaN   \n",
       "4                                                                  NaN   \n",
       "\n",
       "   SEGLANEKEY  CROSSWALKKEY HITPARKEDCAR  \n",
       "0           0             0            N  \n",
       "1           0             0            Y  \n",
       "2           0             0            N  \n",
       "3           0             0            N  \n",
       "4           0             0            N  \n",
       "\n",
       "[5 rows x 40 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View the first few rows of the collisions DataFrame.\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 221389 entries, 0 to 221388\n",
      "Data columns (total 40 columns):\n",
      " #   Column           Non-Null Count   Dtype  \n",
      "---  ------           --------------   -----  \n",
      " 0   X                213918 non-null  float64\n",
      " 1   Y                213918 non-null  float64\n",
      " 2   OBJECTID         221389 non-null  int64  \n",
      " 3   INCKEY           221389 non-null  int64  \n",
      " 4   COLDETKEY        221389 non-null  int64  \n",
      " 5   REPORTNO         221389 non-null  object \n",
      " 6   STATUS           221389 non-null  object \n",
      " 7   ADDRTYPE         217677 non-null  object \n",
      " 8   INTKEY           71884 non-null   float64\n",
      " 9   LOCATION         216801 non-null  object \n",
      " 10  EXCEPTRSNCODE    100986 non-null  object \n",
      " 11  EXCEPTRSNDESC    11779 non-null   object \n",
      " 12  SEVERITYCODE     221388 non-null  object \n",
      " 13  SEVERITYDESC     221389 non-null  object \n",
      " 14  COLLISIONTYPE    195159 non-null  object \n",
      " 15  PERSONCOUNT      221389 non-null  int64  \n",
      " 16  PEDCOUNT         221389 non-null  int64  \n",
      " 17  PEDCYLCOUNT      221389 non-null  int64  \n",
      " 18  VEHCOUNT         221389 non-null  int64  \n",
      " 19  INJURIES         221389 non-null  int64  \n",
      " 20  SERIOUSINJURIES  221389 non-null  int64  \n",
      " 21  FATALITIES       221389 non-null  int64  \n",
      " 22  INCDATE          221389 non-null  object \n",
      " 23  INCDTTM          221389 non-null  object \n",
      " 24  JUNCTIONTYPE     209417 non-null  object \n",
      " 25  SDOT_COLCODE     221388 non-null  float64\n",
      " 26  SDOT_COLDESC     221388 non-null  object \n",
      " 27  INATTENTIONIND   30188 non-null   object \n",
      " 28  UNDERINFL        195179 non-null  object \n",
      " 29  WEATHER          194969 non-null  object \n",
      " 30  ROADCOND         195050 non-null  object \n",
      " 31  LIGHTCOND        194880 non-null  object \n",
      " 32  PEDROWNOTGRNT    5192 non-null    object \n",
      " 33  SDOTCOLNUM       127205 non-null  float64\n",
      " 34  SPEEDING         9928 non-null    object \n",
      " 35  ST_COLCODE       211976 non-null  object \n",
      " 36  ST_COLDESC       195159 non-null  object \n",
      " 37  SEGLANEKEY       221389 non-null  int64  \n",
      " 38  CROSSWALKKEY     221389 non-null  int64  \n",
      " 39  HITPARKEDCAR     221389 non-null  object \n",
      "dtypes: float64(5), int64(12), object(23)\n",
      "memory usage: 67.6+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time is 9.36040834999585 seconds.\n"
     ]
    }
   ],
   "source": [
    "print_elapsed_time(notebook_start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"data_wrangling\">Data Wrangling</h2>\n",
    "\n",
    "Steps for working with missing data:\n",
    "<ol>\n",
    "    <li>Identify missing data.</li>\n",
    "    <li>Deal with missing data.</li>\n",
    "    <li>Correct data format.</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 id=\"identifying_missing_data\">Identifying Missing Data</h3>\n",
    "\n",
    "The metadata document that accompanied the data set indicates that certain columns have \"sentinel\" values\n",
    "that indicate an unknown or missing value. Each of these missing values will first be converted into NaN.\n",
    "Subsequently, the NaN values will be dropped from the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If any row of the collisions DataFrame contains a sentinel value representing \"unknown\",\n",
    "# then replace it with NaN. \n",
    "# Sentinels for \"unknown\" are listed in the metadata document that accompanies the dataset.\n",
    "df_unknowns_converted_to_nan = df.replace(to_replace=\\\n",
    "{\"EXCEPTRSNCODE\": \" \",\\\n",
    " \"EXCEPTRSNDESC\": \"Not Enough Information, or Insufficient Location Information\",\\\n",
    " \"SEVERITYCODE\": \"0\",\\\n",
    " \"SEVERITYDESC\": \"Unknown\",\\\n",
    " \"JUNCTIONTYPE\": \"Unknown\",\\\n",
    " \"WEATHER\": \"Unknown\",\\\n",
    " \"ROADCOND\": \"Unknown\",\\\n",
    " \"LIGHTCOND\": \"Unknown\",\\\n",
    " \"SDOT_COLCODE\": float(0),\\\n",
    " \"SDOT_COLDESC\": \"NOT ENOUGH INFORMATION / NOT APPLICABLE\",\\\n",
    " \"ST_COLCODE\": \" \",\\\n",
    " \"ST_COLDESC\": \"Not stated\"},\\\n",
    "value=np.nan, inplace=False, limit=None, regex=False, method='pad')\n",
    "\n",
    "df_unknowns_converted_to_nan.replace(to_replace={\"ST_COLCODE\": \"0\", }, value=np.nan, inplace=True, limit=None, regex=False, method='pad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time is 16.205888537006103 seconds.\n"
     ]
    }
   ],
   "source": [
    "print_elapsed_time(notebook_start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 id=\"deal_with_missing_data\">Deal with Missing Data</h3>\n",
    "\n",
    "<ol>\n",
    "    <li>Drop the Data\n",
    "        <ol>\n",
    "            <li>Drop entire row.</li>\n",
    "            <li>Drop entire column.</li>\n",
    "        </ol>\n",
    "    </li>\n",
    "    <li>Replace the Data\n",
    "        <ol>\n",
    "            <li>Replace data by mean.</li>\n",
    "            <li>Replace data by frequency.</li>\n",
    "            <li>Replace data based on other functions.</li>\n",
    "        </ol>\n",
    "    </li>\n",
    "        \n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whole columns should be dropped only if most entries in the column are empty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['X', 'Y', 'OBJECTID', 'INCKEY', 'COLDETKEY', 'REPORTNO', 'STATUS', 'ADDRTYPE', 'INTKEY', 'LOCATION', 'EXCEPTRSNCODE', 'EXCEPTRSNDESC', 'SEVERITYCODE', 'SEVERITYDESC', 'COLLISIONTYPE', 'PERSONCOUNT', 'PEDCOUNT', 'PEDCYLCOUNT', 'VEHCOUNT', 'INJURIES', 'SERIOUSINJURIES', 'FATALITIES', 'INCDATE', 'INCDTTM', 'JUNCTIONTYPE', 'SDOT_COLCODE', 'SDOT_COLDESC', 'INATTENTIONIND', 'UNDERINFL', 'WEATHER', 'ROADCOND', 'LIGHTCOND', 'PEDROWNOTGRNT', 'SDOTCOLNUM', 'SPEEDING', 'ST_COLCODE', 'ST_COLDESC', 'SEGLANEKEY', 'CROSSWALKKEY', 'HITPARKEDCAR']\n"
     ]
    }
   ],
   "source": [
    "print(list(df.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop any column from the collisions DataFrame if it satisfies at least one of the following conditions:\n",
    "# 1) more than 15% of the column's data is NaN;\n",
    "# 2) the column only contains unique identification keys, or information not useful for model building;\n",
    "# 3) the column's data is categorical but does not fit into a small (< 15) number of categories;\n",
    "# 4) information in the column is redundant because it is already represented by another column;\n",
    "# 5) it is not clear how to interpret the column's data.\n",
    "list_of_columns_to_drop = [\"ADDRTYPE\",\\\n",
    "                           \"STATUS\",\\\n",
    "                           \"OBJECTID\",\\\n",
    "                           \"INCKEY\",\\\n",
    "                           \"COLDETKEY\",\\\n",
    "                           \"REPORTNO\",\\\n",
    "                           \"INTKEY\",\\\n",
    "                           \"LOCATION\",\\\n",
    "                           \"EXCEPTRSNCODE\",\\\n",
    "                           \"EXCEPTRSNDESC\",\\\n",
    "                           \"SEVERITYDESC\",\\\n",
    "                           \"PERSONCOUNT\",\\\n",
    "                           \"INCDATE\",\\\n",
    "                           \"INCDTTM\",\\\n",
    "                           \"JUNCTIONTYPE\",\\\n",
    "                           \"SDOT_COLCODE\",\\\n",
    "                           \"SDOT_COLDESC\",\\\n",
    "                           \"INATTENTIONIND\",\\\n",
    "                           \"UNDERINFL\",\\\n",
    "                           \"PEDROWNOTGRNT\",\\\n",
    "                           \"SDOTCOLNUM\",\\\n",
    "                           \"SPEEDING\",\\\n",
    "                           \"ST_COLCODE\",\\\n",
    "                           \"ST_COLDESC\",\\\n",
    "                           \"SEGLANEKEY\",\\\n",
    "                           \"CROSSWALKKEY\",\\\n",
    "                           \"HITPARKEDCAR\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the selected columns from the DataFrame after converting unknowns to NaN.\n",
    "# Store the result in a new DataFrame.\n",
    "df_drop_columns = df_unknowns_converted_to_nan.drop(columns=list_of_columns_to_drop, inplace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop any row that contains at least one NaN.\n",
    "df_drop_columns_and_rows = df_drop_columns.dropna(axis=\"index\", how=\"any\", thresh=None, subset=None, inplace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "minor_severity_labels = ['1', '2']\n",
    "major_severity_labels = ['2b', '3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "series_is_severe = df_drop_columns_and_rows['SEVERITYCODE'].isin(major_severity_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    168603\n",
       "True       3269\n",
       "Name: SEVERITYCODE, Length: 2, dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "series_is_severe.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a new column called 'IS_SEVERE'.\n",
    "# For each row of the DataFrame, if 'SEVERITYCODE' is '2b' or '3', then 'IS_SEVERE' gets the boolean value of True.\n",
    "# If 'SEVERITYCODE' is '1' or '2', then 'IS_SEVERE' gets the boolean value of False.\n",
    "df_drop_columns_and_rows.insert(0, 'IS_SEVERE', series_is_severe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 171872 entries, 0 to 221388\n",
      "Data columns (total 14 columns):\n",
      " #   Column           Non-Null Count   Dtype  \n",
      "---  ------           --------------   -----  \n",
      " 0   IS_SEVERE        171872 non-null  bool   \n",
      " 1   X                171872 non-null  float64\n",
      " 2   Y                171872 non-null  float64\n",
      " 3   SEVERITYCODE     171872 non-null  object \n",
      " 4   COLLISIONTYPE    171872 non-null  object \n",
      " 5   PEDCOUNT         171872 non-null  int64  \n",
      " 6   PEDCYLCOUNT      171872 non-null  int64  \n",
      " 7   VEHCOUNT         171872 non-null  int64  \n",
      " 8   INJURIES         171872 non-null  int64  \n",
      " 9   SERIOUSINJURIES  171872 non-null  int64  \n",
      " 10  FATALITIES       171872 non-null  int64  \n",
      " 11  WEATHER          171872 non-null  object \n",
      " 12  ROADCOND         171872 non-null  object \n",
      " 13  LIGHTCOND        171872 non-null  object \n",
      "dtypes: bool(1), float64(2), int64(6), object(5)\n",
      "memory usage: 18.5+ MB\n"
     ]
    }
   ],
   "source": [
    "df_drop_columns_and_rows.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>IS_SEVERE</th>\n",
       "      <th>X</th>\n",
       "      <th>Y</th>\n",
       "      <th>SEVERITYCODE</th>\n",
       "      <th>COLLISIONTYPE</th>\n",
       "      <th>PEDCOUNT</th>\n",
       "      <th>PEDCYLCOUNT</th>\n",
       "      <th>VEHCOUNT</th>\n",
       "      <th>INJURIES</th>\n",
       "      <th>SERIOUSINJURIES</th>\n",
       "      <th>FATALITIES</th>\n",
       "      <th>WEATHER</th>\n",
       "      <th>ROADCOND</th>\n",
       "      <th>LIGHTCOND</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>False</td>\n",
       "      <td>-122.320757054</td>\n",
       "      <td>47.609407946</td>\n",
       "      <td>1</td>\n",
       "      <td>Sideswipe</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Raining</td>\n",
       "      <td>Wet</td>\n",
       "      <td>Dark - Street Lights On</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>False</td>\n",
       "      <td>-122.319560827</td>\n",
       "      <td>47.662220664</td>\n",
       "      <td>1</td>\n",
       "      <td>Parked Car</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Clear</td>\n",
       "      <td>Dry</td>\n",
       "      <td>Daylight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>False</td>\n",
       "      <td>-122.374193726</td>\n",
       "      <td>47.564075600</td>\n",
       "      <td>1</td>\n",
       "      <td>Rear Ended</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Clear</td>\n",
       "      <td>Dry</td>\n",
       "      <td>Daylight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>False</td>\n",
       "      <td>-122.290734129</td>\n",
       "      <td>47.709276309</td>\n",
       "      <td>1</td>\n",
       "      <td>Other</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Clear</td>\n",
       "      <td>Wet</td>\n",
       "      <td>Daylight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>False</td>\n",
       "      <td>-122.336564829</td>\n",
       "      <td>47.590397830</td>\n",
       "      <td>1</td>\n",
       "      <td>Sideswipe</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Overcast</td>\n",
       "      <td>Dry</td>\n",
       "      <td>Daylight</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   IS_SEVERE              X            Y SEVERITYCODE COLLISIONTYPE  PEDCOUNT  \\\n",
       "0      False -122.320757054 47.609407946            1     Sideswipe         0   \n",
       "1      False -122.319560827 47.662220664            1    Parked Car         0   \n",
       "5      False -122.374193726 47.564075600            1    Rear Ended         0   \n",
       "6      False -122.290734129 47.709276309            1         Other         0   \n",
       "8      False -122.336564829 47.590397830            1     Sideswipe         0   \n",
       "\n",
       "   PEDCYLCOUNT  VEHCOUNT  INJURIES  SERIOUSINJURIES  FATALITIES   WEATHER  \\\n",
       "0            0         2         0                0           0   Raining   \n",
       "1            0         2         0                0           0     Clear   \n",
       "5            0         2         0                0           0     Clear   \n",
       "6            0         1         0                0           0     Clear   \n",
       "8            0         2         0                0           0  Overcast   \n",
       "\n",
       "  ROADCOND                LIGHTCOND  \n",
       "0      Wet  Dark - Street Lights On  \n",
       "1      Dry                 Daylight  \n",
       "5      Dry                 Daylight  \n",
       "6      Wet                 Daylight  \n",
       "8      Dry                 Daylight  \n",
       "\n",
       "[5 rows x 14 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_drop_columns_and_rows.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time is 18.33498785700067 seconds.\n"
     ]
    }
   ],
   "source": [
    "print_elapsed_time(notebook_start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 id=\"correct_data_format\">Correct Data Format</h3>\n",
    "\n",
    "Ensure that each data type is appropriate for the corresponding feature.\n",
    "Cast columns of type \"object\" as type \"category\", but leave all other column types unaltered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new DataFrame to store converted data types.\n",
    "df_converted = pd.DataFrame()\n",
    "\n",
    "for column in list(df_drop_columns_and_rows.columns):\n",
    "    if (df_drop_columns_and_rows[column].dtype in [np.dtype('object')]):\n",
    "        df_converted[column] = df_drop_columns_and_rows[column].astype('category')\n",
    "    # Copy all other columns to new DataFrame without changing their types.\n",
    "    else:\n",
    "        df_converted[column] = df_drop_columns_and_rows[column]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame of categorical or integer columns, inclusive.\n",
    "df_categorical = df_converted.select_dtypes(include=['bool', 'category', 'integer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time is 18.901598202006426 seconds.\n"
     ]
    }
   ],
   "source": [
    "print_elapsed_time(notebook_start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"feature_selection\">Feature selection</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Features before One-Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>IS_SEVERE</th>\n",
       "      <th>SEVERITYCODE</th>\n",
       "      <th>COLLISIONTYPE</th>\n",
       "      <th>PEDCOUNT</th>\n",
       "      <th>PEDCYLCOUNT</th>\n",
       "      <th>VEHCOUNT</th>\n",
       "      <th>INJURIES</th>\n",
       "      <th>SERIOUSINJURIES</th>\n",
       "      <th>FATALITIES</th>\n",
       "      <th>WEATHER</th>\n",
       "      <th>ROADCOND</th>\n",
       "      <th>LIGHTCOND</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>Sideswipe</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Raining</td>\n",
       "      <td>Wet</td>\n",
       "      <td>Dark - Street Lights On</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>Parked Car</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Clear</td>\n",
       "      <td>Dry</td>\n",
       "      <td>Daylight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>Rear Ended</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Clear</td>\n",
       "      <td>Dry</td>\n",
       "      <td>Daylight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>Other</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Clear</td>\n",
       "      <td>Wet</td>\n",
       "      <td>Daylight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>Sideswipe</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Overcast</td>\n",
       "      <td>Dry</td>\n",
       "      <td>Daylight</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   IS_SEVERE SEVERITYCODE COLLISIONTYPE  PEDCOUNT  PEDCYLCOUNT  VEHCOUNT  \\\n",
       "0      False            1     Sideswipe         0            0         2   \n",
       "1      False            1    Parked Car         0            0         2   \n",
       "5      False            1    Rear Ended         0            0         2   \n",
       "6      False            1         Other         0            0         1   \n",
       "8      False            1     Sideswipe         0            0         2   \n",
       "\n",
       "   INJURIES  SERIOUSINJURIES  FATALITIES   WEATHER ROADCOND  \\\n",
       "0         0                0           0   Raining      Wet   \n",
       "1         0                0           0     Clear      Dry   \n",
       "5         0                0           0     Clear      Dry   \n",
       "6         0                0           0     Clear      Wet   \n",
       "8         0                0           0  Overcast      Dry   \n",
       "\n",
       "                 LIGHTCOND  \n",
       "0  Dark - Street Lights On  \n",
       "1                 Daylight  \n",
       "5                 Daylight  \n",
       "6                 Daylight  \n",
       "8                 Daylight  \n",
       "\n",
       "[5 rows x 12 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_categorical.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 171872 entries, 0 to 221388\n",
      "Data columns (total 12 columns):\n",
      " #   Column           Non-Null Count   Dtype   \n",
      "---  ------           --------------   -----   \n",
      " 0   IS_SEVERE        171872 non-null  bool    \n",
      " 1   SEVERITYCODE     171872 non-null  category\n",
      " 2   COLLISIONTYPE    171872 non-null  category\n",
      " 3   PEDCOUNT         171872 non-null  int64   \n",
      " 4   PEDCYLCOUNT      171872 non-null  int64   \n",
      " 5   VEHCOUNT         171872 non-null  int64   \n",
      " 6   INJURIES         171872 non-null  int64   \n",
      " 7   SERIOUSINJURIES  171872 non-null  int64   \n",
      " 8   FATALITIES       171872 non-null  int64   \n",
      " 9   WEATHER          171872 non-null  category\n",
      " 10  ROADCOND         171872 non-null  category\n",
      " 11  LIGHTCOND        171872 non-null  category\n",
      "dtypes: bool(1), category(5), int64(6)\n",
      "memory usage: 10.2 MB\n"
     ]
    }
   ],
   "source": [
    "df_categorical.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time is 19.307110590001685 seconds.\n"
     ]
    }
   ],
   "source": [
    "print_elapsed_time(notebook_start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"imbalanced_data\">Dealing with Imbalanced Data</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the data is imbalanced, we split the DataFrame into four DataFrames, one for each value of the SEVERITYCODE feature."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(\"SEVERITYCODE relative frequencies:\")\n",
    "print(df_categorical[\"SEVERITYCODE\"].value_counts(normalize=True, dropna=False))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(\"SEVERITYCODE relative frequencies:\")\n",
    "print(df_categorical[\"SEVERITYCODE\"].value_counts(normalize=False, dropna=False))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df_class_1 = df_categorical[df_categorical['SEVERITYCODE'] == '1']\n",
    "df_class_2 = df_categorical[df_categorical['SEVERITYCODE'] == '2']\n",
    "df_class_2b = df_categorical[df_categorical['SEVERITYCODE'] == '2b']\n",
    "df_class_3 = df_categorical[df_categorical['SEVERITYCODE'] == '3']"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(\"SEVERITYCODE relative frequencies:\")\n",
    "print(df_class_1[\"SEVERITYCODE\"].value_counts(normalize=False, dropna=False))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(\"SEVERITYCODE relative frequencies:\")\n",
    "print(df_class_2[\"SEVERITYCODE\"].value_counts(normalize=False, dropna=False))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(\"SEVERITYCODE relative frequencies:\")\n",
    "print(df_class_2b[\"SEVERITYCODE\"].value_counts(normalize=False, dropna=False))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(\"SEVERITYCODE relative frequencies:\")\n",
    "print(df_class_3[\"SEVERITYCODE\"].value_counts(normalize=False, dropna=False))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Store and print the size of the all classes.\n",
    "class_1_size = len(df_class_1)\n",
    "class_2_size = len(df_class_2)\n",
    "class_2b_size = len(df_class_2b)\n",
    "class_3_size = len(df_class_3)\n",
    "print('class_1_size =', class_1_size)\n",
    "print('class_2_size =', class_2_size)\n",
    "print('class_2b_size =', class_2b_size)\n",
    "print('class_3_size =', class_3_size)\n",
    "print()\n",
    "# Store and print the size of the minority class.\n",
    "minority_class_size = len(df_class_3)\n",
    "print('minority_class_size =', minority_class_size)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print_elapsed_time(notebook_start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IS_SEVERE relative frequencies:\n",
      "False   0.980980032\n",
      "True    0.019019968\n",
      "Name: IS_SEVERE, Length: 2, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print('IS_SEVERE relative frequencies:')\n",
    "print(df_categorical['IS_SEVERE'].value_counts(normalize=True, dropna=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IS_SEVERE relative frequencies:\n",
      "False    168603\n",
      "True       3269\n",
      "Name: IS_SEVERE, Length: 2, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"IS_SEVERE relative frequencies:\")\n",
    "print(df_categorical['IS_SEVERE'].value_counts(normalize=False, dropna=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_class_major_severity = df_categorical[df_categorical['IS_SEVERE']]\n",
    "df_class_minor_severity = df_categorical[~df_categorical['IS_SEVERE']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df_class_major_severity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 3269 entries, 116 to 221277\n",
      "Data columns (total 12 columns):\n",
      " #   Column           Non-Null Count  Dtype   \n",
      "---  ------           --------------  -----   \n",
      " 0   IS_SEVERE        3269 non-null   bool    \n",
      " 1   SEVERITYCODE     3269 non-null   category\n",
      " 2   COLLISIONTYPE    3269 non-null   category\n",
      " 3   PEDCOUNT         3269 non-null   int64   \n",
      " 4   PEDCYLCOUNT      3269 non-null   int64   \n",
      " 5   VEHCOUNT         3269 non-null   int64   \n",
      " 6   INJURIES         3269 non-null   int64   \n",
      " 7   SERIOUSINJURIES  3269 non-null   int64   \n",
      " 8   FATALITIES       3269 non-null   int64   \n",
      " 9   WEATHER          3269 non-null   category\n",
      " 10  ROADCOND         3269 non-null   category\n",
      " 11  LIGHTCOND        3269 non-null   category\n",
      "dtypes: bool(1), category(5), int64(6)\n",
      "memory usage: 199.6 KB\n"
     ]
    }
   ],
   "source": [
    "df_class_major_severity.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df_class_minor_severity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 168603 entries, 0 to 221388\n",
      "Data columns (total 12 columns):\n",
      " #   Column           Non-Null Count   Dtype   \n",
      "---  ------           --------------   -----   \n",
      " 0   IS_SEVERE        168603 non-null  bool    \n",
      " 1   SEVERITYCODE     168603 non-null  category\n",
      " 2   COLLISIONTYPE    168603 non-null  category\n",
      " 3   PEDCOUNT         168603 non-null  int64   \n",
      " 4   PEDCYLCOUNT      168603 non-null  int64   \n",
      " 5   VEHCOUNT         168603 non-null  int64   \n",
      " 6   INJURIES         168603 non-null  int64   \n",
      " 7   SERIOUSINJURIES  168603 non-null  int64   \n",
      " 8   FATALITIES       168603 non-null  int64   \n",
      " 9   WEATHER          168603 non-null  category\n",
      " 10  ROADCOND         168603 non-null  category\n",
      " 11  LIGHTCOND        168603 non-null  category\n",
      "dtypes: bool(1), category(5), int64(6)\n",
      "memory usage: 10.0 MB\n"
     ]
    }
   ],
   "source": [
    "df_class_minor_severity.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IS_SEVERE relative frequencies:\n",
      "True    3269\n",
      "Name: IS_SEVERE, Length: 1, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"IS_SEVERE relative frequencies:\")\n",
    "print(df_class_major_severity[\"IS_SEVERE\"].value_counts(normalize=False, dropna=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IS_SEVERE relative frequencies:\n",
      "False    168603\n",
      "Name: IS_SEVERE, Length: 1, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"IS_SEVERE relative frequencies:\")\n",
    "print(df_class_minor_severity[\"IS_SEVERE\"].value_counts(normalize=False, dropna=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class_major_severity_size = 3269\n",
      "class_minor_severity_size = 168603\n",
      "\n",
      "minority_class_size = 3269\n"
     ]
    }
   ],
   "source": [
    "# Store and print the size of the all classes.\n",
    "class_major_severity_size = len(df_class_major_severity)\n",
    "class_minor_severity_size = len(df_class_minor_severity)\n",
    "print('class_major_severity_size =', class_major_severity_size)\n",
    "print('class_minor_severity_size =', class_minor_severity_size)\n",
    "print()\n",
    "# Store and print the size of the minority class.\n",
    "minority_class_size = len(df_class_major_severity)\n",
    "print('minority_class_size =', minority_class_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time is 20.526612133995513 seconds.\n"
     ]
    }
   ],
   "source": [
    "print_elapsed_time(notebook_start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id='sample_size_class_3'>Sample all Classes Equally to Create a Balanced Training Set<\\h2>"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "# This parameter determines what fraction of the minority class's data are reserved for testing.\n",
    "test_size_ratio = .2\n",
    "\n",
    "# Using train/test splits, set aside part of the data for testing.\n",
    "df_class_1_train_pre_sampling, df_class_1_test = train_test_split(df_class_1, test_size=test_size_ratio, random_state=seed)\n",
    "df_class_2_train_pre_sampling, df_class_2_test = train_test_split(df_class_2, test_size=test_size_ratio, random_state=seed)\n",
    "df_class_2b_train_pre_sampling, df_class_2b_test = train_test_split(df_class_2b, test_size=test_size_ratio, random_state=seed)\n",
    "df_class_3_train_pre_sampling, df_class_3_test = train_test_split(df_class_3, test_size=test_size_ratio, random_state=seed)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "# Build a balanced training set by sampling the imbalanced training data equally by class:\n",
    "\n",
    "# Set a sampling parameter to control the amount of oversampling for the minority class.\n",
    "# Oversampling requires this parameter to be greater than 1.\n",
    "# If this parameter is less than or equal to 1, then oversampling will not occur.\n",
    "sampling_ratio = 10 # 10 is the number of folds for cross-valiation\n",
    "print('sampling_ratio = %f' % (sampling_ratio))\n",
    "\n",
    "# Set the replace parameter of the sample method based on the sampling ratio.\n",
    "# If the sampling_ratio > 1, sample with replacement.\n",
    "# Otherwise, sample without replacement.\n",
    "sample_with_replacement = bool(sampling_ratio > 1)\n",
    "# Print the value of the replace parameter before passing it to the sample method.\n",
    "print('Sample class 3 with replacement: %s' % (sample_with_replacement))\n",
    "\n",
    "# Sample the minority class's training set based on the sampling parameter and store the sample for later concatenation.\n",
    "df_class_3_train = df_class_3_train_pre_sampling.sample(frac=sampling_ratio, replace=sample_with_replacement, axis='index', random_state=seed)\n",
    "\n",
    "# Store the size of the minority class's training set.\n",
    "class_3_train_size = df_class_3_train.shape[0]\n",
    "\n",
    "# Take samples of the other classes' training data, where the sample size taken is equal to the size of the minority class's training set.\n",
    "# If the sample size to be taken exceeds the number of samples in the class's training data, sample with replacement.\n",
    "\n",
    "# Sampling for class 2b.\n",
    "sample_with_replacement = bool(class_3_train_size > df_class_2b_train_pre_sampling.shape[0])\n",
    "# Print the value of the replace parameter before passing it to the sample method.\n",
    "print('Sample class 2b with replacement: %s' % (sample_with_replacement))\n",
    "df_class_2b_train = df_class_2b_train_pre_sampling.sample(n=class_3_train_size, replace=sample_with_replacement, axis='index', random_state=seed)\n",
    "\n",
    "# Sampling for class 2.\n",
    "sample_with_replacement = bool(class_3_train_size > df_class_2_train_pre_sampling.shape[0])\n",
    "# Print the value of the replace parameter before passing it to the sample method.\n",
    "print('Sample class 2 with replacement: %s' % (sample_with_replacement))\n",
    "df_class_2_train = df_class_2_train_pre_sampling.sample(n=class_3_train_size, replace=sample_with_replacement, axis='index', random_state=seed)\n",
    "\n",
    "# Sampling for class 1.\n",
    "sample_with_replacement = bool(class_3_train_size > df_class_1_train_pre_sampling.shape[0])\n",
    "# Print the value of the replace parameter before passing it to the sample method.\n",
    "print('Sample class 1 with replacement: %s' % (sample_with_replacement))\n",
    "df_class_1_train = df_class_1_train_pre_sampling.sample(n=class_3_train_size, replace=sample_with_replacement, axis='index', random_state=seed)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "# Make a balanced, unshuffled training set by concatenating the equal sized samples of the training sets for each class.\n",
    "df_train_balanced_not_shuffled = pd.concat([df_class_1_train, df_class_2_train, df_class_2b_train, df_class_3_train], axis='index')\n",
    "\n",
    "# Make a not necessarily balanced testing set by concatenating the testing sets for each class.\n",
    "df_test_not_shuffled = pd.concat([df_class_1_test, df_class_2_test, df_class_2b_test, df_class_3_test], axis='index')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "# Shuffle the training set and store it for tuning hyper-parameters and for cross-validation.\n",
    "df_train = shuffle(df_train_balanced_not_shuffled, random_state=seed)\n",
    "\n",
    "# Shuffle the unbalanced DataFrame and store it for validation and for comparing the models.\n",
    "df_test = shuffle(df_test_not_shuffled, random_state=seed)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "# Verify the training DataFrame is balanced.\n",
    "print(\"SEVERITYCODE relative frequencies:\")\n",
    "print(df_train[\"SEVERITYCODE\"].value_counts(normalize=False, dropna=False))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "# Verify the new DataFrame is balanced.\n",
    "print(\"SEVERITYCODE relative frequencies:\")\n",
    "print(df_train[\"SEVERITYCODE\"].value_counts(normalize=True, dropna=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number_of_folds = 5\n"
     ]
    }
   ],
   "source": [
    "# Number of folds for cross-validation\n",
    "number_of_folds = 5\n",
    "print('number_of_folds = %d' % (number_of_folds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size_of_test_set = 20000\n",
      "test_size_ratio = 0.116366\n"
     ]
    }
   ],
   "source": [
    "# Parameter to set the number of samples in training set.\n",
    "size_of_test_set = 20000 # Number of samples in the test set.\n",
    "\n",
    "# This parameter determines what fraction of the data is reserved for testing.\n",
    "# It must be between 0 and 1, exclusive, i.e. 0 < test_size_ratio < 1.\n",
    "test_size_ratio = size_of_test_set / len(df_categorical)\n",
    "print('size_of_test_set = %d' % (size_of_test_set))\n",
    "print('test_size_ratio = %f' % (test_size_ratio))\n",
    "\n",
    "# Using train/test splits, set aside part of the data for testing.\n",
    "df_class_minor_severity_train_pre_sampling, df_class_minor_severity_test =\\\n",
    "    train_test_split(df_class_minor_severity, test_size=test_size_ratio, random_state=seed)\n",
    "\n",
    "df_class_major_severity_train_pre_sampling, df_class_major_severity_test =\\\n",
    "    train_test_split(df_class_major_severity, test_size=test_size_ratio, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size_of_training_set = 400\n",
      "sampling_ratio = 0.069252\n",
      "Sample class major severity with replacement: False\n",
      "Sample class minor_severity with replacement: False\n"
     ]
    }
   ],
   "source": [
    "# Build a balanced training set by sampling the imbalanced training data equally by class:\n",
    "\n",
    "# Parameter to set the number of samples in training set.\n",
    "size_of_training_set = 400\n",
    "\n",
    "# Set a sampling parameter to control how much of the non-testing data is actually used for training.\n",
    "# Oversampling of the minority class requires this parameter to be greater than 1.\n",
    "# If this parameter is less than or equal to 1, then oversampling will not occur.\n",
    "# The factor of 2 in the expression below accounts for the fact that the final training set will be balanced\n",
    "# across the two classes of accident severity.\n",
    "sampling_ratio = size_of_training_set / (2 * len(df_class_major_severity_train_pre_sampling)) \n",
    "print('size_of_training_set = %d' % (size_of_training_set))\n",
    "print('sampling_ratio = %f' % (sampling_ratio))\n",
    "\n",
    "# Set the replace parameter of the sample method based on the sampling ratio.\n",
    "# If the sampling_ratio > 1, sample with replacement.\n",
    "# Otherwise, sample without replacement.\n",
    "sample_with_replacement = bool(sampling_ratio > 1)\n",
    "# Print the value of the replace parameter before passing it to the sample method.\n",
    "print('Sample class major severity with replacement: %s' % (sample_with_replacement))\n",
    "\n",
    "# Sample the minority class's training set based on the sampling parameter and store the sample for later concatenation.\n",
    "df_class_major_severity_train = df_class_major_severity_train_pre_sampling.sample(frac=sampling_ratio,\\\n",
    "                                                                                  replace=sample_with_replacement,\\\n",
    "                                                                                  axis='index',\\\n",
    "                                                                                  random_state=seed)\n",
    "\n",
    "# Store the size of the minority class's training set.\n",
    "df_class_major_severity_train_size = df_class_major_severity_train.shape[0]\n",
    "\n",
    "# Take a sample of the other class's pre-sampling training data,\n",
    "# where the sample size taken is equal to the size of the minority class's training set.\n",
    "# If the sample size to be taken exceeds the number of samples in available, sample with replacement.\n",
    "\n",
    "# Sampling for class minor_severity\n",
    "sample_with_replacement = bool(df_class_major_severity_train_size > df_class_minor_severity_train_pre_sampling.shape[0])\n",
    "# Print the value of the replace parameter before passing it to the sample method.\n",
    "print('Sample class minor_severity with replacement: %s' % (sample_with_replacement))\n",
    "df_class_minor_severity_train = df_class_minor_severity_train_pre_sampling.sample(n=df_class_major_severity_train_size,\\\n",
    "                                                                                   replace=sample_with_replacement, axis='index',\\\n",
    "                                                                                   random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a balanced, unshuffled training set by concatenating the equal sized samples of the training sets for each class.\n",
    "df_train_balanced_not_shuffled = pd.concat([df_class_minor_severity_train, df_class_major_severity_train], axis='index')\n",
    "\n",
    "# Make a not necessarily balanced testing set by concatenating the testing sets for each class.\n",
    "df_test_not_shuffled = pd.concat([df_class_minor_severity_test, df_class_major_severity_test], axis='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle the training set and store it for tuning hyper-parameters and for cross-validation.\n",
    "df_train = shuffle(df_train_balanced_not_shuffled, random_state=seed)\n",
    "\n",
    "# Shuffle the unbalanced DataFrame and store it for validation and for comparing the models.\n",
    "df_test = shuffle(df_test_not_shuffled, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IS_SEVERE relative frequencies:\n",
      "True     200\n",
      "False    200\n",
      "Name: IS_SEVERE, Length: 2, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Verify the training DataFrame is balanced.\n",
    "print(\"IS_SEVERE relative frequencies:\")\n",
    "print(df_train[\"IS_SEVERE\"].value_counts(normalize=False, dropna=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IS_SEVERE relative frequencies:\n",
      "True    0.500000000\n",
      "False   0.500000000\n",
      "Name: IS_SEVERE, Length: 2, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Verify the new DataFrame is balanced.\n",
    "print(\"IS_SEVERE relative frequencies:\")\n",
    "print(df_train[\"IS_SEVERE\"].value_counts(normalize=True, dropna=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define a feature set represented by the DataFrame X. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_train.drop(columns=['IS_SEVERE', 'SEVERITYCODE', 'INJURIES', 'SERIOUSINJURIES', 'FATALITIES'], inplace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 400 entries, 129303 to 127194\n",
      "Data columns (total 7 columns):\n",
      " #   Column         Non-Null Count  Dtype   \n",
      "---  ------         --------------  -----   \n",
      " 0   COLLISIONTYPE  400 non-null    category\n",
      " 1   PEDCOUNT       400 non-null    int64   \n",
      " 2   PEDCYLCOUNT    400 non-null    int64   \n",
      " 3   VEHCOUNT       400 non-null    int64   \n",
      " 4   WEATHER        400 non-null    category\n",
      " 5   ROADCOND       400 non-null    category\n",
      " 6   LIGHTCOND      400 non-null    category\n",
      "dtypes: category(4), int64(3)\n",
      "memory usage: 15.6 KB\n"
     ]
    }
   ],
   "source": [
    "X.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also define the data for the target variable, SEVERITYCODE, by the array y:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "y = df_train[\"SEVERITYCODE\"].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df_train['IS_SEVERE'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time is 21.886095701003796 seconds.\n"
     ]
    }
   ],
   "source": [
    "print_elapsed_time(notebook_start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform the Data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We normalize the data by transforming it so that it is compatible\n",
    "with the machine learning estimators we use in this notebook.\n",
    "We use special care with sparse matrix data so as to not destroy the\n",
    "structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting OneHotEncoder to training data...\n",
      "Completed in 0.22457394799857866 seconds.\n",
      "['x0_Angles' 'x0_Cycles' 'x0_Head On' 'x0_Left Turn' 'x0_Other'\n",
      " 'x0_Parked Car' 'x0_Pedestrian' 'x0_Rear Ended' 'x0_Right Turn'\n",
      " 'x0_Sideswipe' 'x1_0' 'x1_1' 'x1_2' 'x1_3' 'x1_4' 'x1_5' 'x1_6' 'x2_0'\n",
      " 'x2_1' 'x2_2' 'x3_0' 'x3_1' 'x3_2' 'x3_3' 'x3_4' 'x3_5' 'x3_6' 'x3_7'\n",
      " 'x3_8' 'x3_9' 'x3_10' 'x3_11' 'x3_12' 'x3_13' 'x3_14' 'x3_15'\n",
      " 'x4_Blowing Sand/Dirt' 'x4_Clear' 'x4_Fog/Smog/Smoke' 'x4_Other'\n",
      " 'x4_Overcast' 'x4_Partly Cloudy' 'x4_Raining' 'x4_Severe Crosswind'\n",
      " 'x4_Sleet/Hail/Freezing Rain' 'x4_Snowing' 'x5_Dry' 'x5_Ice' 'x5_Oil'\n",
      " 'x5_Other' 'x5_Sand/Mud/Dirt' 'x5_Snow/Slush' 'x5_Standing Water'\n",
      " 'x5_Wet' 'x6_Dark - No Street Lights' 'x6_Dark - Street Lights Off'\n",
      " 'x6_Dark - Street Lights On' 'x6_Dark - Unknown Lighting' 'x6_Dawn'\n",
      " 'x6_Daylight' 'x6_Dusk' 'x6_Other']\n"
     ]
    }
   ],
   "source": [
    "# Create a OneHotEncoder and fit it to the features.\n",
    "# The fit is performed on the data set before the any test/train splits.\n",
    "# The data will be encoded as a sparse matrix, the default behavior.\n",
    "start_time = default_timer()\n",
    "print(\"Fitting OneHotEncoder to training data...\")\n",
    "encoder = OneHotEncoder(sparse=True, handle_unknown='error')\n",
    "encoder.fit(df_categorical.drop(columns=['IS_SEVERE', 'SEVERITYCODE', 'INJURIES', 'SERIOUSINJURIES', 'FATALITIES'], inplace=False))\n",
    "#encoder.fit(X)\n",
    "X = encoder.transform(X)\n",
    "print(\"Completed in\", elapsed_time(start_time), \"seconds.\")\n",
    "# Display the categories of the encoder.\n",
    "print(encoder.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scipy.sparse.csr.csr_matrix"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400, 62)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400,)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time is 22.59077660999901 seconds.\n"
     ]
    }
   ],
   "source": [
    "print_elapsed_time(notebook_start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"logistic_regression\">Building a Logistic Regression Model<\\h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression score keys: ['estimator', 'fit_time', 'score_time', 'test_f1_macro', 'test_f1_weighted', 'test_neg_log_loss', 'test_precision_macro', 'test_precision_weighted', 'test_recall_macro', 'test_recall_weighted']\n",
      "Logistic Regression classifiers constructed in 8.046710 seconds.\n"
     ]
    }
   ],
   "source": [
    "t0 = default_timer()\n",
    "logistic_regression_clf = make_pipeline(\\\n",
    "    StandardScaler(with_mean=False),\\\n",
    "    LogisticRegression(solver='saga', multi_class='auto', penalty='l1', max_iter=10000, verbose=1000, n_jobs=-1, random_state=seed), verbose=True)\n",
    "scoring = ['f1_macro', 'f1_weighted', 'neg_log_loss', 'precision_macro', 'precision_weighted', 'recall_macro', 'recall_weighted']\n",
    "logistic_regression_scores = cross_validate(logistic_regression_clf, X, y, scoring=scoring, n_jobs=-1, cv=number_of_folds, return_estimator=True)\n",
    "print(\"Logistic regression score keys:\", sorted(logistic_regression_scores.keys()))\n",
    "print('Logistic Regression classifiers constructed in %f seconds.' % elapsed_time(t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time is 30.771631406998495 seconds.\n"
     ]
    }
   ],
   "source": [
    "print_elapsed_time(notebook_start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"support_vector_machine\">Building a Support Vector Machine<\\h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Support Vector Machine score keys: ['estimator', 'fit_time', 'score_time', 'test_f1_macro', 'test_f1_weighted', 'test_precision_macro', 'test_precision_weighted', 'test_recall_macro', 'test_recall_weighted']\n",
      "Support Vector Machine classifiers constructed in 0.399033 seconds.\n"
     ]
    }
   ],
   "source": [
    "t0 = default_timer()\n",
    "linear_svc_clf = make_pipeline(\\\n",
    "    StandardScaler(with_mean=False),\\\n",
    "    LinearSVC(penalty='l1', dual=False, random_state=seed, max_iter=10000), verbose=True)\n",
    "scoring = ['f1_macro', 'f1_weighted', 'precision_macro', 'precision_weighted', 'recall_macro', 'recall_weighted']\n",
    "linear_svc_scores = cross_validate(linear_svc_clf, X, y, scoring=scoring, n_jobs=-1, cv=number_of_folds, return_estimator=True)\n",
    "print(\"Support Vector Machine score keys:\", sorted(linear_svc_scores.keys()))\n",
    "print('Support Vector Machine classifiers constructed in %f seconds.' % elapsed_time(t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time is 31.321836924995296 seconds.\n"
     ]
    }
   ],
   "source": [
    "print_elapsed_time(notebook_start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"knn_classifier\">Building a k-Nearsest Neighbors Classifier<\\h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum_number_of_neighbors = 79\n",
      "Fitting 5 folds for each of 39 candidates, totalling 195 fits\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed:    4.0s\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of 195 | elapsed:    4.1s remaining:  6.6min\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of 195 | elapsed:    4.1s remaining:  4.4min\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of 195 | elapsed:    4.2s remaining:  3.3min\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of 195 | elapsed:    4.3s remaining:  2.7min\n",
      "[Parallel(n_jobs=-1)]: Done   6 out of 195 | elapsed:    4.3s remaining:  2.3min\n",
      "[Parallel(n_jobs=-1)]: Done   7 out of 195 | elapsed:    4.4s remaining:  2.0min\n",
      "[Parallel(n_jobs=-1)]: Done   8 out of 195 | elapsed:    4.5s remaining:  1.7min\n",
      "[Parallel(n_jobs=-1)]: Done   9 out of 195 | elapsed:    4.6s remaining:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of 195 | elapsed:    4.6s remaining:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done  11 out of 195 | elapsed:    4.7s remaining:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done  12 out of 195 | elapsed:    4.7s remaining:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done  13 out of 195 | elapsed:    4.8s remaining:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done  14 out of 195 | elapsed:    4.8s remaining:  1.0min\n",
      "[Parallel(n_jobs=-1)]: Done  15 out of 195 | elapsed:    5.0s remaining:   59.4s\n",
      "[Parallel(n_jobs=-1)]: Done  16 out of 195 | elapsed:    5.0s remaining:   56.2s\n",
      "[Parallel(n_jobs=-1)]: Done  17 out of 195 | elapsed:    5.1s remaining:   53.4s\n",
      "[Parallel(n_jobs=-1)]: Done  18 out of 195 | elapsed:    5.2s remaining:   50.7s\n",
      "[Parallel(n_jobs=-1)]: Done  19 out of 195 | elapsed:    5.2s remaining:   48.3s\n",
      "[Parallel(n_jobs=-1)]: Done  20 out of 195 | elapsed:    5.3s remaining:   46.2s\n",
      "[Parallel(n_jobs=-1)]: Done  21 out of 195 | elapsed:    5.4s remaining:   44.7s\n",
      "[Parallel(n_jobs=-1)]: Done  22 out of 195 | elapsed:    5.4s remaining:   42.5s\n",
      "[Parallel(n_jobs=-1)]: Done  23 out of 195 | elapsed:    5.5s remaining:   41.3s\n",
      "[Parallel(n_jobs=-1)]: Done  24 out of 195 | elapsed:    5.6s remaining:   39.6s\n",
      "[Parallel(n_jobs=-1)]: Done  25 out of 195 | elapsed:    5.6s remaining:   38.3s\n",
      "[Parallel(n_jobs=-1)]: Done  26 out of 195 | elapsed:    5.7s remaining:   37.1s\n",
      "[Parallel(n_jobs=-1)]: Done  27 out of 195 | elapsed:    5.8s remaining:   36.1s\n",
      "[Parallel(n_jobs=-1)]: Done  28 out of 195 | elapsed:    5.9s remaining:   35.0s\n",
      "[Parallel(n_jobs=-1)]: Done  29 out of 195 | elapsed:    5.9s remaining:   33.9s\n",
      "[Parallel(n_jobs=-1)]: Done  30 out of 195 | elapsed:    6.0s remaining:   33.1s\n",
      "[Parallel(n_jobs=-1)]: Done  31 out of 195 | elapsed:    6.1s remaining:   32.4s\n",
      "[Parallel(n_jobs=-1)]: Done  32 out of 195 | elapsed:    6.2s remaining:   31.6s\n",
      "[Parallel(n_jobs=-1)]: Done  33 out of 195 | elapsed:    6.2s remaining:   30.6s\n",
      "[Parallel(n_jobs=-1)]: Done  34 out of 195 | elapsed:    6.3s remaining:   29.9s\n",
      "[Parallel(n_jobs=-1)]: Done  35 out of 195 | elapsed:    6.4s remaining:   29.1s\n",
      "[Parallel(n_jobs=-1)]: Done  36 out of 195 | elapsed:    6.5s remaining:   28.6s\n",
      "[Parallel(n_jobs=-1)]: Done  37 out of 195 | elapsed:    6.5s remaining:   27.7s\n",
      "[Parallel(n_jobs=-1)]: Done  38 out of 195 | elapsed:    6.6s remaining:   27.1s\n",
      "[Parallel(n_jobs=-1)]: Done  39 out of 195 | elapsed:    6.6s remaining:   26.5s\n",
      "[Parallel(n_jobs=-1)]: Done  40 out of 195 | elapsed:    6.7s remaining:   25.8s\n",
      "[Parallel(n_jobs=-1)]: Done  41 out of 195 | elapsed:    6.7s remaining:   25.2s\n",
      "[Parallel(n_jobs=-1)]: Done  42 out of 195 | elapsed:    6.8s remaining:   24.7s\n",
      "[Parallel(n_jobs=-1)]: Done  43 out of 195 | elapsed:    6.8s remaining:   24.1s\n",
      "[Parallel(n_jobs=-1)]: Done  44 out of 195 | elapsed:    6.9s remaining:   23.6s\n",
      "[Parallel(n_jobs=-1)]: Done  45 out of 195 | elapsed:    7.0s remaining:   23.2s\n",
      "[Parallel(n_jobs=-1)]: Done  46 out of 195 | elapsed:    7.0s remaining:   22.8s\n",
      "[Parallel(n_jobs=-1)]: Done  47 out of 195 | elapsed:    7.1s remaining:   22.4s\n",
      "[Parallel(n_jobs=-1)]: Done  48 out of 195 | elapsed:    7.2s remaining:   22.0s\n",
      "[Parallel(n_jobs=-1)]: Done  49 out of 195 | elapsed:    7.2s remaining:   21.5s\n",
      "[Parallel(n_jobs=-1)]: Done  50 out of 195 | elapsed:    7.3s remaining:   21.1s\n",
      "[Parallel(n_jobs=-1)]: Done  51 out of 195 | elapsed:    7.3s remaining:   20.7s\n",
      "[Parallel(n_jobs=-1)]: Done  52 out of 195 | elapsed:    7.4s remaining:   20.3s\n",
      "[Parallel(n_jobs=-1)]: Done  53 out of 195 | elapsed:    7.5s remaining:   20.0s\n",
      "[Parallel(n_jobs=-1)]: Done  54 out of 195 | elapsed:    7.5s remaining:   19.6s\n",
      "[Parallel(n_jobs=-1)]: Done  55 out of 195 | elapsed:    7.6s remaining:   19.3s\n",
      "[Parallel(n_jobs=-1)]: Done  56 out of 195 | elapsed:    7.6s remaining:   18.9s\n",
      "[Parallel(n_jobs=-1)]: Done  57 out of 195 | elapsed:    7.7s remaining:   18.6s\n",
      "[Parallel(n_jobs=-1)]: Done  58 out of 195 | elapsed:    7.7s remaining:   18.3s\n",
      "[Parallel(n_jobs=-1)]: Done  59 out of 195 | elapsed:    7.8s remaining:   18.0s\n",
      "[Parallel(n_jobs=-1)]: Done  60 out of 195 | elapsed:    7.8s remaining:   17.6s\n",
      "[Parallel(n_jobs=-1)]: Done  61 out of 195 | elapsed:    7.9s remaining:   17.4s\n",
      "[Parallel(n_jobs=-1)]: Done  62 out of 195 | elapsed:    8.0s remaining:   17.1s\n",
      "[Parallel(n_jobs=-1)]: Done  63 out of 195 | elapsed:    8.0s remaining:   16.8s\n",
      "[Parallel(n_jobs=-1)]: Done  64 out of 195 | elapsed:    8.1s remaining:   16.6s\n",
      "[Parallel(n_jobs=-1)]: Done  65 out of 195 | elapsed:    8.2s remaining:   16.3s\n",
      "[Parallel(n_jobs=-1)]: Done  66 out of 195 | elapsed:    8.2s remaining:   16.0s\n",
      "[Parallel(n_jobs=-1)]: Done  67 out of 195 | elapsed:    8.3s remaining:   15.9s\n",
      "[Parallel(n_jobs=-1)]: Done  68 out of 195 | elapsed:    8.3s remaining:   15.6s\n",
      "[Parallel(n_jobs=-1)]: Done  69 out of 195 | elapsed:    8.4s remaining:   15.4s\n",
      "[Parallel(n_jobs=-1)]: Done  70 out of 195 | elapsed:    8.4s remaining:   15.1s\n",
      "[Parallel(n_jobs=-1)]: Done  71 out of 195 | elapsed:    8.5s remaining:   14.9s\n",
      "[Parallel(n_jobs=-1)]: Done  72 out of 195 | elapsed:    8.5s remaining:   14.6s\n",
      "[Parallel(n_jobs=-1)]: Done  73 out of 195 | elapsed:    8.7s remaining:   14.5s\n",
      "[Parallel(n_jobs=-1)]: Done  74 out of 195 | elapsed:    8.7s remaining:   14.2s\n",
      "[Parallel(n_jobs=-1)]: Done  75 out of 195 | elapsed:    8.8s remaining:   14.1s\n",
      "[Parallel(n_jobs=-1)]: Done  76 out of 195 | elapsed:    8.9s remaining:   13.9s\n",
      "[Parallel(n_jobs=-1)]: Done  77 out of 195 | elapsed:    8.9s remaining:   13.7s\n",
      "[Parallel(n_jobs=-1)]: Done  78 out of 195 | elapsed:    8.9s remaining:   13.4s\n",
      "[Parallel(n_jobs=-1)]: Done  79 out of 195 | elapsed:    9.0s remaining:   13.2s\n",
      "[Parallel(n_jobs=-1)]: Done  80 out of 195 | elapsed:    9.1s remaining:   13.0s\n",
      "[Parallel(n_jobs=-1)]: Done  81 out of 195 | elapsed:    9.2s remaining:   12.9s\n",
      "[Parallel(n_jobs=-1)]: Done  82 out of 195 | elapsed:    9.2s remaining:   12.6s\n",
      "[Parallel(n_jobs=-1)]: Done  83 out of 195 | elapsed:    9.3s remaining:   12.5s\n",
      "[Parallel(n_jobs=-1)]: Done  84 out of 195 | elapsed:    9.3s remaining:   12.3s\n",
      "[Parallel(n_jobs=-1)]: Done  85 out of 195 | elapsed:    9.4s remaining:   12.1s\n",
      "[Parallel(n_jobs=-1)]: Done  86 out of 195 | elapsed:    9.4s remaining:   11.9s\n",
      "[Parallel(n_jobs=-1)]: Done  87 out of 195 | elapsed:    9.5s remaining:   11.8s\n",
      "[Parallel(n_jobs=-1)]: Done  88 out of 195 | elapsed:    9.5s remaining:   11.5s\n",
      "[Parallel(n_jobs=-1)]: Done  89 out of 195 | elapsed:    9.6s remaining:   11.5s\n",
      "[Parallel(n_jobs=-1)]: Done  90 out of 195 | elapsed:    9.6s remaining:   11.2s\n",
      "[Parallel(n_jobs=-1)]: Done  91 out of 195 | elapsed:    9.7s remaining:   11.1s\n",
      "[Parallel(n_jobs=-1)]: Done  92 out of 195 | elapsed:    9.7s remaining:   10.9s\n",
      "[Parallel(n_jobs=-1)]: Done  93 out of 195 | elapsed:    9.8s remaining:   10.8s\n",
      "[Parallel(n_jobs=-1)]: Done  94 out of 195 | elapsed:    9.9s remaining:   10.6s\n",
      "[Parallel(n_jobs=-1)]: Done  95 out of 195 | elapsed:    9.9s remaining:   10.5s\n",
      "[Parallel(n_jobs=-1)]: Done  96 out of 195 | elapsed:   10.0s remaining:   10.3s\n",
      "[Parallel(n_jobs=-1)]: Done  97 out of 195 | elapsed:   10.1s remaining:   10.2s\n",
      "[Parallel(n_jobs=-1)]: Done  98 out of 195 | elapsed:   10.1s remaining:   10.0s\n",
      "[Parallel(n_jobs=-1)]: Done  99 out of 195 | elapsed:   10.2s remaining:    9.9s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 195 | elapsed:   10.2s remaining:    9.7s\n",
      "[Parallel(n_jobs=-1)]: Done 101 out of 195 | elapsed:   10.3s remaining:    9.5s\n",
      "[Parallel(n_jobs=-1)]: Done 102 out of 195 | elapsed:   10.3s remaining:    9.4s\n",
      "[Parallel(n_jobs=-1)]: Done 103 out of 195 | elapsed:   10.3s remaining:    9.2s\n",
      "[Parallel(n_jobs=-1)]: Done 104 out of 195 | elapsed:   10.4s remaining:    9.1s\n",
      "[Parallel(n_jobs=-1)]: Done 105 out of 195 | elapsed:   10.5s remaining:    9.0s\n",
      "[Parallel(n_jobs=-1)]: Done 106 out of 195 | elapsed:   10.5s remaining:    8.8s\n",
      "[Parallel(n_jobs=-1)]: Done 107 out of 195 | elapsed:   10.6s remaining:    8.7s\n",
      "[Parallel(n_jobs=-1)]: Done 108 out of 195 | elapsed:   10.6s remaining:    8.6s\n",
      "[Parallel(n_jobs=-1)]: Done 109 out of 195 | elapsed:   10.7s remaining:    8.4s\n",
      "[Parallel(n_jobs=-1)]: Done 110 out of 195 | elapsed:   10.7s remaining:    8.3s\n",
      "[Parallel(n_jobs=-1)]: Done 111 out of 195 | elapsed:   10.8s remaining:    8.2s\n",
      "[Parallel(n_jobs=-1)]: Done 112 out of 195 | elapsed:   10.9s remaining:    8.1s\n",
      "[Parallel(n_jobs=-1)]: Done 113 out of 195 | elapsed:   11.0s remaining:    8.0s\n",
      "[Parallel(n_jobs=-1)]: Done 114 out of 195 | elapsed:   11.1s remaining:    7.9s\n",
      "[Parallel(n_jobs=-1)]: Done 115 out of 195 | elapsed:   11.1s remaining:    7.7s\n",
      "[Parallel(n_jobs=-1)]: Done 116 out of 195 | elapsed:   11.2s remaining:    7.6s\n",
      "[Parallel(n_jobs=-1)]: Done 117 out of 195 | elapsed:   11.2s remaining:    7.5s\n",
      "[Parallel(n_jobs=-1)]: Done 118 out of 195 | elapsed:   11.3s remaining:    7.4s\n",
      "[Parallel(n_jobs=-1)]: Done 119 out of 195 | elapsed:   11.3s remaining:    7.2s\n",
      "[Parallel(n_jobs=-1)]: Done 120 out of 195 | elapsed:   11.4s remaining:    7.1s\n",
      "[Parallel(n_jobs=-1)]: Done 121 out of 195 | elapsed:   11.5s remaining:    7.0s\n",
      "[Parallel(n_jobs=-1)]: Done 122 out of 195 | elapsed:   11.6s remaining:    6.9s\n",
      "[Parallel(n_jobs=-1)]: Done 123 out of 195 | elapsed:   11.6s remaining:    6.8s\n",
      "[Parallel(n_jobs=-1)]: Done 124 out of 195 | elapsed:   11.7s remaining:    6.7s\n",
      "[Parallel(n_jobs=-1)]: Done 125 out of 195 | elapsed:   11.7s remaining:    6.6s\n",
      "[Parallel(n_jobs=-1)]: Done 126 out of 195 | elapsed:   11.8s remaining:    6.5s\n",
      "[Parallel(n_jobs=-1)]: Done 127 out of 195 | elapsed:   11.8s remaining:    6.3s\n",
      "[Parallel(n_jobs=-1)]: Done 128 out of 195 | elapsed:   11.9s remaining:    6.3s\n",
      "[Parallel(n_jobs=-1)]: Done 129 out of 195 | elapsed:   12.0s remaining:    6.1s\n",
      "[Parallel(n_jobs=-1)]: Done 130 out of 195 | elapsed:   12.1s remaining:    6.1s\n",
      "[Parallel(n_jobs=-1)]: Done 131 out of 195 | elapsed:   12.1s remaining:    5.9s\n",
      "[Parallel(n_jobs=-1)]: Done 132 out of 195 | elapsed:   12.2s remaining:    5.8s\n",
      "[Parallel(n_jobs=-1)]: Done 133 out of 195 | elapsed:   12.3s remaining:    5.7s\n",
      "[Parallel(n_jobs=-1)]: Done 134 out of 195 | elapsed:   12.3s remaining:    5.6s\n",
      "[Parallel(n_jobs=-1)]: Done 135 out of 195 | elapsed:   12.4s remaining:    5.5s\n",
      "[Parallel(n_jobs=-1)]: Done 136 out of 195 | elapsed:   12.5s remaining:    5.4s\n",
      "[Parallel(n_jobs=-1)]: Done 137 out of 195 | elapsed:   12.6s remaining:    5.3s\n",
      "[Parallel(n_jobs=-1)]: Done 138 out of 195 | elapsed:   12.6s remaining:    5.2s\n",
      "[Parallel(n_jobs=-1)]: Done 139 out of 195 | elapsed:   12.7s remaining:    5.1s\n",
      "[Parallel(n_jobs=-1)]: Done 140 out of 195 | elapsed:   12.8s remaining:    5.0s\n",
      "[Parallel(n_jobs=-1)]: Done 141 out of 195 | elapsed:   12.8s remaining:    4.9s\n",
      "[Parallel(n_jobs=-1)]: Done 142 out of 195 | elapsed:   12.9s remaining:    4.8s\n",
      "[Parallel(n_jobs=-1)]: Done 143 out of 195 | elapsed:   12.9s remaining:    4.7s\n",
      "[Parallel(n_jobs=-1)]: Done 144 out of 195 | elapsed:   13.1s remaining:    4.6s\n",
      "[Parallel(n_jobs=-1)]: Done 145 out of 195 | elapsed:   13.1s remaining:    4.5s\n",
      "[Parallel(n_jobs=-1)]: Done 146 out of 195 | elapsed:   13.2s remaining:    4.4s\n",
      "[Parallel(n_jobs=-1)]: Done 147 out of 195 | elapsed:   13.2s remaining:    4.3s\n",
      "[Parallel(n_jobs=-1)]: Done 148 out of 195 | elapsed:   13.3s remaining:    4.2s\n",
      "[Parallel(n_jobs=-1)]: Done 149 out of 195 | elapsed:   13.4s remaining:    4.1s\n",
      "[Parallel(n_jobs=-1)]: Done 150 out of 195 | elapsed:   13.5s remaining:    4.0s\n",
      "[Parallel(n_jobs=-1)]: Done 151 out of 195 | elapsed:   13.5s remaining:    3.9s\n",
      "[Parallel(n_jobs=-1)]: Done 152 out of 195 | elapsed:   13.6s remaining:    3.8s\n",
      "[Parallel(n_jobs=-1)]: Done 153 out of 195 | elapsed:   13.6s remaining:    3.7s\n",
      "[Parallel(n_jobs=-1)]: Done 154 out of 195 | elapsed:   13.6s remaining:    3.6s\n",
      "[Parallel(n_jobs=-1)]: Done 155 out of 195 | elapsed:   13.7s remaining:    3.5s\n",
      "[Parallel(n_jobs=-1)]: Done 156 out of 195 | elapsed:   13.8s remaining:    3.4s\n",
      "[Parallel(n_jobs=-1)]: Done 157 out of 195 | elapsed:   13.8s remaining:    3.3s\n",
      "[Parallel(n_jobs=-1)]: Done 158 out of 195 | elapsed:   13.8s remaining:    3.2s\n",
      "[Parallel(n_jobs=-1)]: Done 159 out of 195 | elapsed:   13.9s remaining:    3.2s\n",
      "[Parallel(n_jobs=-1)]: Done 160 out of 195 | elapsed:   14.0s remaining:    3.1s\n",
      "[Parallel(n_jobs=-1)]: Done 161 out of 195 | elapsed:   14.1s remaining:    3.0s\n",
      "[Parallel(n_jobs=-1)]: Done 162 out of 195 | elapsed:   14.2s remaining:    2.9s\n",
      "[Parallel(n_jobs=-1)]: Done 163 out of 195 | elapsed:   14.3s remaining:    2.8s\n",
      "[Parallel(n_jobs=-1)]: Done 164 out of 195 | elapsed:   14.3s remaining:    2.7s\n",
      "[Parallel(n_jobs=-1)]: Done 165 out of 195 | elapsed:   14.4s remaining:    2.6s\n",
      "[Parallel(n_jobs=-1)]: Done 166 out of 195 | elapsed:   14.5s remaining:    2.5s\n",
      "[Parallel(n_jobs=-1)]: Done 167 out of 195 | elapsed:   14.5s remaining:    2.4s\n",
      "[Parallel(n_jobs=-1)]: Done 168 out of 195 | elapsed:   14.6s remaining:    2.3s\n",
      "[Parallel(n_jobs=-1)]: Done 169 out of 195 | elapsed:   14.6s remaining:    2.2s\n",
      "[Parallel(n_jobs=-1)]: Done 170 out of 195 | elapsed:   14.7s remaining:    2.2s\n",
      "[Parallel(n_jobs=-1)]: Done 171 out of 195 | elapsed:   14.8s remaining:    2.1s\n",
      "[Parallel(n_jobs=-1)]: Done 172 out of 195 | elapsed:   14.9s remaining:    2.0s\n",
      "[Parallel(n_jobs=-1)]: Done 173 out of 195 | elapsed:   14.9s remaining:    1.9s\n",
      "[Parallel(n_jobs=-1)]: Done 174 out of 195 | elapsed:   15.0s remaining:    1.8s\n",
      "[Parallel(n_jobs=-1)]: Done 175 out of 195 | elapsed:   15.1s remaining:    1.7s\n",
      "[Parallel(n_jobs=-1)]: Done 176 out of 195 | elapsed:   15.2s remaining:    1.6s\n",
      "[Parallel(n_jobs=-1)]: Done 177 out of 195 | elapsed:   15.2s remaining:    1.5s\n",
      "[Parallel(n_jobs=-1)]: Done 178 out of 195 | elapsed:   15.3s remaining:    1.5s\n",
      "[Parallel(n_jobs=-1)]: Done 179 out of 195 | elapsed:   15.3s remaining:    1.4s\n",
      "[Parallel(n_jobs=-1)]: Done 180 out of 195 | elapsed:   15.4s remaining:    1.3s\n",
      "[Parallel(n_jobs=-1)]: Done 181 out of 195 | elapsed:   15.5s remaining:    1.2s\n",
      "[Parallel(n_jobs=-1)]: Done 182 out of 195 | elapsed:   15.5s remaining:    1.1s\n",
      "[Parallel(n_jobs=-1)]: Done 183 out of 195 | elapsed:   15.6s remaining:    1.0s\n",
      "[Parallel(n_jobs=-1)]: Done 184 out of 195 | elapsed:   15.7s remaining:    0.9s\n",
      "[Parallel(n_jobs=-1)]: Done 185 out of 195 | elapsed:   15.7s remaining:    0.8s\n",
      "[Parallel(n_jobs=-1)]: Done 186 out of 195 | elapsed:   15.8s remaining:    0.8s\n",
      "[Parallel(n_jobs=-1)]: Done 187 out of 195 | elapsed:   15.9s remaining:    0.7s\n",
      "[Parallel(n_jobs=-1)]: Done 188 out of 195 | elapsed:   15.9s remaining:    0.6s\n",
      "[Parallel(n_jobs=-1)]: Done 189 out of 195 | elapsed:   16.0s remaining:    0.5s\n",
      "[Parallel(n_jobs=-1)]: Done 190 out of 195 | elapsed:   16.0s remaining:    0.4s\n",
      "[Parallel(n_jobs=-1)]: Done 191 out of 195 | elapsed:   16.1s remaining:    0.3s\n",
      "[Parallel(n_jobs=-1)]: Done 192 out of 195 | elapsed:   16.2s remaining:    0.3s\n",
      "[Parallel(n_jobs=-1)]: Done 193 out of 195 | elapsed:   16.2s remaining:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Done 195 out of 195 | elapsed:   16.4s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done 195 out of 195 | elapsed:   16.4s finished\n",
      "[Pipeline] .... (step 1 of 2) Processing standardscaler, total=   0.0s\n",
      "[Pipeline]  (step 2 of 2) Processing kneighborsclassifier, total=   0.0s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "             estimator=Pipeline(steps=[('standardscaler',\n",
       "                                        StandardScaler(with_mean=False)),\n",
       "                                       ('kneighborsclassifier',\n",
       "                                        KNeighborsClassifier(n_jobs=-1, p=1,\n",
       "                                                             weights='distance'))],\n",
       "                                verbose=True),\n",
       "             n_jobs=-1,\n",
       "             param_grid={'kneighborsclassifier__n_neighbors': range(79, 1, -2)},\n",
       "             pre_dispatch=32768, refit='recall_weighted',\n",
       "             scoring=['recall_weighted'], verbose=200)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t0 = default_timer()\n",
    "\n",
    "# Set the maximum number of neighbors based on number of training samples in each folds to prevent ValueError.\n",
    "# Also, ensure every number of neighbors generated by range is odd.\n",
    "maximum_number_of_neighbors = min(200, (np.int(2 * ((X.shape[0] / number_of_folds) // 2) - 1 ))) # 200 is chosen from anecdotal experience.\n",
    "print('maximum_number_of_neighbors = %d' % (maximum_number_of_neighbors))\n",
    "k_neighbors_pipeline = make_pipeline(\\\n",
    "    StandardScaler(with_mean=False),\\\n",
    "    KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski', metric_params=None, n_jobs=-1, p=1, weights='distance'), verbose=True)\n",
    "\n",
    "# Use only odd numbers of neighbors to avoid ties.\n",
    "# Number of neighbors range starts high and ends low, allowing user to  monitor kernel messages for possible local maxima in score.\n",
    "grid_parameters = {'kneighborsclassifier__n_neighbors': range(maximum_number_of_neighbors, 1, -2)}\n",
    "scoring = ['recall_weighted']\n",
    "grid_search_cv = GridSearchCV(k_neighbors_pipeline, param_grid=grid_parameters,\\\n",
    "                              scoring=scoring, \\\n",
    "                              n_jobs=-1, refit='recall_weighted', cv=number_of_folds, verbose=200, pre_dispatch=2**15, error_score='raise',\\\n",
    "                              return_train_score=False)\n",
    "\n",
    "grid_search_cv.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed grid search in 17.311510 seconds\n"
     ]
    }
   ],
   "source": [
    "print('Completed grid search in %f seconds' % (elapsed_time(t0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best KNeighborsClassifier parameters:\n",
      "memory : None\n",
      "steps : [('standardscaler', StandardScaler(with_mean=False)), ('kneighborsclassifier', KNeighborsClassifier(n_jobs=-1, n_neighbors=11, p=1, weights='distance'))]\n",
      "verbose : True\n",
      "standardscaler : StandardScaler(with_mean=False)\n",
      "kneighborsclassifier : KNeighborsClassifier(n_jobs=-1, n_neighbors=11, p=1, weights='distance')\n",
      "standardscaler__copy : True\n",
      "standardscaler__with_mean : False\n",
      "standardscaler__with_std : True\n",
      "kneighborsclassifier__algorithm : auto\n",
      "kneighborsclassifier__leaf_size : 30\n",
      "kneighborsclassifier__metric : minkowski\n",
      "kneighborsclassifier__metric_params : None\n",
      "kneighborsclassifier__n_jobs : -1\n",
      "kneighborsclassifier__n_neighbors : 11\n",
      "kneighborsclassifier__p : 1\n",
      "kneighborsclassifier__weights : distance\n"
     ]
    }
   ],
   "source": [
    "# Store best KNeighborsClassifier from GridSearchCV and print its parameters.\n",
    "k_neighbors_clf = grid_search_cv.best_estimator_\n",
    "print('Best KNeighborsClassifier parameters:')\n",
    "for key in k_neighbors_clf.get_params().keys():\n",
    "    print(key, ':', k_neighbors_clf.get_params()[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = default_timer()\n",
    "scoring = ['f1_macro', 'f1_weighted', 'neg_log_loss', 'precision_macro', 'precision_weighted', 'recall_macro', 'recall_weighted']\n",
    "k_neighbors_scores = cross_validate(k_neighbors_clf, X, y, scoring=scoring, n_jobs=-1, cv=number_of_folds, return_estimator=True)\n",
    "print(\"k-Neighbors score keys:\", sorted(k_neighbors_scores.keys()))\n",
    "print('k-Neighbors Classifiers constructed in %f seconds.' % elapsed_time(t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_elapsed_time(notebook_start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the Models and Compare their Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_categorical.info()\n",
    "df_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the relative frequencies for the validation DataFrame.\n",
    "print(\"IS_SEVERE relative frequencies:\")\n",
    "print(df_test['IS_SEVERE'].value_counts(normalize=False, dropna=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the relative frequencies for the validation DataFrame.\n",
    "print(\"IS_SEVERE relative frequencies:\")\n",
    "print(df_test['IS_SEVERE'].value_counts(normalize=True, dropna=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the data using the OneHotEncoder fitted before the train/test split.\n",
    "start_time = default_timer()\n",
    "print('Transforming features using OneHotEncoder...')\n",
    "X_test = df_test.drop(columns=['IS_SEVERE', 'SEVERITYCODE', 'INJURIES', 'SERIOUSINJURIES', 'FATALITIES'], inplace=False)\n",
    "X_test = encoder.transform(X_test)\n",
    "print(\"Encoding completed in\", elapsed_time(start_time), \"seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = df_test['IS_SEVERE'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_elapsed_time(notebook_start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set starting time for reports and graphics.\n",
    "t0 = default_timer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Report statistics of the models built for each fold of the cross-validation.\n",
    "print('Logistic Regression Cross-validation Scores:')\n",
    "print()\n",
    "for key in sorted(logistic_regression_scores.keys()):\n",
    "    if key != 'estimator':\n",
    "        print('%s: mean = %f, std = %f' % (key, np.mean(logistic_regression_scores[key]), np.std(logistic_regression_scores[key])), sep='')\n",
    "        print('%s :%s' % (key, logistic_regression_scores[key]), sep='')\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each logistic regression classifier in the cross-validation, print the macro recall, score report, and confusion matrices.\n",
    "number_of_models = len(logistic_regression_scores['estimator'])\n",
    "#labels = ['1', '2', '2b', '3']\n",
    "labels = [True, False]\n",
    "target_names = ['Major', 'Minor']\n",
    "display_labels = target_names\n",
    "for index, logistic_regression_clf in zip(range(len(logistic_regression_scores['estimator'])), logistic_regression_scores['estimator']):\n",
    "    y_test_predicted = logistic_regression_clf.predict(X_test)\n",
    "    print('Logistic Regression Model %d of %d:' % (index + 1, number_of_models))\n",
    "    print()\n",
    "    print(classification_report(y_test, y_test_predicted, labels=labels, target_names=target_names, digits=6))\n",
    "        \n",
    "    # Create a figure.\n",
    "    fig = plt.figure(num=str(index + 1), figsize=(6.4 * 3, 4.8))\n",
    "    fig.suptitle('Confusion Matrices for Logistic Regression Model %d of %d:' % (index + 1, number_of_models), fontsize=20)\n",
    "    \n",
    "    ax = plt.subplot(1, 3, 1)\n",
    "    ax.set_title(\"Normalized over Predicted Severity\", fontsize=12)\n",
    "    plot_confusion_matrix(logistic_regression_clf, X_test, y_test, labels=labels, display_labels=display_labels, normalize='pred', ax=ax)\n",
    "    \n",
    "    ax = plt.subplot(1, 3, 2)\n",
    "    ax.set_title(\"Normalized over True Severity\", fontsize=12)\n",
    "    plot_confusion_matrix(logistic_regression_clf, X_test, y_test, labels=labels, display_labels=display_labels, normalize='true', ax=ax)\n",
    "    \n",
    "    ax = plt.subplot(1, 3, 3)\n",
    "    ax.set_title(\"Not Normalized\", fontsize=12)\n",
    "    plot_confusion_matrix(logistic_regression_clf, X_test, y_test, labels=labels, display_labels=display_labels, normalize=None, ax=ax)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_elapsed_time(notebook_start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Report statistics of the models built for each fold of the cross-validation.\n",
    "print('Support Vector Machine Cross-validation Scores:')\n",
    "print()\n",
    "for key in sorted(linear_svc_scores.keys()):\n",
    "    if key != 'estimator':\n",
    "        print('%s: mean = %f, std = %f' % (key, np.mean(linear_svc_scores[key]), np.std(linear_svc_scores[key])), sep='')\n",
    "        print('%s :%s' % (key, linear_svc_scores[key]), sep='')\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each support vector machine classifier in the cross-validation, print the score report and confusion matrices.\n",
    "number_of_models = len(linear_svc_scores['estimator'])\n",
    "#labels = ['1', '2', '2b', '3']\n",
    "labels = [True, False]\n",
    "target_names = ['Major', 'Minor']\n",
    "display_labels = target_names\n",
    "for index, linear_svc_clf in zip(range(len(linear_svc_scores['estimator'])), linear_svc_scores['estimator']):\n",
    "    y_test_predicted = linear_svc_clf.predict(X_test)\n",
    "    print('Support Vector Machine Model %d of %d:' % (index + 1, number_of_models))\n",
    "    print()\n",
    "    print(classification_report(y_test, y_test_predicted, labels=labels, target_names=target_names, digits=6))\n",
    "        \n",
    "    # Create a figure.\n",
    "    fig = plt.figure(num=str(index + 1), figsize=(6.4 * 3, 4.8))\n",
    "    fig.suptitle('Confusion Matrices for Support Vector Machine Model %d of %d:' % (index + 1, number_of_models), fontsize=20)\n",
    "    \n",
    "    ax = plt.subplot(1,3,1)\n",
    "    ax.set_title(\"Normalized over Predicted Severity\", fontsize=12)\n",
    "    plot_confusion_matrix(linear_svc_clf, X_test, y_test, labels=labels, display_labels=display_labels, normalize='pred', ax=ax)\n",
    "    \n",
    "    ax = plt.subplot(1,3,2)\n",
    "    ax.set_title(\"Normalized over True Severity\", fontsize=12)\n",
    "    plot_confusion_matrix(linear_svc_clf, X_test, y_test, labels=labels, display_labels=display_labels, normalize='true', ax=ax)\n",
    "    \n",
    "    ax = plt.subplot(1,3,3)\n",
    "    ax.set_title(\"Not Normalized\", fontsize=12)\n",
    "    plot_confusion_matrix(linear_svc_clf, X_test, y_test, labels=labels, display_labels=display_labels, normalize=None, ax=ax)\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_elapsed_time(notebook_start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Report statistics of the models built for each fold of the cross-validation.\n",
    "print('k-Nearest Neighbors Cross-validation Scores:')\n",
    "print()\n",
    "for key in sorted(k_neighbors_scores.keys()):\n",
    "    if key != 'estimator':\n",
    "        print('%s: mean = %f, std = %f' % (key, np.mean(k_neighbors_scores[key]), np.std(k_neighbors_scores[key])), sep='')\n",
    "        print('%s :%s' % (key, k_neighbors_scores[key]), sep='')\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each k-Nearest Neighbor classifier in the cross-validation, print the macro recall, score report, and confusion matrices.\n",
    "number_of_models = len(k_neighbors_scores['estimator'])\n",
    "#labels = ['1', '2', '2b', '3']\n",
    "labels = [True, False]\n",
    "target_names = ['Major', 'Minor']\n",
    "display_labels = target_names\n",
    "for index, knn_clf in zip(range(len(k_neighbors_scores['estimator'])), k_neighbors_scores['estimator']):\n",
    "    y_test_predicted = knn_clf.predict(X_test)\n",
    "    print('k-Nearest Neighbor Model %d of %d:' % (index + 1, number_of_models))\n",
    "    print()\n",
    "    print(classification_report(y_test, y_test_predicted, labels=labels, target_names=target_names, digits=6))\n",
    "        \n",
    "    # Create a figure.\n",
    "    fig = plt.figure(num=str(index + 1), figsize=(6.4 * 3, 4.8))\n",
    "    fig.suptitle('Confusion Matrices for k-Nearest Neighbor Model %d of %d:' % (index + 1, number_of_models), fontsize=20)\n",
    "    \n",
    "    ax = plt.subplot(1, 3, 1)\n",
    "    ax.set_title(\"Normalized over Predicted Severity\", fontsize=12)\n",
    "    plot_confusion_matrix(knn_clf, X_test, y_test, labels=labels, display_labels=display_labels, normalize='pred', ax=ax)\n",
    "    \n",
    "    ax = plt.subplot(1, 3, 2)\n",
    "    ax.set_title(\"Normalized over True Severity\", fontsize=12)\n",
    "    plot_confusion_matrix(knn_clf, X_test, y_test, labels=labels, display_labels=display_labels, normalize='true', ax=ax)\n",
    "    \n",
    "    ax = plt.subplot(1, 3, 3)\n",
    "    ax.set_title(\"Not Normalized\", fontsize=12)\n",
    "    plot_confusion_matrix(knn_clf, X_test, y_test, labels=labels, display_labels=display_labels, normalize=None, ax=ax)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('All reports and graphics generated in %f seconds.' % (elapsed_time(t0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_elapsed_time(notebook_start_time)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
