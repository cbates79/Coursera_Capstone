{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 id=\"title\">Code for Predicting Motor Vehicle Accident Severity in Seattle, Washington</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains the Python code for the Applied Data Science Capstone Project.\n",
    "In this project, we use a publicly available data set to build several kinds of models to predict the severity of motor vehicle accidents in Seattle, Washington."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages and modules.\n",
    "import io\n",
    "import itertools\n",
    "import matplotlib as mpl\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import pylab as pl\n",
    "import scipy\n",
    "import scipy.optimize as opt\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "import sys\n",
    "import timeit\n",
    "import warnings\n",
    "from matplotlib.ticker import NullFormatter\n",
    "from scipy import optimize\n",
    "from scipy.optimize import curve_fit\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn import linear_model\n",
    "from sklearn import metrics\n",
    "from sklearn import pipeline\n",
    "from sklearn import preprocessing\n",
    "from sklearn import svm\n",
    "from sklearn import tree\n",
    "from sklearn import utils\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import jaccard_score\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.tree import export_text\n",
    "from sklearn.tree import plot_tree\n",
    "from sklearn.utils import resample\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "from timeit import default_timer\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning, module=\"sklearn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the starting time for this notebook. \n",
    "notebook_start_time = default_timer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function computes the time elapsed in seconds from the \n",
    "# time represented by the first parameter (start_time)\n",
    "# to the time represented by the second parameter (end_time)\n",
    "# This function requires the os package to be imported.\n",
    "def elapsed_time(start_time = notebook_start_time):\n",
    "    return default_timer() - start_time   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function prints the time elapsed in seconds from the \n",
    "# time represented by the first parameter (start_time)\n",
    "# to the time represented by the second parameter (end_time)\n",
    "# This function requires the os package to be imported.\n",
    "def print_elapsed_time(start_time = notebook_start_time):\n",
    "    print(\"Elapsed time is\", elapsed_time(start_time), \"seconds.\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of display options.\n",
    "list_of_display_options_fully_qualified_names = str(\\\n",
    "\"pd.options.display.chop_threshold, pd.options.display.float_format, pd.options.display.max_info_columns, pd.options.display.notebook_repr_html, \\\n",
    "pd.options.display.colheader_justify, pd.options.display.html, pd.options.display.max_info_rows, pd.options.display.pprint_nest_depth, \\\n",
    "pd.options.display.column_space, pd.options.display.large_repr, pd.options.display.max_rows, pd.options.display.precision, \\\n",
    "pd.options.display.date_dayfirst, pd.options.display.latex, pd.options.display.max_seq_items, pd.options.display.show_dimensions, \\\n",
    "pd.options.display.date_yearfirst, pd.options.display.max_categories, pd.options.display.memory_usage, pd.options.display.unicode, \\\n",
    "pd.options.display.encoding, pd.options.display.max_columns, pd.options.display.min_rows, pd.options.display.width, \\\n",
    "pd.options.display.expand_frame_repr, pd.options.display.max_colwidth, pd.options.display.multi_sparse\").split(sep=', ')\n",
    "\n",
    "# Initialize an empty list to store all the short names for display options.\n",
    "list_of_display_options_short_names = list()\n",
    "# For each fully qualified option name,\n",
    "# get the option's short name and add it to the list of short names.\n",
    "for fully_qualified_option_name in list_of_display_options_fully_qualified_names:\n",
    "    # Get short option name.\n",
    "    short_option_name = fully_qualified_option_name.split(sep='.')[-1]\n",
    "    \n",
    "    # Add short option name to list of display option short names.\n",
    "    list_of_display_options_short_names.append(short_option_name)\n",
    "\n",
    "# Define dictionary of display option settings.\n",
    "dict_of_display_option_settings_short_names=\\\n",
    "{\"max_info_columns\": 1000,\\\n",
    "\"colheader_justify\": \"right\",\\\n",
    "\"max_info_rows\": 1000000,\\\n",
    "\"column_space\": 1000,\\\n",
    "\"max_rows\": 1000000,\\\n",
    "\"precision\": 9,\\\n",
    "\"max_seq_items\": 1000000000000,\\\n",
    "\"show_dimensions\": True,\\\n",
    "\"max_categories\": 100,\\\n",
    "\"memory_usage\": True,\\\n",
    "\"max_columns\": 1000,\\\n",
    "\"max_colwidth\": 1000,\\\n",
    "\"float_format\": lambda x: '%.9f' % x}\n",
    "\n",
    "# Set pandas display options using dictionary of short names,\n",
    "# and display the options/value pairs.\n",
    "print(\"Setting display options...\")\n",
    "for key in list(dict_of_display_option_settings_short_names.keys()):\n",
    "    # Set display option.\n",
    "    pd.set_option(key, dict_of_display_option_settings_short_names[key])\n",
    "    # Print display option name and value.\n",
    "    print(key, \": \", pd.get_option(key), sep='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed for random number generator.\n",
    "# seed = np.int(os.times()[4]) # Use this line for better pseudo-random behavior.\n",
    "seed = 42\n",
    "print(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attribute Information URL: https://www.seattle.gov/Documents/Departments/SDOT/GIS/Collisions_OD.pdf\n",
    "# Read the Collisions Data CSV file and store it as a DataFrame.\n",
    "# url=\"https://opendata.arcgis.com/datasets/5b5c745e0f1f48e7a53acec63a0022ab_0.csv\" # HTTPError at 202009151050, using local copy of .csv instead.\n",
    "# print(os.listdir(\"..\")) # Print list of contents of current working directory.\n",
    "local_path_to_csv = '~/IBM Data Science Professional Certificate Course/Course 9 - Applied Data Science Capstone/projects/Collisions.csv'\n",
    "df=pd.read_csv(local_path_to_csv, low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the first few rows of the collisions DataFrame.\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_elapsed_time(notebook_start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"data_wrangling\">Data Wrangling</h2>\n",
    "\n",
    "Steps for working with missing data:\n",
    "<ol>\n",
    "    <li>Identify missing data.</li>\n",
    "    <li>Deal with missing data.</li>\n",
    "    <li>Correct data format.</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 id=\"identifying_missing_data\">Identifying Missing Data</h3>\n",
    "\n",
    "The metadata document that accompanied the data set indicates that certain columns have \"sentinel\" values\n",
    "that indicate an unknown or missing value. Each of these missing values will first be converted into NaN.\n",
    "Subsequently, the NaN values will be dropped from the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If any row of the collisions DataFrame contains a sentinel value representing \"unknown\" or \"other\",\n",
    "# then replace it with NaN. \n",
    "# Sentinels for \"unknown\" are listed in the metadata document that accompanies the dataset.\n",
    "df_unknowns_converted_to_nan = df.replace(to_replace=\\\n",
    "    {\"EXCEPTRSNCODE\": \" \",\\\n",
    "     \"EXCEPTRSNDESC\": \"Not Enough Information, or Insufficient Location Information\",\\\n",
    "     \"COLLISIONTYPE\": \"Other\",\\\n",
    "     \"SEVERITYCODE\": \"0\",\\\n",
    "     \"SEVERITYDESC\": \"Unknown\",\\\n",
    "     \"JUNCTIONTYPE\": \"Unknown\",\\\n",
    "     \"WEATHER\": \"Unknown\",\\\n",
    "     \"ROADCOND\": \"Unknown\",\\\n",
    "     \"LIGHTCOND\": \"Unknown\",\\\n",
    "     \"SDOT_COLCODE\": float(0),\\\n",
    "     \"SDOT_COLDESC\": \"NOT ENOUGH INFORMATION / NOT APPLICABLE\",\\\n",
    "     \"ST_COLCODE\": \" \",\\\n",
    "     \"ST_COLDESC\": \"Not stated\"}, value=np.nan, inplace=False, limit=None, regex=False, method='pad')\n",
    "\n",
    "df_unknowns_converted_to_nan.replace(to_replace=\\\n",
    "    {\"ST_COLCODE\": \"0\",\\\n",
    "     \"WEATHER\": \"Other\",\\\n",
    "     \"ROADCOND\": \"Other\",\\\n",
    "     \"LIGHTCOND\": \"Other\"}, value=np.nan, inplace=True, limit=None, regex=False, method='pad')\n",
    "\n",
    "df_unknowns_converted_to_nan.replace(to_replace=\\\n",
    "    {\"LIGHTCOND\": \"Dark - Unknown Lighting\"}, value=np.nan, inplace=True, limit=None, regex=False, method='pad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_elapsed_time(notebook_start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 id=\"deal_with_missing_data\">Deal with Missing Data</h3>\n",
    "\n",
    "<ol>\n",
    "    <li>Drop the Data\n",
    "        <ol>\n",
    "            <li>Drop entire row.</li>\n",
    "            <li>Drop entire column.</li>\n",
    "        </ol>\n",
    "    </li>\n",
    "    <li>Replace the Data\n",
    "        <ol>\n",
    "            <li>Replace data by mean.</li>\n",
    "            <li>Replace data by frequency.</li>\n",
    "            <li>Replace data based on other functions.</li>\n",
    "        </ol>\n",
    "    </li>\n",
    "        \n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whole columns should be dropped only if most entries in the column are empty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(df.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop any column from the collisions DataFrame if it satisfies at least one of the following conditions:\n",
    "# 1) more than 15% of the column's data is NaN;\n",
    "# 2) the column only contains unique identification keys, or information not useful for model building;\n",
    "# 3) the column's data is categorical but does not fit into a small (< 15) number of categories;\n",
    "# 4) information in the column is redundant because it is already represented by another column;\n",
    "# 5) it is not clear how to interpret the column's data.\n",
    "list_of_columns_to_drop = [\"ADDRTYPE\",\\\n",
    "                           \"STATUS\",\\\n",
    "                           \"OBJECTID\",\\\n",
    "                           \"INCKEY\",\\\n",
    "                           \"COLDETKEY\",\\\n",
    "                           \"REPORTNO\",\\\n",
    "                           \"INTKEY\",\\\n",
    "                           \"LOCATION\",\\\n",
    "                           \"EXCEPTRSNCODE\",\\\n",
    "                           \"EXCEPTRSNDESC\",\\\n",
    "                           \"SEVERITYDESC\",\\\n",
    "                           \"PERSONCOUNT\",\\\n",
    "                           \"VEHCOUNT\",\\\n",
    "                           \"INJURIES\",\\\n",
    "                           \"SERIOUSINJURIES\",\\\n",
    "                           \"FATALITIES\",\\\n",
    "                           \"INCDATE\",\\\n",
    "                           \"INCDTTM\",\\\n",
    "                           \"JUNCTIONTYPE\",\\\n",
    "                           \"SDOT_COLCODE\",\\\n",
    "                           \"SDOT_COLDESC\",\\\n",
    "                           \"INATTENTIONIND\",\\\n",
    "                           \"UNDERINFL\",\\\n",
    "                           \"PEDROWNOTGRNT\",\\\n",
    "                           \"SDOTCOLNUM\",\\\n",
    "                           \"SPEEDING\",\\\n",
    "                           \"ST_COLCODE\",\\\n",
    "                           \"ST_COLDESC\",\\\n",
    "                           \"SEGLANEKEY\",\\\n",
    "                           \"CROSSWALKKEY\",\\\n",
    "                           \"HITPARKEDCAR\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the selected columns from the DataFrame after converting unknowns to NaN.\n",
    "# Store the result in a new DataFrame.\n",
    "df_drop_columns = df_unknowns_converted_to_nan.drop(columns=list_of_columns_to_drop, inplace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop any row that contains at least one NaN.\n",
    "df_drop_columns_and_rows = df_drop_columns.dropna(axis=\"index\", how=\"any\", thresh=None, subset=None, inplace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minor_severity_labels = ['1', '2']\n",
    "major_severity_labels = ['2b', '3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "series_is_severe = df_drop_columns_and_rows['SEVERITYCODE'].isin(major_severity_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "series_is_severe.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a new column called 'IS_SEVERE'.\n",
    "# For each row of the DataFrame, if 'SEVERITYCODE' is '2b' or '3', then 'IS_SEVERE' gets the boolean value of True.\n",
    "# If 'SEVERITYCODE' is '1' or '2', then 'IS_SEVERE' gets the boolean value of False.\n",
    "df_drop_columns_and_rows.insert(0, 'IS_SEVERE', series_is_severe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop column SEVERITYCODE from DataFrame in place, because severity is now represented by column IS_SEVERE.\n",
    "df_drop_columns_and_rows = df_drop_columns_and_rows.drop(columns=['SEVERITYCODE'], inplace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_drop_columns_and_rows.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_drop_columns_and_rows.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_elapsed_time(notebook_start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 id=\"correct_data_format\">Correct Data Format</h3>\n",
    "\n",
    "Ensure that each data type is appropriate for the corresponding feature.\n",
    "Cast columns of type \"object\" as type \"category\", but leave all other column types unaltered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new DataFrame to store converted data types.\n",
    "df_converted = pd.DataFrame()\n",
    "\n",
    "for column in list(df_drop_columns_and_rows.columns):\n",
    "    if (df_drop_columns_and_rows[column].dtype in [np.dtype('object')]):\n",
    "        df_converted[column] = df_drop_columns_and_rows[column].astype('category')\n",
    "    # Copy all other columns to new DataFrame without changing their types.\n",
    "    else:\n",
    "        df_converted[column] = df_drop_columns_and_rows[column]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame of categorical or integer columns, inclusive.\n",
    "df_categorical = df_converted.select_dtypes(include=['bool', 'category', 'integer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_elapsed_time(notebook_start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"feature_selection\">Feature selection</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Features before One-Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_categorical.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_categorical.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Severity vs. Collision Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('IS_SEVERE Relative Frequencies:\\n')\n",
    "df_categorical['IS_SEVERE'].value_counts(normalize=True, dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('IS_SEVERE Value Counts:\\n')\n",
    "df_categorical['IS_SEVERE'].value_counts(normalize=False, dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_categorical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each class of COLLISIONTYPE, get the relative frequencies for IS_SEVERE, i.e.\n",
    "# for each COLLISIONTYPE group, compute the number of rows with IS_SEVERE=True divided by the size of the this COLLISIONTYPE group.\n",
    "\n",
    "# Create a GroupBy object on COLLISIONTYPE.\n",
    "groupby_collisiontype = df_categorical[['COLLISIONTYPE', 'IS_SEVERE']].groupby(by=['COLLISIONTYPE'])\n",
    "\n",
    "# Create a GroupBy object on COLLISIONTYPE, IS_SEVERE.\n",
    "groupby_collisiontype_is_severe = df_categorical[['COLLISIONTYPE', 'IS_SEVERE']].groupby(by=['COLLISIONTYPE', 'IS_SEVERE'])\n",
    "\n",
    "print('IS_SEVERE relative frequencies:')\n",
    "print(df_categorical['IS_SEVERE'].value_counts(normalize=True, dropna=False))\n",
    "print()\n",
    "\n",
    "df_value_counts_by_collisiontype = pd.DataFrame(data=groupby_collisiontype_is_severe.size(), columns=['Value Counts'])\n",
    "print('IS_SEVERE value counts over each COLLISIONTYPE group:\\n', df_value_counts_by_collisiontype)\n",
    "print()\n",
    "\n",
    "severity_frequency_cutoff = df_categorical['IS_SEVERE'].value_counts(normalize=True, dropna=False).min()\n",
    "print('Proportion of all data with IS_SEVERE=True: %f' % (severity_frequency_cutoff))\n",
    "print()\n",
    "\n",
    "df_frequencies_by_collisiontype = pd.DataFrame(data=groupby_collisiontype_is_severe.size() / groupby_collisiontype.size(),\\\n",
    "                                               columns=['Relative Frequencies'])\n",
    "print('IS_SEVERE relative frequencies normalized over each COLLISIONTYPE group:\\n', df_frequencies_by_collisiontype)\n",
    "print()\n",
    "\n",
    "print('IS_SEVERE relative frequencies normalized over each COLLISIONTYPE group,\\n',\\\n",
    "      'given the proportion of IS_SEVERE=True > %f:\\n' % (severity_frequency_cutoff),\\\n",
    "      df_frequencies_by_collisiontype[df_frequencies_by_collisiontype.xs(True, level=1, axis=0) > severity_frequency_cutoff].dropna(), sep='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = df_frequencies_by_collisiontype.plot.bar(alpha=.5, title='Frequency normalized by COLLISIONTYPE vs. (COLLISIONTYPE, IS_SEVERE)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = df_frequencies_by_collisiontype[\n",
    "    df_frequencies_by_collisiontype.xs(True, level=1, axis=0) > severity_frequency_cutoff]\\\n",
    "    .dropna().plot.bar(alpha=.5,\\\n",
    "    title='Frequency normalized by COLLISIONTYPE vs. (COLLISIONTYPE, IS_SEVERE),\\nCondition: Frequency of IS_SEVERE=True > %f' %\\\n",
    "    (severity_frequency_cutoff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = df_frequencies_by_collisiontype[\\\n",
    "    df_frequencies_by_collisiontype.xs(True, level=1, axis=0) > severity_frequency_cutoff]\\\n",
    "    .dropna().xs(True, level=1, axis=0).plot.bar(alpha=.5,\\\n",
    "    title='Frequency normalized by COLLISIONTYPE vs. COLLISIONTYPE\\nConditions: IS_SEVERE=True and frequency of IS_SEVERE=True > %f' %\\\n",
    "    (severity_frequency_cutoff))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Severity vs. Weather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('IS_SEVERE Relative Frequencies:\\n')\n",
    "df_categorical['IS_SEVERE'].value_counts(normalize=True, dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('IS_SEVERE Value Counts:\\n')\n",
    "df_categorical['IS_SEVERE'].value_counts(normalize=False, dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_categorical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each class of WEATHER, get the relative frequencies for IS_SEVERE, i.e.\n",
    "# for each weather group, compute the number of rows with IS_SEVERE=True divided by the size of the this weather group.\n",
    "\n",
    "# Create a GroupBy object on WEATHER.\n",
    "groupby_weather = df_categorical[['WEATHER', 'IS_SEVERE']].groupby(by=['WEATHER'])\n",
    "\n",
    "# Create a GroupBy object on WEATHER, IS_SEVERE.\n",
    "groupby_weather_is_severe = df_categorical[['WEATHER', 'IS_SEVERE']].groupby(by=['WEATHER', 'IS_SEVERE'])\n",
    "\n",
    "# Create a multiindexed DataFrame.\n",
    "#df_grouped_by_weather_severity = pd.DataFrame(df_categorical[['WEATHER', 'IS_SEVERE']].groupby(by=['WEATHER', 'IS_SEVERE']).count())\n",
    "#print(df_grouped_by_weather_severity.head())\n",
    "#print()\n",
    "\n",
    "print('IS_SEVERE relative frequencies:')\n",
    "print(df_categorical['IS_SEVERE'].value_counts(normalize=True, dropna=False))\n",
    "print()\n",
    "\n",
    "df_value_counts_by_weather = pd.DataFrame(data=groupby_weather_is_severe.size(), columns=['Value Counts'])\n",
    "print('IS_SEVERE value counts for each WEATHER class:\\n', df_value_counts_by_weather)\n",
    "print()\n",
    "\n",
    "severity_frequency_cutoff = df_categorical['IS_SEVERE'].value_counts(normalize=True, dropna=False).min()\n",
    "print('Proportion of all data with IS_SEVERE=True: %f' % (severity_frequency_cutoff))\n",
    "print()\n",
    "\n",
    "df_frequencies_by_weather = pd.DataFrame(data=groupby_weather_is_severe.size() / groupby_weather.size(), columns=['Relative Frequencies'])\n",
    "print('IS_SEVERE relative frequencies normalized by WEATHER class:\\n', df_frequencies_by_weather)\n",
    "print()\n",
    "\n",
    "print('IS_SEVERE relative frequencies normalized by WEATHER class,\\n',\\\n",
    "      'given the proportion of IS_SEVERE=True > %f:\\n' % (severity_frequency_cutoff),\\\n",
    "      df_frequencies_by_weather[df_frequencies_by_weather.xs(True, level=1, axis=0) > severity_frequency_cutoff].dropna(), sep='')\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = df_frequencies_by_weather.plot.bar(alpha=.5, title='Frequency normalized by WEATHER vs. (WEATHER, IS_SEVERE)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = df_frequencies_by_weather[df_frequencies_by_weather.xs(True, level=1, axis=0) > severity_frequency_cutoff].dropna().plot.bar(alpha=.5,\\\n",
    "    title='Frequency normalized by WEATHER vs. (WEATHER, IS_SEVERE),\\nCondition: Frequency of IS_SEVERE=True > %f' %\n",
    "    (severity_frequency_cutoff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = df_frequencies_by_weather[\\\n",
    "    df_frequencies_by_weather.xs(True, level=1, axis=0) > severity_frequency_cutoff]\\\n",
    "    .dropna().xs(True, level=1, axis=0).plot.bar(alpha=.5,\\\n",
    "    title='Frequency normalized by WEATHER vs. WEATHER\\nConditions: IS_SEVERE=True and Frequency of IS_SEVERE=True > %f' %\\\n",
    "    (severity_frequency_cutoff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Severity vs. Road Conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('IS_SEVERE Relative Frequencies:\\n')\n",
    "df_categorical['IS_SEVERE'].value_counts(normalize=True, dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('IS_SEVERE Value Counts:\\n')\n",
    "df_categorical['IS_SEVERE'].value_counts(normalize=False, dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_categorical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each class of ROADCOND, get the relative frequencies for IS_SEVERE, i.e.\n",
    "# for each ROADCOND group, compute the number of rows with IS_SEVERE=True divided by the size of the this ROADCOND group.\n",
    "\n",
    "# Create a GroupBy object on ROADCOND.\n",
    "groupby_roadcond = df_categorical[['ROADCOND', 'IS_SEVERE']].groupby(by=['ROADCOND'])\n",
    "\n",
    "# Create a GroupBy object on ROADCOND, IS_SEVERE.\n",
    "groupby_roadcond_is_severe = df_categorical[['ROADCOND', 'IS_SEVERE']].groupby(by=['ROADCOND', 'IS_SEVERE'])\n",
    "\n",
    "print('IS_SEVERE relative frequencies:')\n",
    "print(df_categorical['IS_SEVERE'].value_counts(normalize=True, dropna=False))\n",
    "print()\n",
    "\n",
    "df_value_counts_by_roadcond = pd.DataFrame(data=groupby_roadcond_is_severe.size(), columns=['Value Counts'])\n",
    "print('IS_SEVERE value counts over each ROADCOND group:\\n', df_value_counts_by_roadcond)\n",
    "print()\n",
    "\n",
    "severity_frequency_cutoff = df_categorical['IS_SEVERE'].value_counts(normalize=True, dropna=False).min()\n",
    "print('Proportion of all data with IS_SEVERE=True: %f' % (severity_frequency_cutoff))\n",
    "print()\n",
    "\n",
    "df_frequencies_by_roadcond = pd.DataFrame(data=groupby_roadcond_is_severe.size() / groupby_roadcond.size(), columns=['Relative Frequencies'])\n",
    "print('IS_SEVERE relative frequencies normalized over each ROADCOND group:\\n', df_frequencies_by_roadcond)\n",
    "print()\n",
    "\n",
    "print('IS_SEVERE relative frequencies normalized over each ROADCOND group,\\n',\\\n",
    "      'given the proportion of IS_SEVERE=True > %f:\\n' % (severity_frequency_cutoff),\\\n",
    "      df_frequencies_by_roadcond[df_frequencies_by_roadcond.xs(True, level=1, axis=0) > severity_frequency_cutoff].dropna(), sep='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = df_frequencies_by_roadcond.plot.bar(alpha=.5, title='Frequency normalized by ROADCOND vs. (ROADCOND, IS_SEVERE)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = df_frequencies_by_roadcond[df_frequencies_by_roadcond.xs(True, level=1, axis=0) > severity_frequency_cutoff].dropna().plot.bar(alpha=.5,\\\n",
    "    title='Frequency normalized by ROADCOND vs. (ROADCOND, IS_SEVERE),\\nCondition: Frequency of IS_SEVERE=True > %f' %\n",
    "    (severity_frequency_cutoff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = df_frequencies_by_roadcond[\\\n",
    "    df_frequencies_by_roadcond.xs(True, level=1, axis=0) > severity_frequency_cutoff]\\\n",
    "    .dropna().xs(True, level=1, axis=0).plot.bar(alpha=.5,\\\n",
    "    title='Frequency normalized by ROADCOND vs. ROADCOND\\nConditions: IS_SEVERE=True and frequency of IS_SEVERE=True > %f' %\\\n",
    "    (severity_frequency_cutoff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Severity vs. Light Conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('IS_SEVERE Relative Frequencies:\\n')\n",
    "df_categorical['IS_SEVERE'].value_counts(normalize=True, dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('IS_SEVERE Value Counts:\\n')\n",
    "df_categorical['IS_SEVERE'].value_counts(normalize=False, dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_categorical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each class of LIGHTCOND, get the relative frequencies for IS_SEVERE, i.e.\n",
    "# for each LIGHTCOND group, compute the number of rows with IS_SEVERE=True divided by the size of the this LIGHTCOND group.\n",
    "\n",
    "# Create a GroupBy object on LIGHTCOND.\n",
    "groupby_lightcond = df_categorical[['LIGHTCOND', 'IS_SEVERE']].groupby(by=['LIGHTCOND'])\n",
    "\n",
    "# Create a GroupBy object on LIGHTCOND, IS_SEVERE.\n",
    "groupby_lightcond_is_severe = df_categorical[['LIGHTCOND', 'IS_SEVERE']].groupby(by=['LIGHTCOND', 'IS_SEVERE'])\n",
    "\n",
    "print('IS_SEVERE relative frequencies:')\n",
    "print(df_categorical['IS_SEVERE'].value_counts(normalize=True, dropna=False))\n",
    "print()\n",
    "\n",
    "df_value_counts_by_lightcond = pd.DataFrame(data=groupby_lightcond_is_severe.size(), columns=['Value Counts'])\n",
    "print('IS_SEVERE value counts over each LIGHTCOND group:\\n', df_value_counts_by_lightcond)\n",
    "print()\n",
    "\n",
    "severity_frequency_cutoff = df_categorical['IS_SEVERE'].value_counts(normalize=True, dropna=False).min()\n",
    "print('Proportion of all data with IS_SEVERE=True: %f' % (severity_frequency_cutoff))\n",
    "print()\n",
    "\n",
    "df_frequencies_by_lightcond = pd.DataFrame(data=groupby_lightcond_is_severe.size() / groupby_lightcond.size(), columns=['Relative Frequencies'])\n",
    "print('IS_SEVERE relative frequencies normalized over each LIGHTCOND group:\\n', df_frequencies_by_lightcond)\n",
    "print()\n",
    "\n",
    "print('IS_SEVERE relative frequencies normalized over each LIGHTCOND group,\\n',\\\n",
    "      'given the proportion of IS_SEVERE=True > %f:\\n' % (severity_frequency_cutoff),\\\n",
    "      df_frequencies_by_lightcond[df_frequencies_by_lightcond.xs(True, level=1, axis=0) > severity_frequency_cutoff].dropna(), sep='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = df_frequencies_by_lightcond.plot.bar(alpha=.5, title='Frequency normalized by LIGHTCOND vs. (LIGHTCOND, IS_SEVERE)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = df_frequencies_by_lightcond[df_frequencies_by_lightcond.xs(True, level=1, axis=0) > severity_frequency_cutoff].dropna().plot.bar(alpha=.5,\\\n",
    "    title='Frequency normalized by LIGHTCOND vs. (LIGHTCOND, IS_SEVERE),\\nCondition: Frequency of IS_SEVERE=True > %f' %\n",
    "    (severity_frequency_cutoff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = df_frequencies_by_lightcond[\\\n",
    "    df_frequencies_by_lightcond.xs(True, level=1, axis=0) > severity_frequency_cutoff]\\\n",
    "    .dropna().xs(True, level=1, axis=0).plot.bar(alpha=.5,\\\n",
    "    title='Frequency normalized by LIGHTCOND vs. LIGHTCOND\\nConditions: IS_SEVERE=True and frequency of IS_SEVERE=True > %f' %\\\n",
    "    (severity_frequency_cutoff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_elapsed_time(notebook_start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"imbalanced_data\">Dealing with Imbalanced Data</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the data is imbalanced, we split the DataFrame into two DataFrames, one for each value of the IS_SEVERE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('IS_SEVERE relative frequencies:')\n",
    "print(df_categorical['IS_SEVERE'].value_counts(normalize=True, dropna=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"IS_SEVERE value counts:\")\n",
    "print(df_categorical['IS_SEVERE'].value_counts(normalize=False, dropna=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_class_major_severity = df_categorical[df_categorical['IS_SEVERE']]\n",
    "df_class_minor_severity = df_categorical[~df_categorical['IS_SEVERE']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(df_class_major_severity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_class_major_severity.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(df_class_minor_severity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_class_minor_severity.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"IS_SEVERE relative frequencies:\")\n",
    "print(df_class_major_severity[\"IS_SEVERE\"].value_counts(normalize=False, dropna=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"IS_SEVERE relative frequencies:\")\n",
    "print(df_class_minor_severity[\"IS_SEVERE\"].value_counts(normalize=False, dropna=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store and print the size of the all classes.\n",
    "class_major_severity_size = len(df_class_major_severity)\n",
    "class_minor_severity_size = len(df_class_minor_severity)\n",
    "print('class_major_severity_size =', class_major_severity_size)\n",
    "print('class_minor_severity_size =', class_minor_severity_size)\n",
    "print()\n",
    "# Store and print the size of the minority class.\n",
    "minority_class_size = len(df_class_major_severity)\n",
    "print('minority_class_size =', minority_class_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_elapsed_time(notebook_start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id='sample_size_class_3'>Sample the Accident Severity Classes Equally to Create a Balanced Training Set<\\h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of folds for cross-validation\n",
    "number_of_folds = 2\n",
    "print('number_of_folds = %d' % (number_of_folds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This parameter determines what fraction of the data is reserved for testing.\n",
    "# It must be between 0 and 1, exclusive, i.e. 0 < test_size_ratio < 1.\n",
    "test_size_ratio = .5\n",
    "print('test_size_ratio = %f' % (test_size_ratio))\n",
    "\n",
    "# Using train/test splits, set aside part of the data for testing.\n",
    "df_class_minor_severity_train_pre_sampling, df_class_minor_severity_test =\\\n",
    "    train_test_split(df_class_minor_severity, test_size=test_size_ratio, random_state=seed)\n",
    "\n",
    "df_class_major_severity_train_pre_sampling, df_class_major_severity_test =\\\n",
    "    train_test_split(df_class_major_severity, test_size=test_size_ratio, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a balanced training set by sampling the imbalanced training data equally by class:\n",
    "\n",
    "sampling_ratio = 1\n",
    "print('sampling_ratio = %f' % (sampling_ratio))\n",
    "\n",
    "# Set the boolean <replace> parameter of the sample method based on the sampling ratio.\n",
    "# If the sampling_ratio > 1, sample with replacement.\n",
    "# Otherwise, sample without replacement.\n",
    "sample_with_replacement = bool(sampling_ratio > 1)\n",
    "\n",
    "# Print the value of the replace parameter before passing it to the sample method.\n",
    "print('Sample class major severity with replacement: %s' % (sample_with_replacement))\n",
    "\n",
    "# Sample the minority class's training set based on the sampling parameter and store the sample for later concatenation.\n",
    "df_class_major_severity_train = df_class_major_severity_train_pre_sampling.sample(frac=sampling_ratio,\\\n",
    "                                                                                  replace=sample_with_replacement,\\\n",
    "                                                                                  axis='index',\\\n",
    "                                                                                  random_state=seed)\n",
    "\n",
    "# Store the size of the minority class's training set.\n",
    "df_class_major_severity_train_size = df_class_major_severity_train.shape[0]\n",
    "\n",
    "# Take a sample of the other class's pre-sampling training data,\n",
    "# where the sample size taken is equal to the size of the minority class's training set.\n",
    "# If the sample size to be taken exceeds the number of samples in available, sample with replacement.\n",
    "\n",
    "# Sampling for class minor_severity\n",
    "sample_with_replacement = bool(df_class_major_severity_train_size > df_class_minor_severity_train_pre_sampling.shape[0])\n",
    "# Print the value of the replace parameter before passing it to the sample method.\n",
    "print('Sample class minor severity with replacement: %s' % (sample_with_replacement))\n",
    "df_class_minor_severity_train = df_class_minor_severity_train_pre_sampling.sample(n=df_class_major_severity_train_size,\\\n",
    "                                                                                   replace=sample_with_replacement, axis='index',\\\n",
    "                                                                                   random_state=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Generate a Balanced Training Set and an Unbalanced Test Set</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a balanced, unshuffled training set by concatenating the equal sized samples of the training sets for each class.\n",
    "df_train_balanced_not_shuffled = pd.concat([df_class_minor_severity_train, df_class_major_severity_train], axis='index')\n",
    "\n",
    "# Make a not necessarily balanced testing set by concatenating the testing sets for each class.\n",
    "df_test_not_shuffled = pd.concat([df_class_minor_severity_test, df_class_major_severity_test], axis='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle the training set and store it for tuning hyper-parameters and for cross-validation.\n",
    "df_train = shuffle(df_train_balanced_not_shuffled, random_state=seed)\n",
    "\n",
    "# Shuffle the unbalanced DataFrame and store it for validation and for comparing the models.\n",
    "df_test = shuffle(df_test_not_shuffled, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the training DataFrame is balanced.\n",
    "print(\"IS_SEVERE relative frequencies:\")\n",
    "print(df_train[\"IS_SEVERE\"].value_counts(normalize=False, dropna=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the new DataFrame is balanced.\n",
    "print(\"IS_SEVERE relative frequencies:\")\n",
    "print(df_train[\"IS_SEVERE\"].value_counts(normalize=True, dropna=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define a feature set for the training data represented by a DataFrame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a data frame to store the features for the training data.\n",
    "df_features = df_train.drop(columns=['IS_SEVERE'], inplace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display information about the features DataFrame.\n",
    "df_features.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The boolean array y represents the target variable IS_SEVERE for the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the training data target into a numpy array.\n",
    "y = df_train['IS_SEVERE'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_elapsed_time(notebook_start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform the Data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We normalize the data by transforming it so that it is compatible\n",
    "with the machine learning estimators we use in this notebook.\n",
    "The features are stored in sparse matrix format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a OneHotEncoder and fit it to the features.\n",
    "# The fit is performed on the data set before the any test/train splits.\n",
    "# The data will be encoded as a sparse matrix, the default behavior.\n",
    "start_time = default_timer()\n",
    "print(\"Fitting OneHotEncoder to training data...\")\n",
    "encoder = OneHotEncoder(sparse=True, handle_unknown='error')\n",
    "encoder.fit(df_categorical.drop(columns=['IS_SEVERE'], inplace=False))\n",
    "X = encoder.transform(df_features)\n",
    "print(\"Completed in\", elapsed_time(start_time), \"seconds.\")\n",
    "# Display the categories of the encoder.\n",
    "print(encoder.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sparse matrix X represents the one-hot encoded feature set for the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the type for the training data feature set.\n",
    "type(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the shape of the training data feature set.\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the type for the training data target array.\n",
    "type(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the shape of the training data target array.\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_elapsed_time(notebook_start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_categorical.info()\n",
    "df_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the relative frequencies for the validation DataFrame.\n",
    "print(\"IS_SEVERE relative frequencies:\")\n",
    "print(df_test['IS_SEVERE'].value_counts(normalize=False, dropna=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the relative frequencies for the validation DataFrame.\n",
    "print(\"IS_SEVERE relative frequencies:\")\n",
    "print(df_test['IS_SEVERE'].value_counts(normalize=True, dropna=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the test data features using the same instance of the OneHotEncoder that was fitted on the training data features.\n",
    "start_time = default_timer()\n",
    "print('Transforming features using OneHotEncoder...')\n",
    "X_test = df_test.drop(columns=['IS_SEVERE'], inplace=False)\n",
    "X_test = encoder.transform(X_test)\n",
    "print(\"Encoding completed in\", elapsed_time(start_time), \"seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = df_test['IS_SEVERE'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_elapsed_time(notebook_start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"logistic_regression\">Build and Test a Logistic Regression Model<\\h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = default_timer()\n",
    "\n",
    "# Make a pipline.\n",
    "logistic_regression_pipe = make_pipeline(StandardScaler(with_mean=False), LogisticRegression(solver='saga', random_state=seed), verbose=False)\n",
    "\n",
    "# Fit the model to the balanced training data.\n",
    "logistic_regression_pipe.fit(X, y)\n",
    "\n",
    "# Use the fitted model to generate predictions based on test data.\n",
    "y_pred = logistic_regression_pipe.predict(X_test)\n",
    "\n",
    "# Set display labels.\n",
    "labels = [True, False]\n",
    "target_names = ['Major', 'Minor']\n",
    "display_labels = target_names\n",
    "\n",
    "# Display the classification report for Logistic Regression model applied to unbalanced test data.\n",
    "print()\n",
    "print('Classification Report for Logistic Regression on Unbalanced Test Data')\n",
    "print()\n",
    "print(classification_report(y_test, y_pred, labels=labels, target_names=target_names, digits=6))\n",
    "print()\n",
    "\n",
    "# Display the confusion matrices based on the unbalanced test data.\n",
    "# Create a figure.\n",
    "fig = plt.figure(figsize=(6.4 * 2, 4.8 * 2))\n",
    "fig.suptitle('Confusion Matrices for Logistic Regression on Unbalanced Test Data', fontsize=20)\n",
    "    \n",
    "ax = plt.subplot(2, 2, 1)\n",
    "ax.set_title('Normalized over True Severity', fontsize=12)\n",
    "plot_confusion_matrix(logistic_regression_pipe, X_test, y_test, labels=labels, display_labels=display_labels, normalize='true', ax=ax)\n",
    "    \n",
    "ax = plt.subplot(2, 2, 2)\n",
    "ax.set_title('Normalized over Predicted Severity', fontsize=12)\n",
    "plot_confusion_matrix(logistic_regression_pipe, X_test, y_test, labels=labels, display_labels=display_labels, normalize='pred', ax=ax)\n",
    "    \n",
    "ax = plt.subplot(2, 2, 3)\n",
    "ax.set_title('Normalized', fontsize=12)\n",
    "plot_confusion_matrix(logistic_regression_pipe, X_test, y_test, labels=labels, display_labels=display_labels, normalize='all', ax=ax)\n",
    "\n",
    "ax = plt.subplot(2, 2, 4)\n",
    "ax.set_title('Normalized', fontsize=12)\n",
    "plot_confusion_matrix(logistic_regression_pipe, X_test, y_test, labels=labels, display_labels=display_labels, normalize=None, ax=ax)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print()\n",
    "print('Logistic Regression Classification Report and Confusion Matrices generated in %f seconds.' % elapsed_time(t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_elapsed_time(notebook_start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"support_vector_machine\">Build and Test a Support Vector Machine<\\h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = default_timer()\n",
    "\n",
    "# Make a pipline.\n",
    "support_vector_machine_pipe = make_pipeline(StandardScaler(with_mean=False), LinearSVC(dual=False, random_state=seed), verbose=False)\n",
    "\n",
    "# Fit the model to the balanced training data.\n",
    "support_vector_machine_pipe.fit(X, y)\n",
    "\n",
    "# Use the fitted model to generate predictions based on test data.\n",
    "y_pred = support_vector_machine_pipe.predict(X_test)\n",
    "\n",
    "# Set display labels.\n",
    "labels = [True, False]\n",
    "target_names = ['Major', 'Minor']\n",
    "display_labels = target_names\n",
    "\n",
    "\n",
    "# Display the classification report for Support Vector Machine model applied to unbalanced test data.\n",
    "print()\n",
    "print('Classification Report for Support Vector Machine for Unbalanced Test Data')\n",
    "print()\n",
    "print(classification_report(y_test, y_pred, labels=labels, target_names=target_names, digits=6))\n",
    "print()\n",
    "\n",
    "# Display the confusion matrices based on the unbalanced test data.\n",
    "# Create a figure.\n",
    "fig = plt.figure(figsize=(6.4 * 2, 4.8 * 2))\n",
    "fig.suptitle('Confusion Matrices for Unbalanced Test Data', fontsize=20)\n",
    "    \n",
    "ax = plt.subplot(2, 2, 1)\n",
    "ax.set_title('Normalized over True Severity', fontsize=12)\n",
    "plot_confusion_matrix(support_vector_machine_pipe, X_test, y_test, labels=labels, display_labels=display_labels, normalize='true', ax=ax)\n",
    "    \n",
    "ax = plt.subplot(2, 2, 2)\n",
    "ax.set_title('Normalized over Predicted Severity', fontsize=12)\n",
    "plot_confusion_matrix(support_vector_machine_pipe, X_test, y_test, labels=labels, display_labels=display_labels, normalize='pred', ax=ax)\n",
    "    \n",
    "ax = plt.subplot(2, 2, 3)\n",
    "ax.set_title('Normalized', fontsize=12)\n",
    "plot_confusion_matrix(support_vector_machine_pipe, X_test, y_test, labels=labels, display_labels=display_labels, normalize='all', ax=ax)\n",
    "\n",
    "ax = plt.subplot(2, 2, 4)\n",
    "ax.set_title('Normalized', fontsize=12)\n",
    "plot_confusion_matrix(support_vector_machine_pipe, X_test, y_test, labels=labels, display_labels=display_labels, normalize=None, ax=ax)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print()\n",
    "print('Support Vector Machine Classification Report and Confusion Matrices generated in %f seconds.' % elapsed_time(t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_elapsed_time(notebook_start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"knn_classifier\">Building a k-Nearest Neighbors Classifier<\\h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = default_timer()\n",
    "\n",
    "# Set the maximum number of neighbors based on number of training samples in each folds to prevent ValueError.\n",
    "# Also, ensure number of neighbors is always an odd integer to avoid ties, using '2 * (N // 2) - 1' technique.\n",
    "upper_bound = 10 # Set by user based on system resources.\n",
    "maximum_number_of_neighbors = \\\n",
    "    min(2 * (upper_bound // 2) - 1, (np.int(2 * ((X.shape[0] / number_of_folds) // 2) - 1 )))\n",
    "print('maximum_number_of_neighbors = %d' % (maximum_number_of_neighbors))\n",
    "\n",
    "# Make a pipeline.\n",
    "k_neighbors_pipeline = make_pipeline(StandardScaler(with_mean=False), KNeighborsClassifier(n_jobs=-1), verbose=False)\n",
    "\n",
    "# Use only odd numbers of neighbors to avoid ties.\n",
    "# Number of neighbors range starts high and ends low, allowing user to  monitor kernel messages for possible local maxima in score.\n",
    "grid_parameters = {'kneighborsclassifier__n_neighbors': range(maximum_number_of_neighbors, 1, -2)}\n",
    "scoring = ['recall_weighted']\n",
    "grid_search_cv = GridSearchCV(k_neighbors_pipeline, param_grid=grid_parameters, scoring=scoring, n_jobs=-1, refit='recall_weighted',\\\n",
    "                              cv=number_of_folds, verbose=50, error_score='raise', return_train_score=False)\n",
    "\n",
    "grid_search_cv.fit(X, y)\n",
    "\n",
    "print()\n",
    "print('Completed grid search in %f seconds' % (elapsed_time(t0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store and print the best KNeighborsClassifier from GridSearchCV.\n",
    "k_neighbors_clf = grid_search_cv.best_estimator_\n",
    "print('Best KNeighborsClassifier parameters:')\n",
    "for key in k_neighbors_clf.get_params().keys():\n",
    "    print(key, ':', k_neighbors_clf.get_params()[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = default_timer()\n",
    "\n",
    "# Use the fitted model to generate predictions based on test data.\n",
    "y_pred = k_neighbors_clf.predict(X_test)\n",
    "\n",
    "# Set display labels.\n",
    "labels = [True, False]\n",
    "target_names = ['Major', 'Minor']\n",
    "display_labels = target_names\n",
    "\n",
    "# Display the classification report for k-Nearest Neighbors model applied to unbalanced test data.\n",
    "print()\n",
    "print('Classification Report for k-Nearest Neighbors for Unbalanced Test Data')\n",
    "print()\n",
    "print(classification_report(y_test, y_pred, labels=labels, target_names=target_names, digits=6))\n",
    "print()\n",
    "\n",
    "# Display the confusion matrices based on the unbalanced test data.\n",
    "# Create a figure.\n",
    "fig = plt.figure(figsize=(6.4 * 2, 4.8 * 2))\n",
    "fig.suptitle('Confusion Matrices for Unbalanced Test Data', fontsize=20)\n",
    "    \n",
    "ax = plt.subplot(2, 2, 1)\n",
    "ax.set_title('Normalized over True Severity', fontsize=12)\n",
    "plot_confusion_matrix(k_neighbors_clf, X_test, y_test, labels=labels, display_labels=display_labels, normalize='true', ax=ax)\n",
    "    \n",
    "ax = plt.subplot(2, 2, 2)\n",
    "ax.set_title('Normalized over Predicted Severity', fontsize=12)\n",
    "plot_confusion_matrix(k_neighbors_clf, X_test, y_test, labels=labels, display_labels=display_labels, normalize='pred', ax=ax)\n",
    "    \n",
    "ax = plt.subplot(2, 2, 3)\n",
    "ax.set_title('Normalized', fontsize=12)\n",
    "plot_confusion_matrix(k_neighbors_clf, X_test, y_test, labels=labels, display_labels=display_labels, normalize='all', ax=ax)\n",
    "\n",
    "ax = plt.subplot(2, 2, 4)\n",
    "ax.set_title('Not Normalized', fontsize=12)\n",
    "plot_confusion_matrix(k_neighbors_clf, X_test, y_test, labels=labels, display_labels=display_labels, normalize=None, ax=ax)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print()\n",
    "print('k-Nearest Neighbors Classification Report and Confusion Matrices generated in %f seconds.' % elapsed_time(t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_elapsed_time(notebook_start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"decision_tree_classifier\">Building and Test a Decision Tree Classifier<\\h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = default_timer()\n",
    "\n",
    "# Make a pipline.\n",
    "decision_tree_pipe = make_pipeline(StandardScaler(with_mean=False), DecisionTreeClassifier(random_state=seed), verbose=False)\n",
    "\n",
    "# Fit the model to the balanced training data.\n",
    "decision_tree_pipe.fit(X, y)\n",
    "\n",
    "# Use the fitted model to generate predictions based on test data.\n",
    "y_pred = decision_tree_pipe.predict(X_test)\n",
    "\n",
    "# Set display labels.\n",
    "labels = [True, False]\n",
    "target_names = ['Major', 'Minor']\n",
    "display_labels = target_names\n",
    "\n",
    "# Display the classification report for Decision Tree model applied to unbalanced test data.\n",
    "print()\n",
    "print('Classification Report for Decision Tree for Unbalanced Test Data')\n",
    "print()\n",
    "print(classification_report(y_test, y_pred, labels=labels, target_names=target_names, digits=6))\n",
    "print()\n",
    "\n",
    "# Display the confusion matrices based on the unbalanced test data.\n",
    "# Create a figure.\n",
    "fig = plt.figure(figsize=(6.4 * 2, 4.8 * 2))\n",
    "fig.suptitle('Confusion Matrices for Unbalanced Test Data', fontsize=20)\n",
    "    \n",
    "ax = plt.subplot(2, 2, 1)\n",
    "ax.set_title('Normalized over True Severity', fontsize=12)\n",
    "plot_confusion_matrix(decision_tree_pipe, X_test, y_test, labels=labels, display_labels=display_labels, normalize='true', ax=ax)\n",
    "    \n",
    "ax = plt.subplot(2, 2, 2)\n",
    "ax.set_title('Normalized over Predicted Severity', fontsize=12)\n",
    "plot_confusion_matrix(decision_tree_pipe, X_test, y_test, labels=labels, display_labels=display_labels, normalize='pred', ax=ax)\n",
    "    \n",
    "ax = plt.subplot(2, 2, 3)\n",
    "ax.set_title('Normalized', fontsize=12)\n",
    "plot_confusion_matrix(decision_tree_pipe, X_test, y_test, labels=labels, display_labels=display_labels, normalize='all', ax=ax)\n",
    "\n",
    "ax = plt.subplot(2, 2, 4)\n",
    "ax.set_title('Not Normalized', fontsize=12)\n",
    "plot_confusion_matrix(decision_tree_pipe, X_test, y_test, labels=labels, display_labels=display_labels, normalize=None, ax=ax)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print()\n",
    "print('Decision Tree Classification Report and Confusion Matrices generated in %f seconds.' % elapsed_time(t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_elapsed_time(notebook_start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare the Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_elapsed_time(notebook_start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Build a demo Decision Tree, fit it on all the training data, and print the tree.\n",
    "# This is only for demonstrative purposes.\n",
    "t0 = default_timer()\n",
    "scaler = StandardScaler(with_mean=False)\n",
    "demo_decision_tree_clf = DecisionTreeClassifier(random_state=seed)\n",
    "demo_decision_tree_clf.fit(scaler.fit_transform(X),y)\n",
    "vertical_scale_factor = 2\n",
    "horizontal_scale_factor = 3.5\n",
    "dpi = 10 * 100\n",
    "fig = plt.figure(figsize=[horizontal_scale_factor * 6.4, vertical_scale_factor * 4.8], dpi=dpi)\n",
    "_ = plot_tree(demo_decision_tree_clf, feature_names=encoder.get_feature_names(), filled=True, proportion=True, class_names=True)\n",
    "# Save decision tree figure to file with a unique name using the system time.\n",
    "fig.savefig('../decision_tree-{}x{}-{}dpi-{}.png'.format(str(horizontal_scale_factor), str(vertical_scale_factor), str(dpi),\\\n",
    "                                                      str(int(os.times()[4]))))\n",
    "plt.show()\n",
    "print('Decision Tree plotted in %f seconds.' % elapsed_time(t0))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Print text representation of decision tree.\n",
    "t0 = default_timer()\n",
    "text_representation = export_text(demo_decision_tree_clf)#, feature_names=encoder.get_feature_names())#, show_weights=True)\n",
    "print(text_representation)\n",
    "print('Text Representation of Decision Tree plotted in %f seconds.' % elapsed_time(t0))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "scoring = ['f1_macro', 'f1_weighted', 'neg_log_loss', 'precision_macro', 'precision_weighted', 'recall_macro', 'recall_weighted']\n",
    "decision_tree_scores = cross_validate(decision_tree_clf, X, y, scoring=scoring, n_jobs=-1, cv=number_of_folds, return_estimator=True)\n",
    "print(\"Decision Tree score keys:\", sorted(decision_tree_scores.keys()))\n",
    "print('Decision Tree classifiers constructed in %f seconds.' % elapsed_time(t0))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print_elapsed_time(notebook_start_time)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Set starting time for reports and graphics.\n",
    "t0 = default_timer()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# For the Logistic Regression models, create a dictionary of statistics and\n",
    "# report them for the model of each fold in the cross-validation.\n",
    "print('Logistic Regression Cross-Validation Scores on Balanced Training Data:')\n",
    "print()\n",
    "# Define dictionary of statistics for the Logistic Regression models.\n",
    "logistic_regression_statistics = dict()\n",
    "for key in sorted(logistic_regression_scores.keys()):\n",
    "    if key != 'estimator':\n",
    "        logistic_regression_statistics[key] = {'mean': np.mean(logistic_regression_scores[key]),\\\n",
    "                                               'std': np.std(logistic_regression_scores[key]),\\\n",
    "                                               'values': logistic_regression_scores[key]}\n",
    "        print('%s: mean = %f, std = %f' %\\\n",
    "              (key, logistic_regression_statistics[key]['mean'], logistic_regression_statistics[key]['std']))\n",
    "        print('%s :%s' % (key, logistic_regression_statistics[key]['values']), sep='')\n",
    "        print()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Use cross_val_predict to display confusion matrices for the Logistic Regression model applied to the unbalanced test data.\n",
    "t0 = default_timer()\n",
    "labels = [True, False]\n",
    "target_names = ['Major', 'Minor']\n",
    "display_labels = target_names\n",
    "logistic_regression_clf = make_pipeline(StandardScaler(with_mean=False),\\\n",
    "                                        LogisticRegression(solver='saga', random_state=seed), verbose=False)\n",
    "\n",
    "y_pred = cross_val_predict(logistic_regression_clf, X, y, n_jobs=-1, cv=number_of_folds)\n",
    "\n",
    "logistict_regression_clf = \n",
    "\n",
    "# Display the classification report for Logistic Regression in a cross-validation using training data.\n",
    "print()\n",
    "print('Classification Report for Logistic Regression on Balanced Data in Cross-Validation')\n",
    "print()\n",
    "print(classification_report(y, y_pred, labels=labels, target_names=target_names, digits=6))\n",
    "print()\n",
    "\n",
    "# Fit the model to the balanced training data.\n",
    "logistic_regression_clf.fit(X, y)\n",
    "\n",
    "# Display the confusion matrices based on the unbalanced test data.\n",
    "# Create a figure.\n",
    "fig = plt.figure(figsize=(6.4 * 3, 4.8))\n",
    "fig.suptitle('Confusion Matrices for Logistic Regression on Unbalanced Test Data', fontsize=20)\n",
    "    \n",
    "ax = plt.subplot(1, 3, 1)\n",
    "ax.set_title(\"Normalized over True Severity\", fontsize=12)\n",
    "plot_confusion_matrix(logistic_regression_clf, X_test, y_test, labels=labels, display_labels=display_labels, normalize='true', ax=ax)\n",
    "    \n",
    "ax = plt.subplot(1, 3, 2)\n",
    "ax.set_title(\"Normalized over Predicted Severity\", fontsize=12)\n",
    "plot_confusion_matrix(logistic_regression_clf, X_test, y_test, labels=labels, display_labels=display_labels, normalize='pred', ax=ax)\n",
    "    \n",
    "ax = plt.subplot(1, 3, 3)\n",
    "ax.set_title(\"Not Normalized\", fontsize=12)\n",
    "plot_confusion_matrix(logistic_regression_clf, X_test, y_test, labels=labels, display_labels=display_labels, normalize=None, ax=ax)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print('Logistic Regression Classification Report and Confusion Matrices generated in %f seconds.' % elapsed_time(t0))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# For each logistic regression classifier in the cross-validation, print the macro recall, score report, and confusion matrices.\n",
    "number_of_models = len(logistic_regression_scores['estimator'])\n",
    "#labels = ['1', '2', '2b', '3']\n",
    "labels = [True, False]\n",
    "target_names = ['Major', 'Minor']\n",
    "display_labels = target_names\n",
    "for index, logistic_regression_clf in zip(range(number_of_models), logistic_regression_scores['estimator']):\n",
    "    y_test_predicted = logistic_regression_clf.predict(X_test)\n",
    "    print('Logistic Regression, Fold %d of %d:' % (index + 1, number_of_models))\n",
    "    print()\n",
    "    print(classification_report(y_test, y_test_predicted, labels=labels, target_names=target_names, digits=6))\n",
    "        \n",
    "    # Create a figure.\n",
    "    fig = plt.figure(num=str(index + 1), figsize=(6.4 * 3, 4.8))\n",
    "    fig.suptitle('Confusion Matrices for Logistic Regression, Fold %d of %d:' % (index + 1, number_of_models), fontsize=20)\n",
    "    \n",
    "    ax = plt.subplot(1, 3, 1)\n",
    "    ax.set_title(\"Normalized over Predicted Severity\", fontsize=12)\n",
    "    plot_confusion_matrix(logistic_regression_clf, X_test, y_test, labels=labels, display_labels=display_labels, normalize='pred', ax=ax)\n",
    "    \n",
    "    ax = plt.subplot(1, 3, 2)\n",
    "    ax.set_title(\"Normalized over True Severity\", fontsize=12)\n",
    "    plot_confusion_matrix(logistic_regression_clf, X_test, y_test, labels=labels, display_labels=display_labels, normalize='true', ax=ax)\n",
    "    \n",
    "    ax = plt.subplot(1, 3, 3)\n",
    "    ax.set_title(\"Not Normalized\", fontsize=12)\n",
    "    plot_confusion_matrix(logistic_regression_clf, X_test, y_test, labels=labels, display_labels=display_labels, normalize=None, ax=ax)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print_elapsed_time(notebook_start_time)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# For the Support Vector Machine models, create a dictionary of statistics and\n",
    "# report them for the model of each fold in the cross-validation.\n",
    "print('Support Vector Machine Cross-Validation Scores:')\n",
    "print()\n",
    "# Define dictionary of statistics for the Support Vector Machine models.\n",
    "support_vector_machine_statistics = dict()\n",
    "for key in sorted(linear_svc_scores.keys()):\n",
    "    if key != 'estimator':\n",
    "        support_vector_machine_statistics[key] = {'mean': np.mean(linear_svc_scores[key]),\\\n",
    "                                               'std': np.std(linear_svc_scores[key]),\\\n",
    "                                               'values': linear_svc_scores[key]}\n",
    "        print('%s: mean = %f, std = %f' %\\\n",
    "              (key, support_vector_machine_statistics[key]['mean'], support_vector_machine_statistics[key]['std']))\n",
    "        print('%s :%s' % (key, support_vector_machine_statistics[key]['values']), sep='')\n",
    "        print()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Use cross_val_predict to display confusion matrices for the Support Vector Machine model applied to the test data.\n",
    "t0 = default_timer()\n",
    "labels = [True, False]\n",
    "target_names = ['Major', 'Minor']\n",
    "display_labels = target_names\n",
    "linear_svc_clf = make_pipeline(StandardScaler(with_mean=False), LinearSVC(dual=False, random_state=seed), verbose=False)\n",
    "\n",
    "y_pred = cross_val_predict(linear_svc_clf, X, y, n_jobs=-1, cv=number_of_folds)\n",
    "\n",
    "# Display the classification report for Support Vector Machine in a cross-validation using training data.\n",
    "print()\n",
    "print('Classification Report for Support Vector Machine on Balanced Training Data in Cross-Validation')\n",
    "print()\n",
    "print(classification_report(y, y_pred, labels=labels, target_names=target_names, digits=6))\n",
    "print()\n",
    "\n",
    "# Fit the model to the balanced training data.\n",
    "linear_svc_clf.fit(X, y)\n",
    "\n",
    "# Display the confusion matrices based on the unbalanced test data.\n",
    "# Create a figure.\n",
    "fig = plt.figure(figsize=(6.4 * 3, 4.8))\n",
    "fig.suptitle('Confusion Matrices for Support Vector Machine on Unbalanced Test Data', fontsize=20)\n",
    "    \n",
    "ax = plt.subplot(1, 3, 1)\n",
    "ax.set_title(\"Normalized over True Severity\", fontsize=12)\n",
    "plot_confusion_matrix(linear_svc_clf, X_test, y_test, labels=labels, display_labels=display_labels, normalize='true', ax=ax)\n",
    "    \n",
    "ax = plt.subplot(1, 3, 2)\n",
    "ax.set_title(\"Normalized over Predicted Severity\", fontsize=12)\n",
    "plot_confusion_matrix(linear_svc_clf, X_test, y_test, labels=labels, display_labels=display_labels, normalize='pred', ax=ax)\n",
    "    \n",
    "ax = plt.subplot(1, 3, 3)\n",
    "ax.set_title(\"Not Normalized\", fontsize=12)\n",
    "plot_confusion_matrix(linear_svc_clf, X_test, y_test, labels=labels, display_labels=display_labels, normalize=None, ax=ax)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print('Support Vector Machine Classification Report and Confusion Matrices generated in %f seconds.' % elapsed_time(t0))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# For each support vector machine classifier in the cross-validation, print the score report and confusion matrices.\n",
    "number_of_models = len(linear_svc_scores['estimator'])\n",
    "#labels = ['1', '2', '2b', '3']\n",
    "labels = [True, False]\n",
    "target_names = ['Major', 'Minor']\n",
    "display_labels = target_names\n",
    "for index, linear_svc_clf in zip(range(number_of_models), linear_svc_scores['estimator']):\n",
    "    y_test_predicted = linear_svc_clf.predict(X_test)\n",
    "    print('Support Vector Machine, Fold %d of %d:' % (index + 1, number_of_models))\n",
    "    print()\n",
    "    print(classification_report(y_test, y_test_predicted, labels=labels, target_names=target_names, digits=6))\n",
    "        \n",
    "    # Create a figure.\n",
    "    fig = plt.figure(num=str(index + 1), figsize=(6.4 * 3, 4.8))\n",
    "    fig.suptitle('Confusion Matrices for Support Vector Machine, Fold %d of %d:' % (index + 1, number_of_models), fontsize=20)\n",
    "    \n",
    "    ax = plt.subplot(1,3,1)\n",
    "    ax.set_title(\"Normalized over Predicted Severity\", fontsize=12)\n",
    "    plot_confusion_matrix(linear_svc_clf, X_test, y_test, labels=labels, display_labels=display_labels, normalize='pred', ax=ax)\n",
    "    \n",
    "    ax = plt.subplot(1,3,2)\n",
    "    ax.set_title(\"Normalized over True Severity\", fontsize=12)\n",
    "    plot_confusion_matrix(linear_svc_clf, X_test, y_test, labels=labels, display_labels=display_labels, normalize='true', ax=ax)\n",
    "    \n",
    "    ax = plt.subplot(1,3,3)\n",
    "    ax.set_title(\"Not Normalized\", fontsize=12)\n",
    "    plot_confusion_matrix(linear_svc_clf, X_test, y_test, labels=labels, display_labels=display_labels, normalize=None, ax=ax)\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print_elapsed_time(notebook_start_time)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# For the k-Nearest Neighbors models, create a dictionary of statistics and\n",
    "# report them for the model of each fold in the cross-validation.\n",
    "n_neighbors = k_neighbors_scores['estimator'][0][1].get_params()['n_neighbors']\n",
    "print('k-Nearest Neighbors (k = {}) Cross-Validation Scores:'.format(n_neighbors))\n",
    "print()\n",
    "# Define dictionary of statistics for the k-Nearest Neighbors models.\n",
    "k_neighbors_statistics = dict()\n",
    "k_neighbors_statistics['number_of_neighbors'] = n_neighbors\n",
    "for key in sorted(k_neighbors_scores.keys()):\n",
    "    if key != 'estimator':\n",
    "        k_neighbors_statistics[key] = {'mean': np.mean(k_neighbors_scores[key]),\\\n",
    "                                               'std': np.std(k_neighbors_scores[key]),\\\n",
    "                                               'values': k_neighbors_scores[key]}\n",
    "        print('%s: mean = %f, std = %f' %\\\n",
    "              (key, k_neighbors_statistics[key]['mean'], k_neighbors_statistics[key]['std']))\n",
    "        print('%s :%s' % (key, k_neighbors_statistics[key]['values']), sep='')\n",
    "        print()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Use cross_val_predict to display confusion matrices for the k-Nearest Neighbors model applied to the test data.\n",
    "t0 = default_timer()\n",
    "labels = [True, False]\n",
    "target_names = ['Major', 'Minor']\n",
    "display_labels = target_names\n",
    "k_neighbors_clf = grid_search_cv.best_estimator_\n",
    "\n",
    "n_neighbors = k_neighbors_clf.get_params()['kneighborsclassifier__n_neighbors']\n",
    "\n",
    "#y_pred = cross_val_predict(k_neighbors_clf, X, y, n_jobs=-1, cv=number_of_folds)\n",
    "\n",
    "# Display the classification report for k-Nearest Neighbors in a cross-validation using training data.\n",
    "#print()\n",
    "#print('Classification Report for k-Nearest Neighbors (k = {}) on Balanced Training Data in Cross-Validation'.format(n_neighbors))\n",
    "#print()\n",
    "#plot_confusion_matrix(k_neighbors_clf, X, y,  labels=labels, display_labels=display_labels, normalize='true')\n",
    "#plt.show()\n",
    "#print(classification_report(y, y_pred, labels=labels, target_names=target_names, digits=6))\n",
    "#print()\n",
    "#input('Wait...')\n",
    "# Fit the model to the balanced training data.\n",
    "# k_neighbors_clf.fit(X, y)\n",
    "\n",
    "# Display the confusion matrices based on the unbalanced test data.\n",
    "# Create a figure.\n",
    "fig = plt.figure(figsize=(6.4 * 3, 4.8))\n",
    "fig.suptitle('Confusion Matrices for k-Nearest Neighbors (k = {}) on Unbalanced Test Data'.format(n_neighbors), fontsize=20)\n",
    "    \n",
    "ax = plt.subplot(1, 3, 1)\n",
    "ax.set_title(\"Normalized over True Severity\", fontsize=12)\n",
    "plot_confusion_matrix(k_neighbors_clf, X_test, y_test, labels=labels, display_labels=display_labels, normalize='true', ax=ax)\n",
    "    \n",
    "ax = plt.subplot(1, 3, 2)\n",
    "ax.set_title(\"Normalized over Predicted Severity\", fontsize=12)\n",
    "plot_confusion_matrix(k_neighbors_clf, X_test, y_test, labels=labels, display_labels=display_labels, normalize='pred', ax=ax)\n",
    "    \n",
    "ax = plt.subplot(1, 3, 3)\n",
    "ax.set_title(\"Not Normalized\", fontsize=12)\n",
    "plot_confusion_matrix(k_neighbors_clf, X_test, y_test, labels=labels, display_labels=display_labels, normalize=None, ax=ax)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print('k-Nearest Neighbors Classification Report and Confusion Matrices generated in {} seconds.'.format(elapsed_time(t0)))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# For each k-Nearest Neighbor classifier in the cross-validation, print the macro recall, score report, and confusion matrices.\n",
    "number_of_models = len(k_neighbors_scores['estimator'])\n",
    "#labels = ['1', '2', '2b', '3']\n",
    "labels = [True, False]\n",
    "target_names = ['Major', 'Minor']\n",
    "display_labels = target_names\n",
    "for index, knn_clf in zip(range(number_of_models), k_neighbors_scores['estimator']):\n",
    "    n_neighbors = knn_clf.get_params()['kneighborsclassifier__n_neighbors']\n",
    "    y_test_predicted = knn_clf.predict(X_test)\n",
    "    print('k-Nearest Neighbors (k = %d), Fold %d of %d:' % (n_neighbors, index + 1, number_of_models))\n",
    "    print()\n",
    "    print(classification_report(y_test, y_test_predicted, labels=labels, target_names=target_names, digits=6))\n",
    "        \n",
    "    # Create a figure.\n",
    "    fig = plt.figure(num=str(index + 1), figsize=(6.4 * 3, 4.8))\n",
    "    fig.suptitle('Confusion Matrices for k-Nearest Neighbors (k = %d), Fold %d of %d:' %\\\n",
    "                 (n_neighbors, index + 1, number_of_models), fontsize=20)\n",
    "    \n",
    "    ax = plt.subplot(1, 3, 1)\n",
    "    ax.set_title(\"Normalized over Predicted Severity\", fontsize=12)\n",
    "    plot_confusion_matrix(knn_clf, X_test, y_test, labels=labels, display_labels=display_labels, normalize='pred', ax=ax)\n",
    "    \n",
    "    ax = plt.subplot(1, 3, 2)\n",
    "    ax.set_title(\"Normalized over True Severity\", fontsize=12)\n",
    "    plot_confusion_matrix(knn_clf, X_test, y_test, labels=labels, display_labels=display_labels, normalize='true', ax=ax)\n",
    "    \n",
    "    ax = plt.subplot(1, 3, 3)\n",
    "    ax.set_title(\"Not Normalized\", fontsize=12)\n",
    "    plot_confusion_matrix(knn_clf, X_test, y_test, labels=labels, display_labels=display_labels, normalize=None, ax=ax)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# For the Decision Tree models, create a dictionary of statistics and\n",
    "# report them for the model of each fold in the cross-validation.\n",
    "print('Decision Tree Cross-Validation Scores:')\n",
    "print()\n",
    "# Define dictionary of statistics for the Decision Tree models.\n",
    "decision_tree_statistics = dict()\n",
    "for key in sorted(decision_tree_scores.keys()):\n",
    "    if key != 'estimator':\n",
    "        decision_tree_statistics[key] = {'mean': np.mean(decision_tree_scores[key]),\\\n",
    "                                               'std': np.std(decision_tree_scores[key]),\\\n",
    "                                               'values': decision_tree_scores[key]}\n",
    "        print('%s: mean = %f, std = %f' %\\\n",
    "              (key, decision_tree_statistics[key]['mean'], decision_tree_statistics[key]['std']))\n",
    "        print('%s :%s' % (key, decision_tree_statistics[key]['values']), sep='')\n",
    "        print()        "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Use cross_val_predict to display confusion matrices for the Decision Tree model applied to the unbalanced test data.\n",
    "t0 = default_timer()\n",
    "labels = [True, False]\n",
    "target_names = ['Major', 'Minor']\n",
    "display_labels = target_names\n",
    "\n",
    "decision_tree_clf = make_pipeline(StandardScaler(with_mean=False), DecisionTreeClassifier(random_state=seed), verbose=False)\n",
    "\n",
    "y_pred = cross_val_predict(decision_tree_clf, X, y, n_jobs=-1, cv=number_of_folds)\n",
    "\n",
    "# Display the classification report for k-Nearest Neighbors in a cross-validation using training data.\n",
    "print()\n",
    "print('Classification Report for Decision Tree on Balanced Training Data in Cross-Validation')\n",
    "print()\n",
    "print(classification_report(y, y_pred, labels=labels, target_names=target_names, digits=6))\n",
    "print()\n",
    "\n",
    "# Fit the model to the balanced training data.\n",
    "decision_tree_clf.fit(X, y)\n",
    "\n",
    "# Display the confusion matrices based on the unbalanced test data.\n",
    "# Create a figure.\n",
    "fig = plt.figure(figsize=(6.4 * 3, 4.8))\n",
    "fig.suptitle('Confusion Matrices for Decision Tree on Unbalanced Test Data', fontsize=20)\n",
    "    \n",
    "ax = plt.subplot(1, 3, 1)\n",
    "ax.set_title(\"Normalized over True Severity\", fontsize=12)\n",
    "plot_confusion_matrix(decision_tree_clf, X_test, y_test, labels=labels, display_labels=display_labels, normalize='true', ax=ax)\n",
    "    \n",
    "ax = plt.subplot(1, 3, 2)\n",
    "ax.set_title(\"Normalized over Predicted Severity\", fontsize=12)\n",
    "plot_confusion_matrix(decision_tree_clf, X_test, y_test, labels=labels, display_labels=display_labels, normalize='pred', ax=ax)\n",
    "    \n",
    "ax = plt.subplot(1, 3, 3)\n",
    "ax.set_title(\"Not Normalized\", fontsize=12)\n",
    "plot_confusion_matrix(decision_tree_clf, X_test, y_test, labels=labels, display_labels=display_labels, normalize=None, ax=ax)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print('Decision Tree Classification Report and Confusion Matrices generated in {} seconds.'.format(elapsed_time(t0)))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# For each decision tree classifier in the cross-validation, print the score report and confusion matrices.\n",
    "number_of_models = len(decision_tree_scores['estimator'])\n",
    "#labels = ['1', '2', '2b', '3']\n",
    "labels = [True, False]\n",
    "target_names = ['Major', 'Minor']\n",
    "display_labels = target_names\n",
    "\n",
    "for index, decision_tree_clf in zip(range(number_of_models), decision_tree_scores['estimator']):\n",
    "    y_test_predicted = decision_tree_clf.predict(X_test)\n",
    "    print('Decision Tree, Fold %d of %d:' % (index + 1, number_of_models))\n",
    "    print()\n",
    "    print(classification_report(y_test, y_test_predicted, labels=labels, target_names=target_names, digits=6))\n",
    "        \n",
    "    # Create a figure.\n",
    "    fig = plt.figure(num=str(index + 1), figsize=(6.4 * 3, 4.8))\n",
    "    fig.suptitle('Confusion Matrices for Decision Tree, Fold %d of %d:' % (index + 1, number_of_models), fontsize=20)\n",
    "    \n",
    "    ax = plt.subplot(1,3,1)\n",
    "    ax.set_title(\"Normalized over Predicted Severity\", fontsize=12)\n",
    "    plot_confusion_matrix(decision_tree_clf, X_test, y_test, labels=labels, display_labels=display_labels, normalize='pred', ax=ax)\n",
    "    \n",
    "    ax = plt.subplot(1,3,2)\n",
    "    ax.set_title(\"Normalized over True Severity\", fontsize=12)\n",
    "    plot_confusion_matrix(decision_tree_clf, X_test, y_test, labels=labels, display_labels=display_labels, normalize='true', ax=ax)\n",
    "    \n",
    "    ax = plt.subplot(1,3,3)\n",
    "    ax.set_title(\"Not Normalized\", fontsize=12)\n",
    "    plot_confusion_matrix(decision_tree_clf, X_test, y_test, labels=labels, display_labels=display_labels, normalize=None, ax=ax)\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Define list of strings to use for names of the models.\n",
    "model_names = ['Logistic Regression', 'Support Vector Machine', 'k-Nearest Neighbors', 'Decision Tree']\n",
    "model_names"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Define a dictionary for displaying the statistics for the Logistic Regression model with pretty column names.\n",
    "selected_keys = ['test_f1_weighted', 'test_neg_log_loss', 'test_precision_weighted', 'test_recall_weighted']\n",
    "display_keys = ['F-1 (weighted)', 'Negative Log Loss', 'Precision (weighted)', 'Recall (weighted)']\n",
    "logistic_regression_statistics_display = dict.fromkeys(display_keys)\n",
    "\n",
    "# For each selected key, add the corresponding display/value pair to the dictionary.\n",
    "for key, display_key in zip(selected_keys, display_keys):\n",
    "    logistic_regression_statistics_display[display_key] = logistic_regression_statistics[key]\n",
    "\n",
    "print(logistic_regression_statistics_display)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Define the DataFrame of statistics for the Logistic Regression.\n",
    "df_logistic_regression_statistics_display = pd.DataFrame(data=logistic_regression_statistics_display)\n",
    "df_logistic_regression_scores = pd.DataFrame(df_logistic_regression_statistics_display.iloc[0, :]).T\n",
    "df_logistic_regression_scores.reset_index(inplace=True)\n",
    "df_logistic_regression_scores"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Define a dictionary for displaying the statistics for the Support Vector Machine model with pretty column names.\n",
    "selected_keys = ['test_f1_weighted', 'test_precision_weighted', 'test_recall_weighted']\n",
    "display_keys = ['F-1 (weighted)', 'Precision (weighted)', 'Recall (weighted)']\n",
    "support_vector_machine_statistics_display = dict.fromkeys(display_keys)\n",
    "\n",
    "# For each selected key, add the corresponding display_key/value pair to the dictionary.\n",
    "for key, display_key in zip(selected_keys, display_keys):\n",
    "    support_vector_machine_statistics_display[display_key] = support_vector_machine_statistics[key]\n",
    "    \n",
    "print(support_vector_machine_statistics_display)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Define the DataFrame of scores for the Support Vector Machine.\n",
    "df_support_vector_machine_statistics_display = pd.DataFrame(data=support_vector_machine_statistics_display)\n",
    "df_support_vector_machine_scores = pd.DataFrame(df_support_vector_machine_statistics_display.iloc[0,:]).T\n",
    "df_support_vector_machine_scores.reset_index(inplace=True)\n",
    "df_support_vector_machine_scores"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Define a dictionary for displaying the statistics for the k-Nearest Neighbors model with pretty column names.\n",
    "selected_keys = ['test_f1_weighted', 'test_neg_log_loss', 'test_precision_weighted', 'test_recall_weighted']\n",
    "display_keys = ['F-1 (weighted)', 'Negative Log Loss', 'Precision (weighted)', 'Recall (weighted)']\n",
    "k_neighbors_statistics_display = dict.fromkeys(display_keys)\n",
    "\n",
    "# For each selected key, add the corresponding display_key/value pair to the dictionary.\n",
    "for key, display_key in zip(selected_keys, display_keys):\n",
    "    k_neighbors_statistics_display[display_key] = k_neighbors_statistics[key]\n",
    "    \n",
    "print(k_neighbors_statistics_display)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Define the DataFrame of scores for the Support Vector Machine.\n",
    "df_k_neighbors_statistics_display = pd.DataFrame(data=k_neighbors_statistics_display)\n",
    "df_k_neighbors_scores = pd.DataFrame(df_k_neighbors_statistics_display.iloc[0,:]).T\n",
    "df_k_neighbors_scores.reset_index(inplace=True)\n",
    "df_k_neighbors_scores"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Define a dictionary for displaying the statistics for the Decision Tree model with pretty column names.\n",
    "selected_keys = ['test_f1_weighted', 'test_neg_log_loss', 'test_precision_weighted', 'test_recall_weighted']\n",
    "display_keys = ['F-1 (weighted)', 'Negative Log Loss', 'Precision (weighted)', 'Recall (weighted)']\n",
    "decision_tree_statistics_display = dict.fromkeys(display_keys)\n",
    "\n",
    "# For each selected key, add the corresponding display_key/value pair to the dictionary.\n",
    "for key, display_key in zip(selected_keys, display_keys):\n",
    "    decision_tree_statistics_display[display_key] = decision_tree_statistics[key]\n",
    "    \n",
    "print(decision_tree_statistics_display)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Define the DataFrame of scores for the Support Vector Machine.\n",
    "df_decision_tree_statistics_display = pd.DataFrame(data=decision_tree_statistics_display)\n",
    "df_decision_tree_scores = pd.DataFrame(df_decision_tree_statistics_display.iloc[0,:]).T\n",
    "df_decision_tree_scores.reset_index(inplace=True)\n",
    "df_decision_tree_scores"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "list_of_score_df = [df_logistic_regression_scores, df_support_vector_machine_scores, df_k_neighbors_scores, df_decision_tree_scores]\n",
    "df_scores = pd.concat(list_of_score_df, axis='index', ignore_index=True)\n",
    "df_scores.drop(columns='index', inplace=True)\n",
    "df_scores.set_axis(model_names, axis='index', inplace=True)\n",
    "display_columns = ['Recall (weighted)', 'Precision (weighted)', 'F-1 (weighted)', 'Negative Log Loss']\n",
    "df_scores.sort_values('Recall (weighted)', axis='rows', ascending=False, inplace=True)\n",
    "df_scores[display_columns]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print('Reports and graphics generated in %f seconds.' % (elapsed_time(t0)))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print_elapsed_time(notebook_start_time)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Use cross_val_predict to display confusion matrices for the Logistic Regression model applied to the test data.\n",
    "t0 = default_timer()\n",
    "labels = [True, False]\n",
    "target_names = ['Major', 'Minor']\n",
    "display_labels = target_names\n",
    "logistic_regression_clf = make_pipeline(StandardScaler(with_mean=False),\\\n",
    "                                        LogisticRegression(solver='saga', random_state=seed), verbose=True)\n",
    "\n",
    "y_pred = cross_val_predict(logistic_regression_clf, X, y, n_jobs=-1, cv=number_of_folds, verbose=100)\n",
    "\n",
    "# Display the classification report and confusion matrices based on the predictions.\n",
    "print()\n",
    "print('Classification Report for Logistic Regression in Cross-Validation')\n",
    "print()\n",
    "print(classification_report(y, y_pred, labels=labels, target_names=target_names, digits=6))\n",
    "\n",
    "# Fit the model to the balanced training data.\n",
    "logistic_regression_clf.fit(X, y)\n",
    "\n",
    "# Create a figure.\n",
    "fig = plt.figure(figsize=(6.4 * 3, 4.8))\n",
    "fig.suptitle('Confusion Matrices for Logistic Regression on Test Data', fontsize=20)\n",
    "    \n",
    "ax = plt.subplot(1, 3, 1)\n",
    "ax.set_title(\"Normalized over Predicted Severity\", fontsize=12)\n",
    "plot_confusion_matrix(logistic_regression_clf, X_test, y_test, labels=labels, display_labels=display_labels, normalize='pred', ax=ax)\n",
    "    \n",
    "ax = plt.subplot(1, 3, 2)\n",
    "ax.set_title(\"Normalized over True Severity\", fontsize=12)\n",
    "plot_confusion_matrix(logistic_regression_clf, X_test, y_test, labels=labels, display_labels=display_labels, normalize='true', ax=ax)\n",
    "    \n",
    "ax = plt.subplot(1, 3, 3)\n",
    "ax.set_title(\"Not Normalized\", fontsize=12)\n",
    "plot_confusion_matrix(logistic_regression_clf, X_test, y_test, labels=labels, display_labels=display_labels, normalize=None, ax=ax)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print('Logistic Regression classifiers constructed in %f seconds.' % elapsed_time(t0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
